{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'mnist34', 'workers': 2, 'batchSize': 64, 'imageSize': 28, 'nz': 110, 'ngf': 64, 'ndf': 64, 'niter': 25, 'lr': 0.0002, 'beta1': 0.5, 'cuda': False, 'ngpu': 1, 'netG': '', 'netD': '', 'outf': '.', 'manualSeed': None, 'num_classes': 2, 'gpu_id': 0, 'fraction': 0.005}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "##############################################################################\n",
    "args = {}\n",
    "args['dataset'] = 'mnist34' # 数据集名称\n",
    "#args['dataroot'] = '/path/to/dataset'  # 数据集路径\n",
    "args['workers'] = 2  # 数据加载线程数\n",
    "args['batchSize'] = 64  # 批处理大小\n",
    "args['imageSize'] = 28  # 图像尺寸\n",
    "args['nz'] = 110  # 噪声向量维度 # 100+10\n",
    "args['ngf'] = 64  # 生成器特征图通道数\n",
    "args['ndf'] = 64  # 判别器特征图通道数\n",
    "args['niter'] = 25  # 训练迭代次数\n",
    "args['lr'] = 0.0002  # 学习率\n",
    "args['beta1'] = 0.5  # Adam 优化器的 beta1 参数\n",
    "args['cuda'] = False  # 是否使用 CUDA\n",
    "args['ngpu'] = 1  # 使用的 GPU 数量\n",
    "args['netG'] = ''  # 生成器模型文件\n",
    "args['netD'] = ''  # 判别器模型文件\n",
    "args['outf'] = '.'  # 输出文件夹\n",
    "args['manualSeed'] = None  # 随机数种子\n",
    "args['num_classes'] = 2  # 类别数\n",
    "args['gpu_id'] = 0  # GPU ID\n",
    "args['fraction'] = 0.005 # 有监督学习的样本比例\n",
    "print(args)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the MNIST dataset with labels 3 and 4.\n",
      "Imbalanced Ratio:  0.005\n",
      "Number of label 3 in the final training set:  6131\n",
      "Number of label 4 in the final training set (after downsampling):  30\n",
      "Number of label 3 in the final test set:  1010\n",
      "Number of label 4 in the final test set:  982\n",
      "Total samples in final training set:  6161\n",
      "Total samples in final test set:  1992\n",
      "Number of batches in training set:  96\n",
      "Number of batches in test set:  31\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/Users/max/MasterThesis/Training/\"))\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "print(\"This is the MNIST dataset with labels 3 and 4.\")\n",
    "print(\"Imbalanced Ratio: \", args['fraction'])\n",
    "# 数据预处理\n",
    "mnist34_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "full_train_datasets = datasets.MNIST(root=\"/Users/max/MasterThesisData/MNIST\", train=True, transform=mnist34_transforms, download=True)\n",
    "full_test_datasets = datasets.MNIST(root=\"/Users/max/MasterThesisData/MNIST\", train=False, transform=mnist34_transforms, download=True)\n",
    "\n",
    "# 选取标签为 3 和 4 的索引\n",
    "indices3_train = [i for i in range(len(full_train_datasets)) if full_train_datasets.targets[i] == 3]\n",
    "indices4_train = [i for i in range(len(full_train_datasets)) if full_train_datasets.targets[i] == 4]\n",
    "\n",
    "indices3_test = [i for i in range(len(full_test_datasets)) if full_test_datasets.targets[i] == 3]\n",
    "indices4_test = [i for i in range(len(full_test_datasets)) if full_test_datasets.targets[i] == 4]\n",
    "\n",
    "# 获取训练集中标签为 3 和 4 的数据\n",
    "mnist3_train_data = full_train_datasets.data[indices3_train]\n",
    "mnist3_train_labels = torch.ones(len(indices3_train), dtype=torch.long)  # 标签 3 映射为 1 \n",
    "\n",
    "mnist4_train_data = full_train_datasets.data[indices4_train]\n",
    "mnist4_train_labels = torch.zeros(len(indices4_train), dtype=torch.long)  # 标签 4 映射为 0\n",
    "\n",
    "# 获取测试集中标签为 3 和 4 的数据\n",
    "mnist3_test_data = full_test_datasets.data[indices3_test]\n",
    "mnist3_test_labels = torch.ones(len(indices3_test), dtype=torch.long)  # 标签 3 映射为 1 \n",
    "\n",
    "mnist4_test_data = full_test_datasets.data[indices4_test]\n",
    "mnist4_test_labels = torch.zeros(len(indices4_test), dtype=torch.long)  # 标签 4 映射为 0\n",
    "\n",
    "# we can set the imbalanced ratio 0.005, 0.01, 0.02, 0.05, 0.1, 0.2\n",
    "fraction = int(args['fraction'] * len(mnist3_train_data))  ### control the fraction of the data to be used\n",
    "selected_indices_4 = np.random.choice(len(mnist4_train_data), fraction, replace=False)\n",
    "\n",
    "fraction_mnist4_train_data = mnist4_train_data[selected_indices_4]\n",
    "fraction_mnist4_train_labels = mnist4_train_labels[selected_indices_4]\n",
    "\n",
    "# 创建最终的训练和测试数据集\n",
    "Final_train_data = torch.cat([mnist3_train_data, fraction_mnist4_train_data], dim=0)\n",
    "Final_train_labels = torch.cat([mnist3_train_labels, fraction_mnist4_train_labels], dim=0)\n",
    "\n",
    "Final_test_data = torch.cat([mnist3_test_data, mnist4_test_data], dim=0)\n",
    "Final_test_labels = torch.cat([mnist3_test_labels, mnist4_test_labels], dim=0)\n",
    "\n",
    "# 创建 TensorDataset\n",
    "Final_train_datasets = TensorDataset(Final_train_data.unsqueeze(1).float() / 255, Final_train_labels)\n",
    "Final_test_datasets = TensorDataset(Final_test_data.unsqueeze(1).float() / 255, Final_test_labels)\n",
    "\n",
    "# 数据加载器\n",
    "train_loader = DataLoader(Final_train_datasets, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(Final_test_datasets, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "# 打印信息\n",
    "print(\"Number of label 3 in the final training set: \", len(mnist3_train_data))\n",
    "print(\"Number of label 4 in the final training set (after downsampling): \", len(fraction_mnist4_train_data))\n",
    "print(\"Number of label 3 in the final test set: \", len(mnist3_test_data))\n",
    "print(\"Number of label 4 in the final test set: \", len(mnist4_test_data))\n",
    "\n",
    "print(\"Total samples in final training set: \", len(Final_train_datasets))\n",
    "print(\"Total samples in final test set: \", len(Final_test_datasets))\n",
    "\n",
    "print(\"Number of batches in training set: \", len(train_loader))\n",
    "print(\"Number of batches in test set: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义用于MNIST生成器的网络\n",
    "class _netG_MNIST(nn.Module):\n",
    "    def __init__(self, ngpu, nz):\n",
    "        super(_netG_MNIST, self).__init__()  # 调用父类的构造函数\n",
    "        self.ngpu = ngpu  # 保存GPU数量\n",
    "        self.nz = nz      # 噪声向量的维度（例如：噪声+标签拼接后为110）\n",
    "\n",
    "        # 全连接层：将输入噪声向量映射为256*7*7个特征\n",
    "        self.fc = nn.Linear(nz, 256 * 7 * 7)  # 输入维度为nz，输出维度为256*7*7\n",
    "\n",
    "        # 转置卷积层1：将256通道、7x7的特征图转换为128通道、14x14的特征图\n",
    "        self.tconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),  # 转置卷积，尺寸放大2倍\n",
    "            nn.BatchNorm2d(128),         # 对128个通道做批归一化\n",
    "            nn.ReLU(True)                # 使用ReLU激活函数\n",
    "        )\n",
    "\n",
    "        # 转置卷积层2：将128通道、14x14的特征图转换为1通道、28x28的图像\n",
    "        self.tconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1, bias=False),  # 转置卷积，尺寸放大2倍\n",
    "            nn.Tanh()                  # 使用Tanh激活函数，将输出映射到[-1,1]之间\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 若使用CUDA且多GPU，则采用数据并行方式\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            input = input.view(-1, self.nz)                           # 将输入展平为二维张量[batch_size, nz]\n",
    "            fc = nn.parallel.data_parallel(self.fc, input, range(self.ngpu))  # 并行通过全连接层\n",
    "            fc = fc.view(-1, 256, 7, 7)                               # 重塑为[batch_size, 256, 7, 7]的特征图\n",
    "            tconv1 = nn.parallel.data_parallel(self.tconv1, fc, range(self.ngpu))  # 并行通过第一个转置卷积层\n",
    "            tconv2 = nn.parallel.data_parallel(self.tconv2, tconv1, range(self.ngpu))  # 并行通过第二个转置卷积层\n",
    "            output = tconv2                                         # 得到生成的图像\n",
    "        else:\n",
    "            input = input.view(-1, self.nz)                           # 将输入展平为二维张量[batch_size, nz]\n",
    "            fc = self.fc(input)                                       # 通过全连接层\n",
    "            fc = fc.view(-1, 256, 7, 7)                               # 重塑为[batch_size, 256, 7, 7]的特征图\n",
    "            tconv1 = self.tconv1(fc)                                  # 通过第一个转置卷积层\n",
    "            tconv2 = self.tconv2(tconv1)                              # 通过第二个转置卷积层\n",
    "            output = tconv2                                         # 得到生成的图像\n",
    "        return output  # 返回生成的28x28单通道图像\n",
    "\n",
    "# # 定义用于MNIST判别器的网络\n",
    "class _netD_MNIST(nn.Module):\n",
    "    def __init__(self, ngpu, num_classes=2):\n",
    "        super(_netD_MNIST, self).__init__()  # 调用父类的构造函数\n",
    "        self.ngpu = ngpu  # 保存GPU数量\n",
    "\n",
    "        # 卷积层1：将输入1通道图像转换为16通道，输出尺寸为14x14（28x28经步长为2降采样）\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False),  # 卷积操作\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数，负半轴斜率为0.2\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 卷积层2：保持空间尺寸不变，增加通道数到32\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),  # 卷积操作\n",
    "            nn.BatchNorm2d(32),             # 对32个通道做批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 卷积层3：将特征图尺寸减半（14x14→7x7），通道数增至64\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False),  # 卷积操作\n",
    "            nn.BatchNorm2d(64),             # 对64个通道做批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 卷积层4：保持7x7尺寸，通道数增至128\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),  # 卷积操作\n",
    "            nn.BatchNorm2d(128),            # 对128个通道做批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 卷积层5：将特征图尺寸减半（7x7→4x4，因7+2-3=6，6//2+1=4），通道数增至256\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),  # 卷积操作\n",
    "            nn.BatchNorm2d(256),            # 对256个通道做批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 卷积层6：保持特征图尺寸为4x4，通道数增至512\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),  # 卷积操作\n",
    "            nn.BatchNorm2d(512),            # 对512个通道做批归一化\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # LeakyReLU激活函数\n",
    "            nn.Dropout(0.5, inplace=False)      # Dropout防止过拟合\n",
    "        )\n",
    "\n",
    "        # 全连接层：用于判别图像的真伪（discriminator）\n",
    "        self.fc_dis = nn.Linear(4 * 4 * 512, 1)  # 将4x4x512的特征展平后映射到1个输出\n",
    "        # 全连接层：用于辅助分类器，预测图像类别\n",
    "        self.fc_aux = nn.Linear(4 * 4 * 512, num_classes)  # 将特征映射到类别数\n",
    "        # Softmax层：用于计算辅助分类器输出的概率分布\n",
    "        self.softmax = nn.Softmax(dim=1)  # 指定在类别维度上做归一化\n",
    "        # Sigmoid层：用于计算判别器输出的真伪概率\n",
    "        self.sigmoid = nn.Sigmoid()  # 将输出映射到[0,1]之间\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 若使用CUDA且多GPU，则采用数据并行方式\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            conv1 = nn.parallel.data_parallel(self.conv1, input, range(self.ngpu))  # 第一卷积层并行计算\n",
    "            conv2 = nn.parallel.data_parallel(self.conv2, conv1, range(self.ngpu))  # 第二卷积层并行计算\n",
    "            conv3 = nn.parallel.data_parallel(self.conv3, conv2, range(self.ngpu))  # 第三卷积层并行计算\n",
    "            conv4 = nn.parallel.data_parallel(self.conv4, conv3, range(self.ngpu))  # 第四卷积层并行计算\n",
    "            conv5 = nn.parallel.data_parallel(self.conv5, conv4, range(self.ngpu))  # 第五卷积层并行计算\n",
    "            conv6 = nn.parallel.data_parallel(self.conv6, conv5, range(self.ngpu))  # 第六卷积层并行计算\n",
    "            flat6 = conv6.view(-1, 4 * 4 * 512)  # 将卷积输出展平成二维张量\n",
    "            fc_dis = nn.parallel.data_parallel(self.fc_dis, flat6, range(self.ngpu))  # 判别器全连接层并行计算\n",
    "            fc_aux = nn.parallel.data_parallel(self.fc_aux, flat6, range(self.ngpu))  # 辅助分类器全连接层并行计算\n",
    "        else:\n",
    "            conv1 = self.conv1(input)       # 通过第一卷积层\n",
    "            conv2 = self.conv2(conv1)       # 通过第二卷积层\n",
    "            conv3 = self.conv3(conv2)       # 通过第三卷积层\n",
    "            conv4 = self.conv4(conv3)       # 通过第四卷积层\n",
    "            conv5 = self.conv5(conv4)       # 通过第五卷积层\n",
    "            conv6 = self.conv6(conv5)       # 通过第六卷积层\n",
    "            flat6 = conv6.view(-1, 4 * 4 * 512)  # 将卷积输出展平成二维张量\n",
    "            fc_dis = self.fc_dis(flat6)     # 通过判别器全连接层\n",
    "            fc_aux = self.fc_aux(flat6)     # 通过辅助分类器全连接层\n",
    "\n",
    "        classes = self.softmax(fc_aux)  # 计算辅助分类器的类别概率分布\n",
    "        realfake = self.sigmoid(fc_dis).view(-1, 1).squeeze(1)  # 计算判别器的真伪概率，并调整输出形状\n",
    "        return realfake, classes  # 返回真伪判断结果和类别预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # 引入未来版本的打印函数，确保兼容 Python 3 语法\n",
    "import argparse  # 导入 argparse 模块，用于解析命令行参数\n",
    "import os  # 导入 os 模块，用于文件和目录操作\n",
    "import numpy as np  # 导入 numpy 模块并简写为 np，用于数值计算\n",
    "import random  # 导入 random 模块，用于生成随机数\n",
    "import torch  # 导入 PyTorch 库\n",
    "import torch.nn as nn  # 导入 PyTorch 神经网络模块，并简写为 nn\n",
    "import torch.nn.parallel  # 导入 PyTorch 的并行计算模块\n",
    "import torch.backends.cudnn as cudnn  # 导入 cuDNN 后端，用于加速卷积计算\n",
    "import torch.optim as optim  # 导入 PyTorch 优化器模块，并简写为 optim\n",
    "import torch.utils.data  # 导入 PyTorch 数据加载工具模块\n",
    "import torchvision.datasets as dset  # 导入 torchvision 数据集模块，并简写为 dset\n",
    "import torchvision.transforms as transforms  # 导入 torchvision 数据预处理模块，并简写为 transforms\n",
    "import torchvision.utils as vutils  # 导入 torchvision 工具模块，用于处理图像数据，并简写为 vutils\n",
    "#from torch.autograd import Variable  # 从 torch.autograd 模块中导入 Variable 类（用于封装张量，旧版本 PyTorch 使用）\n",
    "from utils import weights_init, compute_acc  # 从 utils 模块导入自定义权重初始化和计算准确率的函数\n",
    "from network import _netG, _netD, _netD_CIFAR10, _netG_CIFAR10, _netD_MNIST, _netG_MNIST  # 从 network 模块导入生成器和判别器网络\n",
    "#from folder import ImageFolder  # 从 folder 模块导入 ImageFolder 类，用于加载自定义图像数据\n",
    "from Get_datasets import get_datasets # 从 Get_datasets 模块导入 get_datasets 函数，用于加载 MNIST-3/4 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  8379\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "# 如果只使用一个 GPU，则指定使用的 GPU ID\n",
    "if args['ngpu'] == 1:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(args['gpu_id'])  # 设置环境变量，只显示指定的 GPU\n",
    "\n",
    "try:\n",
    "    os.makedirs(args['outf'])  # 尝试创建输出文件夹\n",
    "except OSError:\n",
    "    pass  # 如果输出文件夹已存在，则忽略异常\n",
    "\n",
    "if args['manualSeed'] is None:\n",
    "    args['manualSeed'] = random.randint(1, 10000)  # 如果未手动设置随机种子，则随机生成一个\n",
    "print(\"Random Seed: \", args['manualSeed'])  # 打印随机种子\n",
    "random.seed(args['manualSeed'])  # 设置 Python 内置随机数种子\n",
    "torch.manual_seed(args['manualSeed'])  # 设置 PyTorch 随机数种子\n",
    "if args['cuda']:\n",
    "    torch.cuda.manual_seed_all(args['manualSeed'])  # 如果使用 CUDA，则设置所有 GPU 的随机数种子\n",
    "\n",
    "cudnn.benchmark = True  # 启用 cuDNN benchmark 模式以选择最优算法，加快训练\n",
    "\n",
    "if torch.cuda.is_available() and not args['cuda']:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")  # 如果有 CUDA 设备但未启用 CUDA，则发出警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一些超参数\n",
    "ngpu = int(args['ngpu'])  # 获取 GPU 数量\n",
    "nz = int(args['nz'])  # 获取潜在向量 z 的维度\n",
    "ngf = int(args['ngf'])  # 获取生成器特征图基数\n",
    "ndf = int(args['ndf'])  # 获取判别器特征图基数\n",
    "num_classes = int(args['num_classes'])  # 获取 AC-GAN 的类别数\n",
    "nc = 1  # 定义输入图像的通道数（3 表示 RGB 图像）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG_MNIST(\n",
      "  (fc): Linear(in_features=110, out_features=12544, bias=True)\n",
      "  (tconv1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (tconv2): Sequential(\n",
      "    (0): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义生成器网络并初始化权重\n",
    "if args['dataset'] == 'imagenet':\n",
    "    netG = _netG(ngpu, nz)  # 如果数据集为 imagenet，使用 _netG 生成器\n",
    "elif args['dataset'] == 'cifar10':\n",
    "    netG = _netG_CIFAR10(ngpu, nz)  # 如果数据集为 cifar10，使用 _netG_CIFAR10 生成器\n",
    "else:\n",
    "    netG = _netG_MNIST(ngpu, nz)\n",
    "netG.apply(weights_init)  # 对生成器网络中的所有模块应用自定义权重初始化\n",
    "if args['netG'] != '':  # 如果提供了预训练生成器模型路径，则加载模型参数\n",
    "    netG.load_state_dict(torch.load(args['netG']))\n",
    "print(netG)  # 打印生成器网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD_MNIST(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (fc_dis): Linear(in_features=8192, out_features=1, bias=True)\n",
      "  (fc_aux): Linear(in_features=8192, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义判别器网络并初始化权重\n",
    "if args['dataset'] == 'imagenet':\n",
    "    netD = _netD(ngpu, num_classes)  # 如果数据集为 imagenet，使用 _netD 判别器\n",
    "elif args['dataset'] == 'cifar10':\n",
    "    netD = _netD_CIFAR10(ngpu, num_classes)  # 如果数据集为 cifar10，使用 _netD_CIFAR10 判别器\n",
    "else:\n",
    "    netD = _netD_MNIST(ngpu, num_classes) # 如果数据集为 mnist，使用 _netD_MNIST 判别器\n",
    "netD.apply(weights_init)  # 对判别器网络中的所有模块应用自定义权重初始化\n",
    "if args['netD'] != '':  # 如果提供了预训练判别器模型路径，则加载模型参数\n",
    "    netD.load_state_dict(torch.load(args['netD']))\n",
    "print(netD)  # 打印判别器网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "dis_criterion = nn.BCELoss()  # 使用二元交叉熵损失函数计算判别器的真伪损失\n",
    "aux_criterion = nn.NLLLoss()  # 使用负对数似然损失函数计算辅助分类器的损失\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义张量占位符\n",
    "input = torch.FloatTensor(args['batchSize'], 1, args['imageSize'], args['imageSize'])  # 创建存储输入图像的张量，占位尺寸为 [batchSize, 1, imageSize, imageSize]\n",
    "noise = torch.FloatTensor(args['batchSize'], nz, 1, 1)  # 创建存储生成器噪声的张量，占位尺寸为 [batchSize, nz, 1, 1]\n",
    "eval_noise = torch.FloatTensor(args['batchSize'], nz, 1, 1).normal_(0, 1)  # 创建评估用噪声张量，并用标准正态分布初始化\n",
    "dis_label = torch.FloatTensor(args['batchSize'])  # 创建存储判别器标签的张量，占位尺寸为 [batchSize]\n",
    "aux_label = torch.LongTensor(args['batchSize'])  # 创建存储辅助分类标签的张量，占位尺寸为 [batchSize]\n",
    "real_label = 1  # 定义真实图像标签值为 1\n",
    "fake_label = 0  # 定义生成图像标签值为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用 CUDA，则将网络和张量移动到 GPU 上\n",
    "if args['cuda']:\n",
    "    netD.cuda()  # 将判别器移动到 GPU\n",
    "    netG.cuda()  # 将生成器移动到 GPU\n",
    "    dis_criterion.cuda()  # 将判别器损失函数移动到 GPU\n",
    "    aux_criterion.cuda()  # 将辅助分类损失函数移动到 GPU\n",
    "    input, dis_label, aux_label = input.cuda(), dis_label.cuda(), aux_label.cuda()  # 将输入和标签张量移动到 GPU\n",
    "    noise, eval_noise = noise.cuda(), eval_noise.cuda()  # 将噪声张量移动到 GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用 tensor，不再需要封装为 Variable\n",
    "# 假设 input, noise, eval_noise, dis_label, aux_label 本身已经是 Tensor\n",
    "# 如果需要使某个张量支持梯度计算，可以设置 requires_grad=True\n",
    "input = input       # 输入图像张量\n",
    "noise = noise       # 生成器噪声张量\n",
    "eval_noise = eval_noise  # 评估用噪声张量\n",
    "dis_label = dis_label    # 判别器标签张量\n",
    "aux_label = aux_label    # 辅助分类标签张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.2994]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2667]],\n",
       "\n",
       "         [[ 1.6576]],\n",
       "\n",
       "         [[-0.1196]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]],\n",
       "\n",
       "         [[ 0.3003]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3185]],\n",
       "\n",
       "         [[-1.6611]],\n",
       "\n",
       "         [[ 2.7699]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[-1.5375]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.5708]],\n",
       "\n",
       "         [[ 0.0961]],\n",
       "\n",
       "         [[-0.4277]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[-2.1780]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.1493]],\n",
       "\n",
       "         [[ 0.6419]],\n",
       "\n",
       "         [[ 0.3282]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[-0.9491]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.8170]],\n",
       "\n",
       "         [[-0.1978]],\n",
       "\n",
       "         [[-1.2243]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]],\n",
       "\n",
       "         [[ 0.2882]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.6352]],\n",
       "\n",
       "         [[ 1.4089]],\n",
       "\n",
       "         [[ 0.4634]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成评估用的噪声和标签\n",
    "eval_noise_ = np.random.normal(0, 1, (args['batchSize'], nz))  # 使用正态分布生成评估噪声，尺寸为 [batchSize, nz]\n",
    "eval_label = np.random.randint(0, num_classes, args['batchSize'])  # 随机生成评估标签，取值范围在 0 到 num_classes-1\n",
    "eval_onehot = np.zeros((args['batchSize'], num_classes))  # 创建评估用的 one-hot 编码矩阵，尺寸为 [batchSize, num_classes]\n",
    "eval_onehot[np.arange(args['batchSize']), eval_label] = 1  # 根据生成的标签设置 one-hot 编码\n",
    "eval_noise_[np.arange(args['batchSize']), :num_classes] = eval_onehot[np.arange(args['batchSize'])]  # 将 one-hot 编码嵌入评估噪声的前 num_classes 维\n",
    "eval_noise_ = (torch.from_numpy(eval_noise_))  # 将 numpy 数组转换为 PyTorch 张量\n",
    "eval_noise.data.copy_(eval_noise_.view(args['batchSize'], nz, 1, 1))  # 将生成的评估噪声复制到 eval_noise 变量中，并调整形状\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置优化器\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=args['lr'], betas=(args['beta1'], 0.999))  # 使用 Adam 优化器优化判别器参数\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=args['lr'], betas=(args['beta1'], 0.999))  # 使用 Adam 优化器优化生成器参数\n",
    "\n",
    "avg_loss_D = 0.0  # 初始化判别器平均损失为 0\n",
    "avg_loss_G = 0.0  # 初始化生成器平均损失为 0\n",
    "avg_loss_A = 0.0  # 初始化辅助分类器平均准确率为 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/96] Loss_D: 0.5692 (0.5692) Loss_G: 0.2713 (0.2713) D(x): 0.5720 D(G(z)): 0.6057 / 0.4852 Acc: 37.5000 (37.5000)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[0/25][1/96] Loss_D: 0.1606 (0.3649) Loss_G: 0.4167 (0.3440) D(x): 0.5581 D(G(z)): 0.5207 / 0.4508 Acc: 96.8750 (67.1875)\n",
      "[0/25][2/96] Loss_D: 0.0415 (0.2571) Loss_G: 0.3685 (0.3522) D(x): 0.4981 D(G(z)): 0.4859 / 0.4375 Acc: 98.4375 (77.6042)\n",
      "[0/25][3/96] Loss_D: -0.1832 (0.1470) Loss_G: 0.2540 (0.3276) D(x): 0.5158 D(G(z)): 0.4343 / 0.4742 Acc: 100.0000 (83.2031)\n",
      "[0/25][4/96] Loss_D: -0.2536 (0.0669) Loss_G: 0.3681 (0.3357) D(x): 0.5614 D(G(z)): 0.4335 / 0.4453 Acc: 100.0000 (86.5625)\n",
      "[0/25][5/96] Loss_D: -0.2923 (0.0070) Loss_G: 0.5039 (0.3637) D(x): 0.6300 D(G(z)): 0.4570 / 0.4019 Acc: 96.8750 (88.2812)\n",
      "[0/25][6/96] Loss_D: -0.2815 (-0.0342) Loss_G: 0.5403 (0.3890) D(x): 0.5899 D(G(z)): 0.4240 / 0.3961 Acc: 100.0000 (89.9554)\n",
      "[0/25][7/96] Loss_D: -0.6137 (-0.1066) Loss_G: 0.3689 (0.3865) D(x): 0.6473 D(G(z)): 0.3892 / 0.4063 Acc: 100.0000 (91.2109)\n",
      "[0/25][8/96] Loss_D: -0.6363 (-0.1655) Loss_G: 0.5683 (0.4067) D(x): 0.6666 D(G(z)): 0.3613 / 0.3616 Acc: 100.0000 (92.1875)\n",
      "[0/25][9/96] Loss_D: -0.7032 (-0.2193) Loss_G: 0.5487 (0.4209) D(x): 0.6734 D(G(z)): 0.3305 / 0.3529 Acc: 100.0000 (92.9688)\n",
      "[0/25][10/96] Loss_D: -0.6765 (-0.2608) Loss_G: 0.7373 (0.4496) D(x): 0.6787 D(G(z)): 0.3240 / 0.3233 Acc: 100.0000 (93.6080)\n",
      "[0/25][11/96] Loss_D: -0.6995 (-0.2974) Loss_G: 0.7553 (0.4751) D(x): 0.7125 D(G(z)): 0.3296 / 0.3220 Acc: 100.0000 (94.1406)\n",
      "[0/25][12/96] Loss_D: -0.7587 (-0.3329) Loss_G: 0.9089 (0.5085) D(x): 0.7546 D(G(z)): 0.3295 / 0.2904 Acc: 100.0000 (94.5913)\n",
      "[0/25][13/96] Loss_D: -0.7022 (-0.3592) Loss_G: 0.9934 (0.5431) D(x): 0.7299 D(G(z)): 0.3165 / 0.2755 Acc: 100.0000 (94.9777)\n",
      "[0/25][14/96] Loss_D: -0.8243 (-0.3902) Loss_G: 1.1286 (0.5821) D(x): 0.7252 D(G(z)): 0.2736 / 0.2251 Acc: 100.0000 (95.3125)\n",
      "[0/25][15/96] Loss_D: -0.8931 (-0.4217) Loss_G: 1.0964 (0.6143) D(x): 0.7575 D(G(z)): 0.2491 / 0.2361 Acc: 98.4375 (95.5078)\n",
      "[0/25][16/96] Loss_D: -0.8938 (-0.4495) Loss_G: 0.9905 (0.6364) D(x): 0.7511 D(G(z)): 0.2251 / 0.2721 Acc: 98.4375 (95.6801)\n",
      "[0/25][17/96] Loss_D: -0.8707 (-0.4729) Loss_G: 1.1220 (0.6634) D(x): 0.7841 D(G(z)): 0.2681 / 0.2475 Acc: 100.0000 (95.9201)\n",
      "[0/25][18/96] Loss_D: -0.8300 (-0.4917) Loss_G: 1.2329 (0.6934) D(x): 0.7677 D(G(z)): 0.2567 / 0.2410 Acc: 100.0000 (96.1349)\n",
      "[0/25][19/96] Loss_D: -1.0105 (-0.5176) Loss_G: 1.0803 (0.7127) D(x): 0.7609 D(G(z)): 0.2449 / 0.2361 Acc: 100.0000 (96.3281)\n",
      "[0/25][20/96] Loss_D: -0.8512 (-0.5335) Loss_G: 1.1580 (0.7339) D(x): 0.7597 D(G(z)): 0.2175 / 0.2616 Acc: 100.0000 (96.5030)\n",
      "[0/25][21/96] Loss_D: -0.8410 (-0.5475) Loss_G: 1.3866 (0.7636) D(x): 0.8001 D(G(z)): 0.2388 / 0.2232 Acc: 100.0000 (96.6619)\n",
      "[0/25][22/96] Loss_D: -0.8407 (-0.5602) Loss_G: 1.8137 (0.8093) D(x): 0.7978 D(G(z)): 0.2542 / 0.1625 Acc: 100.0000 (96.8071)\n",
      "[0/25][23/96] Loss_D: -0.8839 (-0.5737) Loss_G: 1.5255 (0.8391) D(x): 0.7463 D(G(z)): 0.2150 / 0.2056 Acc: 100.0000 (96.9401)\n",
      "[0/25][24/96] Loss_D: -1.0145 (-0.5913) Loss_G: 1.5035 (0.8657) D(x): 0.8184 D(G(z)): 0.2269 / 0.1818 Acc: 100.0000 (97.0625)\n",
      "[0/25][25/96] Loss_D: -0.9939 (-0.6068) Loss_G: 1.4695 (0.8889) D(x): 0.8136 D(G(z)): 0.2212 / 0.2013 Acc: 98.4375 (97.1154)\n",
      "[0/25][26/96] Loss_D: -0.9914 (-0.6211) Loss_G: 1.5918 (0.9149) D(x): 0.7402 D(G(z)): 0.1336 / 0.1927 Acc: 100.0000 (97.2222)\n",
      "[0/25][27/96] Loss_D: -1.0516 (-0.6364) Loss_G: 1.4879 (0.9354) D(x): 0.8382 D(G(z)): 0.2009 / 0.2079 Acc: 100.0000 (97.3214)\n",
      "[0/25][28/96] Loss_D: -1.0898 (-0.6521) Loss_G: 1.3481 (0.9496) D(x): 0.8219 D(G(z)): 0.1773 / 0.2231 Acc: 100.0000 (97.4138)\n",
      "[0/25][29/96] Loss_D: -0.8433 (-0.6584) Loss_G: 2.0182 (0.9852) D(x): 0.8237 D(G(z)): 0.2409 / 0.1404 Acc: 98.4375 (97.4479)\n",
      "[0/25][30/96] Loss_D: -0.9632 (-0.6683) Loss_G: 1.6980 (1.0082) D(x): 0.7721 D(G(z)): 0.1391 / 0.1998 Acc: 98.4375 (97.4798)\n",
      "[0/25][31/96] Loss_D: -1.0044 (-0.6788) Loss_G: 1.5975 (1.0266) D(x): 0.7802 D(G(z)): 0.1898 / 0.1801 Acc: 100.0000 (97.5586)\n",
      "[0/25][32/96] Loss_D: -0.9467 (-0.6869) Loss_G: 2.0282 (1.0570) D(x): 0.8429 D(G(z)): 0.2511 / 0.1447 Acc: 100.0000 (97.6326)\n",
      "[0/25][33/96] Loss_D: -1.0262 (-0.6969) Loss_G: 2.0680 (1.0867) D(x): 0.7857 D(G(z)): 0.1336 / 0.1483 Acc: 100.0000 (97.7022)\n",
      "[0/25][34/96] Loss_D: -1.0311 (-0.7064) Loss_G: 1.6184 (1.1019) D(x): 0.7972 D(G(z)): 0.1467 / 0.1855 Acc: 98.4375 (97.7232)\n",
      "[0/25][35/96] Loss_D: -1.2589 (-0.7218) Loss_G: 1.6051 (1.1159) D(x): 0.8581 D(G(z)): 0.1823 / 0.1909 Acc: 100.0000 (97.7865)\n",
      "[0/25][36/96] Loss_D: -1.1835 (-0.7343) Loss_G: 2.1981 (1.1452) D(x): 0.8719 D(G(z)): 0.1646 / 0.1343 Acc: 100.0000 (97.8463)\n",
      "[0/25][37/96] Loss_D: -0.9796 (-0.7407) Loss_G: 2.2107 (1.1732) D(x): 0.8047 D(G(z)): 0.1636 / 0.1218 Acc: 100.0000 (97.9030)\n",
      "[0/25][38/96] Loss_D: -1.0512 (-0.7487) Loss_G: 1.6513 (1.1855) D(x): 0.7951 D(G(z)): 0.1567 / 0.1842 Acc: 100.0000 (97.9567)\n",
      "[0/25][39/96] Loss_D: -1.1120 (-0.7578) Loss_G: 2.1534 (1.2096) D(x): 0.8567 D(G(z)): 0.1363 / 0.1736 Acc: 100.0000 (98.0078)\n",
      "[0/25][40/96] Loss_D: -0.9846 (-0.7633) Loss_G: 2.2913 (1.2360) D(x): 0.8502 D(G(z)): 0.1986 / 0.1157 Acc: 100.0000 (98.0564)\n",
      "[0/25][41/96] Loss_D: -1.0577 (-0.7703) Loss_G: 2.2626 (1.2605) D(x): 0.8736 D(G(z)): 0.1628 / 0.1242 Acc: 100.0000 (98.1027)\n",
      "[0/25][42/96] Loss_D: -1.1368 (-0.7788) Loss_G: 2.3460 (1.2857) D(x): 0.8267 D(G(z)): 0.1122 / 0.1351 Acc: 100.0000 (98.1468)\n",
      "[0/25][43/96] Loss_D: -1.0584 (-0.7852) Loss_G: 2.1329 (1.3050) D(x): 0.8552 D(G(z)): 0.1568 / 0.1279 Acc: 100.0000 (98.1889)\n",
      "[0/25][44/96] Loss_D: -1.1387 (-0.7930) Loss_G: 1.8953 (1.3181) D(x): 0.8591 D(G(z)): 0.1733 / 0.1548 Acc: 100.0000 (98.2292)\n",
      "[0/25][45/96] Loss_D: -1.1273 (-0.8003) Loss_G: 2.2898 (1.3392) D(x): 0.8657 D(G(z)): 0.1634 / 0.1300 Acc: 100.0000 (98.2677)\n",
      "[0/25][46/96] Loss_D: -1.2262 (-0.8094) Loss_G: 2.3445 (1.3606) D(x): 0.8855 D(G(z)): 0.1143 / 0.1334 Acc: 100.0000 (98.3045)\n",
      "[0/25][47/96] Loss_D: -1.2462 (-0.8185) Loss_G: 2.9307 (1.3933) D(x): 0.8779 D(G(z)): 0.1547 / 0.0775 Acc: 100.0000 (98.3398)\n",
      "[0/25][48/96] Loss_D: -1.2251 (-0.8268) Loss_G: 2.5397 (1.4167) D(x): 0.8513 D(G(z)): 0.0890 / 0.1100 Acc: 100.0000 (98.3737)\n",
      "[0/25][49/96] Loss_D: -1.2460 (-0.8351) Loss_G: 1.9656 (1.4277) D(x): 0.8780 D(G(z)): 0.1122 / 0.1777 Acc: 100.0000 (98.4062)\n",
      "[0/25][50/96] Loss_D: -1.1080 (-0.8405) Loss_G: 2.5513 (1.4497) D(x): 0.8983 D(G(z)): 0.1678 / 0.1057 Acc: 100.0000 (98.4375)\n",
      "[0/25][51/96] Loss_D: -1.1876 (-0.8472) Loss_G: 2.5577 (1.4710) D(x): 0.8339 D(G(z)): 0.1331 / 0.1058 Acc: 100.0000 (98.4675)\n",
      "[0/25][52/96] Loss_D: -1.2748 (-0.8552) Loss_G: 2.2732 (1.4862) D(x): 0.8557 D(G(z)): 0.1000 / 0.1246 Acc: 100.0000 (98.4965)\n",
      "[0/25][53/96] Loss_D: -1.1566 (-0.8608) Loss_G: 2.8100 (1.5107) D(x): 0.9001 D(G(z)): 0.1616 / 0.0960 Acc: 96.8750 (98.4664)\n",
      "[0/25][54/96] Loss_D: -1.0216 (-0.8637) Loss_G: 3.0401 (1.5385) D(x): 0.8603 D(G(z)): 0.0849 / 0.0804 Acc: 98.4375 (98.4659)\n",
      "[0/25][55/96] Loss_D: -1.3077 (-0.8717) Loss_G: 2.8200 (1.5614) D(x): 0.9042 D(G(z)): 0.0672 / 0.0881 Acc: 98.4375 (98.4654)\n",
      "[0/25][56/96] Loss_D: -1.2293 (-0.8779) Loss_G: 3.0610 (1.5877) D(x): 0.9081 D(G(z)): 0.1049 / 0.0710 Acc: 98.4375 (98.4649)\n",
      "[0/25][57/96] Loss_D: -1.3002 (-0.8852) Loss_G: 3.6743 (1.6237) D(x): 0.9391 D(G(z)): 0.1153 / 0.0392 Acc: 98.4375 (98.4644)\n",
      "[0/25][58/96] Loss_D: -1.3115 (-0.8924) Loss_G: 3.3806 (1.6534) D(x): 0.8831 D(G(z)): 0.0740 / 0.0523 Acc: 98.4375 (98.4640)\n",
      "[0/25][59/96] Loss_D: -1.3221 (-0.8996) Loss_G: 3.1473 (1.6783) D(x): 0.8977 D(G(z)): 0.0558 / 0.0880 Acc: 100.0000 (98.4896)\n",
      "[0/25][60/96] Loss_D: -1.2564 (-0.9055) Loss_G: 3.3476 (1.7057) D(x): 0.9316 D(G(z)): 0.1003 / 0.0730 Acc: 98.4375 (98.4887)\n",
      "[0/25][61/96] Loss_D: -1.1201 (-0.9089) Loss_G: 3.7521 (1.7387) D(x): 0.8889 D(G(z)): 0.1035 / 0.0521 Acc: 98.4375 (98.4879)\n",
      "[0/25][62/96] Loss_D: -1.3898 (-0.9166) Loss_G: 3.7018 (1.7699) D(x): 0.9343 D(G(z)): 0.0802 / 0.0444 Acc: 100.0000 (98.5119)\n",
      "[0/25][63/96] Loss_D: -1.2466 (-0.9217) Loss_G: 3.4946 (1.7968) D(x): 0.9197 D(G(z)): 0.0591 / 0.0559 Acc: 100.0000 (98.5352)\n",
      "[0/25][64/96] Loss_D: -1.1970 (-0.9259) Loss_G: 3.2568 (1.8193) D(x): 0.8854 D(G(z)): 0.0650 / 0.0709 Acc: 100.0000 (98.5577)\n",
      "[0/25][65/96] Loss_D: -1.1419 (-0.9292) Loss_G: 3.5774 (1.8459) D(x): 0.9414 D(G(z)): 0.1066 / 0.0626 Acc: 98.4375 (98.5559)\n",
      "[0/25][66/96] Loss_D: -1.3183 (-0.9350) Loss_G: 4.1203 (1.8799) D(x): 0.9275 D(G(z)): 0.0941 / 0.0446 Acc: 100.0000 (98.5774)\n",
      "[0/25][67/96] Loss_D: -1.2704 (-0.9400) Loss_G: 3.2761 (1.9004) D(x): 0.8841 D(G(z)): 0.0351 / 0.0725 Acc: 98.4375 (98.5754)\n",
      "[0/25][68/96] Loss_D: -1.3813 (-0.9464) Loss_G: 2.7494 (1.9127) D(x): 0.9272 D(G(z)): 0.0749 / 0.1087 Acc: 100.0000 (98.5960)\n",
      "[0/25][69/96] Loss_D: -1.2917 (-0.9513) Loss_G: 3.6038 (1.9369) D(x): 0.9046 D(G(z)): 0.1231 / 0.0589 Acc: 98.4375 (98.5938)\n",
      "[0/25][70/96] Loss_D: -1.3374 (-0.9567) Loss_G: 3.5590 (1.9597) D(x): 0.9259 D(G(z)): 0.0881 / 0.0613 Acc: 100.0000 (98.6136)\n",
      "[0/25][71/96] Loss_D: -1.2443 (-0.9607) Loss_G: 3.1410 (1.9761) D(x): 0.8987 D(G(z)): 0.0623 / 0.0790 Acc: 100.0000 (98.6328)\n",
      "[0/25][72/96] Loss_D: -1.1858 (-0.9638) Loss_G: 2.8790 (1.9885) D(x): 0.8521 D(G(z)): 0.0644 / 0.1057 Acc: 100.0000 (98.6515)\n",
      "[0/25][73/96] Loss_D: -1.0547 (-0.9650) Loss_G: 3.7613 (2.0124) D(x): 0.9321 D(G(z)): 0.2019 / 0.0399 Acc: 98.4375 (98.6486)\n",
      "[0/25][74/96] Loss_D: -1.2953 (-0.9694) Loss_G: 3.3698 (2.0305) D(x): 0.8638 D(G(z)): 0.0396 / 0.0670 Acc: 100.0000 (98.6667)\n",
      "[0/25][75/96] Loss_D: -1.3266 (-0.9741) Loss_G: 3.3598 (2.0480) D(x): 0.9154 D(G(z)): 0.0771 / 0.0862 Acc: 100.0000 (98.6842)\n",
      "[0/25][76/96] Loss_D: -1.1320 (-0.9762) Loss_G: 3.0785 (2.0614) D(x): 0.8899 D(G(z)): 0.0849 / 0.0899 Acc: 98.4375 (98.6810)\n",
      "[0/25][77/96] Loss_D: -1.3683 (-0.9812) Loss_G: 3.3910 (2.0784) D(x): 0.9206 D(G(z)): 0.0759 / 0.0900 Acc: 100.0000 (98.6979)\n",
      "[0/25][78/96] Loss_D: -1.3515 (-0.9859) Loss_G: 3.6821 (2.0987) D(x): 0.9019 D(G(z)): 0.0805 / 0.0622 Acc: 100.0000 (98.7144)\n",
      "[0/25][79/96] Loss_D: -1.3030 (-0.9899) Loss_G: 3.6677 (2.1184) D(x): 0.9140 D(G(z)): 0.0799 / 0.0540 Acc: 100.0000 (98.7305)\n",
      "[0/25][80/96] Loss_D: -1.3928 (-0.9948) Loss_G: 3.5323 (2.1358) D(x): 0.9522 D(G(z)): 0.0692 / 0.0703 Acc: 100.0000 (98.7461)\n",
      "[0/25][81/96] Loss_D: -1.3172 (-0.9988) Loss_G: 4.2280 (2.1613) D(x): 0.9042 D(G(z)): 0.0441 / 0.0321 Acc: 100.0000 (98.7614)\n",
      "[0/25][82/96] Loss_D: -1.1916 (-1.0011) Loss_G: 4.2697 (2.1867) D(x): 0.9274 D(G(z)): 0.0810 / 0.0465 Acc: 100.0000 (98.7764)\n",
      "[0/25][83/96] Loss_D: -1.3855 (-1.0057) Loss_G: 4.3949 (2.2130) D(x): 0.9420 D(G(z)): 0.0638 / 0.0456 Acc: 98.4375 (98.7723)\n",
      "[0/25][84/96] Loss_D: -1.3365 (-1.0096) Loss_G: 4.8995 (2.2446) D(x): 0.9427 D(G(z)): 0.0546 / 0.0143 Acc: 98.4375 (98.7684)\n",
      "[0/25][85/96] Loss_D: -1.2164 (-1.0120) Loss_G: 3.1722 (2.2554) D(x): 0.8494 D(G(z)): 0.0372 / 0.1019 Acc: 100.0000 (98.7827)\n",
      "[0/25][86/96] Loss_D: -1.2844 (-1.0151) Loss_G: 4.6482 (2.2829) D(x): 0.9574 D(G(z)): 0.1505 / 0.0254 Acc: 100.0000 (98.7967)\n",
      "[0/25][87/96] Loss_D: -1.3738 (-1.0192) Loss_G: 4.7508 (2.3110) D(x): 0.9352 D(G(z)): 0.0637 / 0.0168 Acc: 100.0000 (98.8104)\n",
      "[0/25][88/96] Loss_D: -1.3466 (-1.0229) Loss_G: 4.6237 (2.3369) D(x): 0.9407 D(G(z)): 0.0165 / 0.0240 Acc: 100.0000 (98.8237)\n",
      "[0/25][89/96] Loss_D: -1.2805 (-1.0257) Loss_G: 3.9395 (2.3548) D(x): 0.8882 D(G(z)): 0.0256 / 0.0566 Acc: 98.4375 (98.8194)\n",
      "[0/25][90/96] Loss_D: -1.2540 (-1.0282) Loss_G: 4.2984 (2.3761) D(x): 0.9478 D(G(z)): 0.0899 / 0.0509 Acc: 96.8750 (98.7981)\n",
      "[0/25][91/96] Loss_D: -1.4714 (-1.0330) Loss_G: 4.2072 (2.3960) D(x): 0.9418 D(G(z)): 0.0800 / 0.0555 Acc: 100.0000 (98.8111)\n",
      "[0/25][92/96] Loss_D: -1.5144 (-1.0382) Loss_G: 5.1871 (2.4260) D(x): 0.9623 D(G(z)): 0.0364 / 0.0180 Acc: 98.4375 (98.8071)\n",
      "[0/25][93/96] Loss_D: -1.4390 (-1.0425) Loss_G: 5.0248 (2.4537) D(x): 0.9657 D(G(z)): 0.0228 / 0.0236 Acc: 100.0000 (98.8198)\n",
      "[0/25][94/96] Loss_D: -1.4018 (-1.0463) Loss_G: 4.3592 (2.4737) D(x): 0.9550 D(G(z)): 0.0363 / 0.0626 Acc: 100.0000 (98.8322)\n",
      "[0/25][95/96] Loss_D: -1.2574 (-1.0485) Loss_G: 4.4757 (2.4946) D(x): 0.9333 D(G(z)): 0.0217 / 0.0638 Acc: 98.4375 (98.8281)\n",
      "[1/25][0/96] Loss_D: -1.2300 (-1.0503) Loss_G: 3.4346 (2.5043) D(x): 0.9182 D(G(z)): 0.0552 / 0.0886 Acc: 100.0000 (98.8402)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[1/25][1/96] Loss_D: -1.2725 (-1.0526) Loss_G: 4.3204 (2.5228) D(x): 0.9585 D(G(z)): 0.0786 / 0.0444 Acc: 100.0000 (98.8520)\n",
      "[1/25][2/96] Loss_D: -1.2528 (-1.0546) Loss_G: 4.4374 (2.5421) D(x): 0.9331 D(G(z)): 0.0543 / 0.0486 Acc: 100.0000 (98.8636)\n",
      "[1/25][3/96] Loss_D: -1.3505 (-1.0576) Loss_G: 4.2564 (2.5593) D(x): 0.9438 D(G(z)): 0.0529 / 0.0436 Acc: 98.4375 (98.8594)\n",
      "[1/25][4/96] Loss_D: -1.3256 (-1.0602) Loss_G: 5.7037 (2.5904) D(x): 0.9498 D(G(z)): 0.1097 / 0.0138 Acc: 100.0000 (98.8707)\n",
      "[1/25][5/96] Loss_D: -1.3573 (-1.0631) Loss_G: 3.1356 (2.5958) D(x): 0.8502 D(G(z)): 0.0100 / 0.0904 Acc: 100.0000 (98.8817)\n",
      "[1/25][6/96] Loss_D: -1.3910 (-1.0663) Loss_G: 3.8017 (2.6075) D(x): 0.9711 D(G(z)): 0.0797 / 0.0566 Acc: 100.0000 (98.8926)\n",
      "[1/25][7/96] Loss_D: -1.3294 (-1.0689) Loss_G: 4.8715 (2.6292) D(x): 0.9518 D(G(z)): 0.1002 / 0.0390 Acc: 100.0000 (98.9032)\n",
      "[1/25][8/96] Loss_D: -1.2264 (-1.0704) Loss_G: 4.5491 (2.6475) D(x): 0.8884 D(G(z)): 0.0275 / 0.0304 Acc: 96.8750 (98.8839)\n",
      "[1/25][9/96] Loss_D: -1.2904 (-1.0724) Loss_G: 3.4973 (2.6555) D(x): 0.9125 D(G(z)): 0.0351 / 0.1328 Acc: 100.0000 (98.8945)\n",
      "[1/25][10/96] Loss_D: -1.3252 (-1.0748) Loss_G: 4.4659 (2.6725) D(x): 0.9894 D(G(z)): 0.0849 / 0.0371 Acc: 100.0000 (98.9048)\n",
      "[1/25][11/96] Loss_D: -1.3550 (-1.0774) Loss_G: 5.4115 (2.6978) D(x): 0.9580 D(G(z)): 0.0515 / 0.0132 Acc: 98.4375 (98.9005)\n",
      "[1/25][12/96] Loss_D: -1.3106 (-1.0795) Loss_G: 5.5668 (2.7241) D(x): 0.9443 D(G(z)): 0.0275 / 0.0289 Acc: 98.4375 (98.8962)\n",
      "[1/25][13/96] Loss_D: -1.3165 (-1.0817) Loss_G: 4.6238 (2.7414) D(x): 0.9178 D(G(z)): 0.0104 / 0.0221 Acc: 100.0000 (98.9062)\n",
      "[1/25][14/96] Loss_D: -1.4014 (-1.0846) Loss_G: 4.0719 (2.7534) D(x): 0.9630 D(G(z)): 0.0479 / 0.0607 Acc: 100.0000 (98.9161)\n",
      "[1/25][15/96] Loss_D: -1.3115 (-1.0866) Loss_G: 5.6490 (2.7793) D(x): 0.9740 D(G(z)): 0.0925 / 0.0172 Acc: 100.0000 (98.9258)\n",
      "[1/25][16/96] Loss_D: -1.3934 (-1.0893) Loss_G: 5.2621 (2.8012) D(x): 0.9232 D(G(z)): 0.0239 / 0.0237 Acc: 100.0000 (98.9353)\n",
      "[1/25][17/96] Loss_D: -1.4537 (-1.0925) Loss_G: 4.2956 (2.8143) D(x): 0.9540 D(G(z)): 0.0231 / 0.0526 Acc: 100.0000 (98.9446)\n",
      "[1/25][18/96] Loss_D: -1.3937 (-1.0951) Loss_G: 4.8735 (2.8322) D(x): 0.9649 D(G(z)): 0.0330 / 0.0239 Acc: 100.0000 (98.9538)\n",
      "[1/25][19/96] Loss_D: -1.3937 (-1.0977) Loss_G: 5.4299 (2.8546) D(x): 0.9847 D(G(z)): 0.0541 / 0.0273 Acc: 98.4375 (98.9494)\n",
      "[1/25][20/96] Loss_D: -1.3139 (-1.0995) Loss_G: 4.7072 (2.8705) D(x): 0.9090 D(G(z)): 0.0126 / 0.0522 Acc: 100.0000 (98.9583)\n",
      "[1/25][21/96] Loss_D: -1.3102 (-1.1013) Loss_G: 5.3982 (2.8919) D(x): 0.9691 D(G(z)): 0.0690 / 0.0194 Acc: 100.0000 (98.9672)\n",
      "[1/25][22/96] Loss_D: -1.4287 (-1.1041) Loss_G: 6.4520 (2.9218) D(x): 0.9742 D(G(z)): 0.0451 / 0.0074 Acc: 100.0000 (98.9758)\n",
      "[1/25][23/96] Loss_D: -1.3381 (-1.1060) Loss_G: 4.8253 (2.9377) D(x): 0.9101 D(G(z)): 0.0336 / 0.0469 Acc: 100.0000 (98.9844)\n",
      "[1/25][24/96] Loss_D: -1.3209 (-1.1078) Loss_G: 5.1020 (2.9556) D(x): 0.9373 D(G(z)): 0.0570 / 0.0288 Acc: 98.4375 (98.9799)\n",
      "[1/25][25/96] Loss_D: -1.4371 (-1.1105) Loss_G: 5.4509 (2.9760) D(x): 0.9781 D(G(z)): 0.0343 / 0.0247 Acc: 98.4375 (98.9754)\n",
      "[1/25][26/96] Loss_D: -1.3629 (-1.1126) Loss_G: 5.6358 (2.9976) D(x): 0.9726 D(G(z)): 0.0292 / 0.0239 Acc: 100.0000 (98.9837)\n",
      "[1/25][27/96] Loss_D: -1.3301 (-1.1143) Loss_G: 5.2500 (3.0158) D(x): 0.9548 D(G(z)): 0.0167 / 0.0324 Acc: 98.4375 (98.9793)\n",
      "[1/25][28/96] Loss_D: -1.3344 (-1.1161) Loss_G: 5.3530 (3.0345) D(x): 0.9551 D(G(z)): 0.0273 / 0.0268 Acc: 100.0000 (98.9875)\n",
      "[1/25][29/96] Loss_D: -1.5158 (-1.1192) Loss_G: 6.1560 (3.0593) D(x): 0.9893 D(G(z)): 0.0405 / 0.0115 Acc: 100.0000 (98.9955)\n",
      "[1/25][30/96] Loss_D: -1.5395 (-1.1226) Loss_G: 6.5570 (3.0868) D(x): 0.9692 D(G(z)): 0.0368 / 0.0127 Acc: 100.0000 (99.0034)\n",
      "[1/25][31/96] Loss_D: -1.2444 (-1.1235) Loss_G: 4.3442 (3.0966) D(x): 0.8937 D(G(z)): 0.0147 / 0.0537 Acc: 100.0000 (99.0112)\n",
      "[1/25][32/96] Loss_D: -1.4696 (-1.1262) Loss_G: 5.1102 (3.1122) D(x): 0.9772 D(G(z)): 0.0904 / 0.0335 Acc: 100.0000 (99.0189)\n",
      "[1/25][33/96] Loss_D: -1.3651 (-1.1280) Loss_G: 6.1685 (3.1358) D(x): 0.9443 D(G(z)): 0.0349 / 0.0121 Acc: 100.0000 (99.0264)\n",
      "[1/25][34/96] Loss_D: -1.4105 (-1.1302) Loss_G: 5.9266 (3.1571) D(x): 0.9495 D(G(z)): 0.0218 / 0.0333 Acc: 100.0000 (99.0339)\n",
      "[1/25][35/96] Loss_D: -1.2807 (-1.1313) Loss_G: 4.6387 (3.1683) D(x): 0.9256 D(G(z)): 0.0358 / 0.0669 Acc: 98.4375 (99.0294)\n",
      "[1/25][36/96] Loss_D: -1.4259 (-1.1335) Loss_G: 4.4647 (3.1780) D(x): 0.9660 D(G(z)): 0.0353 / 0.0452 Acc: 100.0000 (99.0367)\n",
      "[1/25][37/96] Loss_D: -1.3833 (-1.1354) Loss_G: 6.3630 (3.2018) D(x): 0.9787 D(G(z)): 0.0498 / 0.0116 Acc: 100.0000 (99.0438)\n",
      "[1/25][38/96] Loss_D: -1.3513 (-1.1370) Loss_G: 4.6993 (3.2129) D(x): 0.9288 D(G(z)): 0.0206 / 0.0735 Acc: 100.0000 (99.0509)\n",
      "[1/25][39/96] Loss_D: -1.4345 (-1.1392) Loss_G: 6.5047 (3.2371) D(x): 0.9710 D(G(z)): 0.0753 / 0.0203 Acc: 98.4375 (99.0464)\n",
      "[1/25][40/96] Loss_D: -1.3640 (-1.1408) Loss_G: 5.7184 (3.2552) D(x): 0.9369 D(G(z)): 0.0144 / 0.0154 Acc: 100.0000 (99.0534)\n",
      "[1/25][41/96] Loss_D: -1.3728 (-1.1425) Loss_G: 4.8900 (3.2671) D(x): 0.9607 D(G(z)): 0.0079 / 0.0579 Acc: 96.8750 (99.0376)\n",
      "[1/25][42/96] Loss_D: -1.4517 (-1.1447) Loss_G: 4.9537 (3.2792) D(x): 0.9726 D(G(z)): 0.0518 / 0.0326 Acc: 98.4375 (99.0333)\n",
      "[1/25][43/96] Loss_D: -1.3949 (-1.1465) Loss_G: 4.3133 (3.2866) D(x): 0.9435 D(G(z)): 0.0289 / 0.0535 Acc: 100.0000 (99.0402)\n",
      "[1/25][44/96] Loss_D: -1.3518 (-1.1480) Loss_G: 6.2992 (3.3079) D(x): 0.9717 D(G(z)): 0.0547 / 0.0279 Acc: 98.4375 (99.0359)\n",
      "[1/25][45/96] Loss_D: -1.4063 (-1.1498) Loss_G: 5.3588 (3.3224) D(x): 0.9408 D(G(z)): 0.0221 / 0.0346 Acc: 100.0000 (99.0427)\n",
      "[1/25][46/96] Loss_D: -1.4366 (-1.1518) Loss_G: 5.1246 (3.3350) D(x): 0.9743 D(G(z)): 0.0275 / 0.0332 Acc: 100.0000 (99.0494)\n",
      "[1/25][47/96] Loss_D: -1.4374 (-1.1538) Loss_G: 6.1999 (3.3549) D(x): 0.9695 D(G(z)): 0.0498 / 0.0149 Acc: 100.0000 (99.0560)\n",
      "[1/25][48/96] Loss_D: -1.5474 (-1.1565) Loss_G: 6.4529 (3.3762) D(x): 0.9654 D(G(z)): 0.0205 / 0.0129 Acc: 100.0000 (99.0625)\n",
      "[1/25][49/96] Loss_D: -1.4127 (-1.1583) Loss_G: 5.0809 (3.3879) D(x): 0.9530 D(G(z)): 0.0139 / 0.0659 Acc: 100.0000 (99.0689)\n",
      "[1/25][50/96] Loss_D: -1.5005 (-1.1606) Loss_G: 4.4823 (3.3954) D(x): 0.9634 D(G(z)): 0.0431 / 0.0515 Acc: 98.4375 (99.0646)\n",
      "[1/25][51/96] Loss_D: -1.3964 (-1.1622) Loss_G: 7.3497 (3.4221) D(x): 0.9864 D(G(z)): 0.0663 / 0.0056 Acc: 100.0000 (99.0709)\n",
      "[1/25][52/96] Loss_D: -1.3175 (-1.1632) Loss_G: 5.2474 (3.4343) D(x): 0.9281 D(G(z)): 0.0056 / 0.0392 Acc: 98.4375 (99.0667)\n",
      "[1/25][53/96] Loss_D: -1.3869 (-1.1647) Loss_G: 6.1938 (3.4527) D(x): 0.9830 D(G(z)): 0.0627 / 0.0346 Acc: 100.0000 (99.0729)\n",
      "[1/25][54/96] Loss_D: -1.3700 (-1.1661) Loss_G: 5.7867 (3.4682) D(x): 0.9514 D(G(z)): 0.0078 / 0.0167 Acc: 100.0000 (99.0791)\n",
      "[1/25][55/96] Loss_D: -1.3989 (-1.1676) Loss_G: 4.2003 (3.4730) D(x): 0.9724 D(G(z)): 0.0202 / 0.1125 Acc: 96.8750 (99.0646)\n",
      "[1/25][56/96] Loss_D: -1.3241 (-1.1686) Loss_G: 5.4944 (3.4862) D(x): 0.9613 D(G(z)): 0.0666 / 0.0185 Acc: 100.0000 (99.0707)\n",
      "[1/25][57/96] Loss_D: -1.3301 (-1.1697) Loss_G: 4.8122 (3.4948) D(x): 0.9335 D(G(z)): 0.0130 / 0.0494 Acc: 100.0000 (99.0767)\n",
      "[1/25][58/96] Loss_D: -1.2607 (-1.1703) Loss_G: 5.5798 (3.5083) D(x): 0.9838 D(G(z)): 0.0296 / 0.0169 Acc: 98.4375 (99.0726)\n",
      "[1/25][59/96] Loss_D: -1.3076 (-1.1711) Loss_G: 6.9200 (3.5301) D(x): 0.9637 D(G(z)): 0.0715 / 0.0041 Acc: 100.0000 (99.0785)\n",
      "[1/25][60/96] Loss_D: -1.3436 (-1.1722) Loss_G: 5.6898 (3.5439) D(x): 0.9513 D(G(z)): 0.0049 / 0.0208 Acc: 100.0000 (99.0844)\n",
      "[1/25][61/96] Loss_D: -1.4487 (-1.1740) Loss_G: 4.8688 (3.5523) D(x): 0.9501 D(G(z)): 0.0322 / 0.0555 Acc: 100.0000 (99.0902)\n",
      "[1/25][62/96] Loss_D: -1.3965 (-1.1754) Loss_G: 4.4964 (3.5582) D(x): 0.9730 D(G(z)): 0.0386 / 0.0460 Acc: 100.0000 (99.0959)\n",
      "[1/25][63/96] Loss_D: -1.4529 (-1.1771) Loss_G: 6.4033 (3.5760) D(x): 0.9720 D(G(z)): 0.0399 / 0.0083 Acc: 100.0000 (99.1016)\n",
      "[1/25][64/96] Loss_D: -1.2736 (-1.1777) Loss_G: 5.0859 (3.5854) D(x): 0.9166 D(G(z)): 0.0162 / 0.0347 Acc: 100.0000 (99.1071)\n",
      "[1/25][65/96] Loss_D: -1.2937 (-1.1784) Loss_G: 6.0956 (3.6009) D(x): 0.9943 D(G(z)): 0.0679 / 0.0141 Acc: 100.0000 (99.1127)\n",
      "[1/25][66/96] Loss_D: -1.4942 (-1.1804) Loss_G: 7.2034 (3.6230) D(x): 0.9696 D(G(z)): 0.0273 / 0.0130 Acc: 100.0000 (99.1181)\n",
      "[1/25][67/96] Loss_D: -1.4045 (-1.1817) Loss_G: 6.2344 (3.6389) D(x): 0.9466 D(G(z)): 0.0143 / 0.0112 Acc: 100.0000 (99.1235)\n",
      "[1/25][68/96] Loss_D: -1.3958 (-1.1830) Loss_G: 3.7041 (3.6393) D(x): 0.9338 D(G(z)): 0.0339 / 0.0961 Acc: 100.0000 (99.1288)\n",
      "[1/25][69/96] Loss_D: -1.2211 (-1.1833) Loss_G: 6.8805 (3.6588) D(x): 0.9542 D(G(z)): 0.0959 / 0.0093 Acc: 100.0000 (99.1340)\n",
      "[1/25][70/96] Loss_D: -1.3133 (-1.1840) Loss_G: 4.8239 (3.6658) D(x): 0.9211 D(G(z)): 0.0174 / 0.0502 Acc: 98.4375 (99.1299)\n",
      "[1/25][71/96] Loss_D: -1.3997 (-1.1853) Loss_G: 4.5285 (3.6709) D(x): 0.9742 D(G(z)): 0.0366 / 0.0630 Acc: 98.4375 (99.1257)\n",
      "[1/25][72/96] Loss_D: -1.3961 (-1.1866) Loss_G: 7.1296 (3.6914) D(x): 0.9892 D(G(z)): 0.0787 / 0.0077 Acc: 96.8750 (99.1124)\n",
      "[1/25][73/96] Loss_D: -1.3730 (-1.1877) Loss_G: 5.3468 (3.7011) D(x): 0.8894 D(G(z)): 0.0042 / 0.0340 Acc: 100.0000 (99.1176)\n",
      "[1/25][74/96] Loss_D: -1.4434 (-1.1892) Loss_G: 4.5870 (3.7063) D(x): 0.9854 D(G(z)): 0.0480 / 0.0406 Acc: 100.0000 (99.1228)\n",
      "[1/25][75/96] Loss_D: -1.4459 (-1.1907) Loss_G: 6.2205 (3.7209) D(x): 0.9914 D(G(z)): 0.0351 / 0.0239 Acc: 100.0000 (99.1279)\n",
      "[1/25][76/96] Loss_D: -1.6137 (-1.1931) Loss_G: 5.4958 (3.7312) D(x): 0.9732 D(G(z)): 0.0124 / 0.0431 Acc: 100.0000 (99.1329)\n",
      "[1/25][77/96] Loss_D: -1.3503 (-1.1940) Loss_G: 3.2339 (3.7283) D(x): 0.9194 D(G(z)): 0.0213 / 0.1239 Acc: 100.0000 (99.1379)\n",
      "[1/25][78/96] Loss_D: -1.3554 (-1.1949) Loss_G: 4.5582 (3.7331) D(x): 0.9698 D(G(z)): 0.0759 / 0.0534 Acc: 100.0000 (99.1429)\n",
      "[1/25][79/96] Loss_D: -1.3055 (-1.1956) Loss_G: 7.4975 (3.7545) D(x): 0.9852 D(G(z)): 0.0630 / 0.0049 Acc: 98.4375 (99.1388)\n",
      "[1/25][80/96] Loss_D: -1.1778 (-1.1955) Loss_G: 4.7405 (3.7600) D(x): 0.8692 D(G(z)): 0.0078 / 0.0676 Acc: 100.0000 (99.1437)\n",
      "[1/25][81/96] Loss_D: -1.3304 (-1.1962) Loss_G: 5.3053 (3.7687) D(x): 0.9944 D(G(z)): 0.0668 / 0.0334 Acc: 98.4375 (99.1397)\n",
      "[1/25][82/96] Loss_D: -1.2790 (-1.1967) Loss_G: 8.5442 (3.7954) D(x): 0.9865 D(G(z)): 0.1032 / 0.0013 Acc: 100.0000 (99.1446)\n",
      "[1/25][83/96] Loss_D: -0.9871 (-1.1955) Loss_G: 0.1964 (3.7754) D(x): 0.6840 D(G(z)): 0.0013 / 0.6488 Acc: 100.0000 (99.1493)\n",
      "[1/25][84/96] Loss_D: -0.0591 (-1.1892) Loss_G: 10.5779 (3.8130) D(x): 0.9993 D(G(z)): 0.5314 / 0.0003 Acc: 100.0000 (99.1540)\n",
      "[1/25][85/96] Loss_D: -0.7270 (-1.1867) Loss_G: 8.5516 (3.8390) D(x): 0.6876 D(G(z)): 0.0004 / 0.0096 Acc: 100.0000 (99.1587)\n",
      "[1/25][86/96] Loss_D: -1.2561 (-1.1871) Loss_G: 5.1685 (3.8463) D(x): 0.8967 D(G(z)): 0.0062 / 0.0890 Acc: 96.8750 (99.1462)\n",
      "[1/25][87/96] Loss_D: -1.4634 (-1.1886) Loss_G: 3.9542 (3.8469) D(x): 0.9868 D(G(z)): 0.0444 / 0.0789 Acc: 100.0000 (99.1508)\n",
      "[1/25][88/96] Loss_D: -1.0998 (-1.1881) Loss_G: 7.4790 (3.8665) D(x): 0.9839 D(G(z)): 0.1525 / 0.0035 Acc: 96.8750 (99.1385)\n",
      "[1/25][89/96] Loss_D: -1.3587 (-1.1890) Loss_G: 7.5000 (3.8860) D(x): 0.9462 D(G(z)): 0.0078 / 0.0069 Acc: 100.0000 (99.1431)\n",
      "[1/25][90/96] Loss_D: -1.3292 (-1.1898) Loss_G: 6.8147 (3.9017) D(x): 0.9341 D(G(z)): 0.0050 / 0.0105 Acc: 100.0000 (99.1477)\n",
      "[1/25][91/96] Loss_D: -1.3538 (-1.1906) Loss_G: 4.9078 (3.9071) D(x): 0.9522 D(G(z)): 0.0111 / 0.0567 Acc: 100.0000 (99.1523)\n",
      "[1/25][92/96] Loss_D: -1.4444 (-1.1920) Loss_G: 5.4852 (3.9154) D(x): 0.9902 D(G(z)): 0.0684 / 0.0528 Acc: 100.0000 (99.1567)\n",
      "[1/25][93/96] Loss_D: -1.5353 (-1.1938) Loss_G: 6.7357 (3.9303) D(x): 0.9772 D(G(z)): 0.0438 / 0.0348 Acc: 100.0000 (99.1612)\n",
      "[1/25][94/96] Loss_D: -1.3596 (-1.1947) Loss_G: 6.3263 (3.9428) D(x): 0.9595 D(G(z)): 0.0160 / 0.0107 Acc: 100.0000 (99.1656)\n",
      "[1/25][95/96] Loss_D: -1.3077 (-1.1952) Loss_G: 5.6114 (3.9515) D(x): 0.9280 D(G(z)): 0.0084 / 0.0327 Acc: 100.0000 (99.1699)\n",
      "[2/25][0/96] Loss_D: -1.4228 (-1.1964) Loss_G: 5.4820 (3.9594) D(x): 0.9822 D(G(z)): 0.0470 / 0.0409 Acc: 98.4375 (99.1661)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[2/25][1/96] Loss_D: -1.3778 (-1.1974) Loss_G: 7.0507 (3.9754) D(x): 0.9739 D(G(z)): 0.0540 / 0.0041 Acc: 98.4375 (99.1624)\n",
      "[2/25][2/96] Loss_D: -1.3230 (-1.1980) Loss_G: 6.1295 (3.9864) D(x): 0.9587 D(G(z)): 0.0262 / 0.0151 Acc: 98.4375 (99.1587)\n",
      "[2/25][3/96] Loss_D: -1.4411 (-1.1992) Loss_G: 5.9821 (3.9966) D(x): 0.9475 D(G(z)): 0.0111 / 0.0267 Acc: 98.4375 (99.1550)\n",
      "[2/25][4/96] Loss_D: -1.4712 (-1.2006) Loss_G: 5.2003 (4.0027) D(x): 0.9596 D(G(z)): 0.0194 / 0.0220 Acc: 100.0000 (99.1593)\n",
      "[2/25][5/96] Loss_D: -1.3957 (-1.2016) Loss_G: 5.9455 (4.0125) D(x): 0.9796 D(G(z)): 0.0544 / 0.0422 Acc: 96.8750 (99.1477)\n",
      "[2/25][6/96] Loss_D: -1.3011 (-1.2021) Loss_G: 7.4310 (4.0297) D(x): 0.9551 D(G(z)): 0.0510 / 0.0224 Acc: 100.0000 (99.1520)\n",
      "[2/25][7/96] Loss_D: -1.5532 (-1.2039) Loss_G: 6.8745 (4.0439) D(x): 0.9757 D(G(z)): 0.0144 / 0.0092 Acc: 100.0000 (99.1563)\n",
      "[2/25][8/96] Loss_D: -1.4127 (-1.2049) Loss_G: 6.2232 (4.0547) D(x): 0.9660 D(G(z)): 0.0296 / 0.0282 Acc: 100.0000 (99.1604)\n",
      "[2/25][9/96] Loss_D: -1.3755 (-1.2058) Loss_G: 7.0628 (4.0696) D(x): 0.9734 D(G(z)): 0.0265 / 0.0209 Acc: 96.8750 (99.1491)\n",
      "[2/25][10/96] Loss_D: -1.5932 (-1.2077) Loss_G: 7.1903 (4.0850) D(x): 0.9786 D(G(z)): 0.0081 / 0.0068 Acc: 100.0000 (99.1533)\n",
      "[2/25][11/96] Loss_D: -1.2780 (-1.2080) Loss_G: 5.4923 (4.0919) D(x): 0.9356 D(G(z)): 0.0363 / 0.0321 Acc: 100.0000 (99.1575)\n",
      "[2/25][12/96] Loss_D: -1.4330 (-1.2091) Loss_G: 6.3459 (4.1029) D(x): 0.9859 D(G(z)): 0.0303 / 0.0161 Acc: 100.0000 (99.1616)\n",
      "[2/25][13/96] Loss_D: -1.3691 (-1.2099) Loss_G: 6.5597 (4.1148) D(x): 0.9820 D(G(z)): 0.0281 / 0.0256 Acc: 98.4375 (99.1581)\n",
      "[2/25][14/96] Loss_D: -1.3752 (-1.2107) Loss_G: 6.7163 (4.1274) D(x): 0.9698 D(G(z)): 0.0466 / 0.0143 Acc: 100.0000 (99.1621)\n",
      "[2/25][15/96] Loss_D: -1.5282 (-1.2122) Loss_G: 5.9210 (4.1360) D(x): 0.9527 D(G(z)): 0.0065 / 0.0276 Acc: 100.0000 (99.1662)\n",
      "[2/25][16/96] Loss_D: -1.3241 (-1.2127) Loss_G: 6.1098 (4.1455) D(x): 0.9468 D(G(z)): 0.0391 / 0.0224 Acc: 100.0000 (99.1702)\n",
      "[2/25][17/96] Loss_D: -1.2349 (-1.2128) Loss_G: 6.1547 (4.1550) D(x): 0.9547 D(G(z)): 0.0434 / 0.0249 Acc: 96.8750 (99.1592)\n",
      "[2/25][18/96] Loss_D: -1.3145 (-1.2133) Loss_G: 6.4275 (4.1658) D(x): 0.9347 D(G(z)): 0.0437 / 0.0262 Acc: 100.0000 (99.1632)\n",
      "[2/25][19/96] Loss_D: -1.4696 (-1.2145) Loss_G: 5.2683 (4.1710) D(x): 0.9749 D(G(z)): 0.0144 / 0.0778 Acc: 100.0000 (99.1672)\n",
      "[2/25][20/96] Loss_D: -1.3462 (-1.2152) Loss_G: 5.8634 (4.1790) D(x): 0.9902 D(G(z)): 0.0412 / 0.0441 Acc: 98.4375 (99.1637)\n",
      "[2/25][21/96] Loss_D: -1.3769 (-1.2159) Loss_G: 6.5707 (4.1901) D(x): 0.9693 D(G(z)): 0.0249 / 0.0190 Acc: 100.0000 (99.1676)\n",
      "[2/25][22/96] Loss_D: -1.2745 (-1.2162) Loss_G: 6.0009 (4.1986) D(x): 0.9267 D(G(z)): 0.0107 / 0.0182 Acc: 100.0000 (99.1715)\n",
      "[2/25][23/96] Loss_D: -1.4473 (-1.2173) Loss_G: 4.2475 (4.1988) D(x): 0.9807 D(G(z)): 0.0143 / 0.0938 Acc: 100.0000 (99.1753)\n",
      "[2/25][24/96] Loss_D: -1.3895 (-1.2180) Loss_G: 6.8394 (4.2109) D(x): 0.9788 D(G(z)): 0.0773 / 0.0101 Acc: 100.0000 (99.1791)\n",
      "[2/25][25/96] Loss_D: -1.4489 (-1.2191) Loss_G: 6.9074 (4.2233) D(x): 0.9800 D(G(z)): 0.0400 / 0.0098 Acc: 100.0000 (99.1829)\n",
      "[2/25][26/96] Loss_D: -1.3022 (-1.2195) Loss_G: 7.2849 (4.2373) D(x): 0.9500 D(G(z)): 0.0154 / 0.0132 Acc: 100.0000 (99.1866)\n",
      "[2/25][27/96] Loss_D: -1.5021 (-1.2208) Loss_G: 6.7350 (4.2486) D(x): 0.9660 D(G(z)): 0.0218 / 0.0230 Acc: 100.0000 (99.1903)\n",
      "[2/25][28/96] Loss_D: -1.3763 (-1.2215) Loss_G: 6.3521 (4.2582) D(x): 0.9732 D(G(z)): 0.0090 / 0.0263 Acc: 100.0000 (99.1940)\n",
      "[2/25][29/96] Loss_D: -1.5418 (-1.2229) Loss_G: 5.4902 (4.2637) D(x): 0.9738 D(G(z)): 0.0179 / 0.0296 Acc: 100.0000 (99.1976)\n",
      "[2/25][30/96] Loss_D: -1.3036 (-1.2233) Loss_G: 6.1677 (4.2723) D(x): 0.9746 D(G(z)): 0.0320 / 0.0274 Acc: 100.0000 (99.2012)\n",
      "[2/25][31/96] Loss_D: -1.2585 (-1.2234) Loss_G: 8.2543 (4.2900) D(x): 0.9845 D(G(z)): 0.0690 / 0.0096 Acc: 98.4375 (99.1978)\n",
      "[2/25][32/96] Loss_D: -1.3730 (-1.2241) Loss_G: 5.2936 (4.2945) D(x): 0.8879 D(G(z)): 0.0048 / 0.0579 Acc: 100.0000 (99.2014)\n",
      "[2/25][33/96] Loss_D: -1.4333 (-1.2250) Loss_G: 5.8998 (4.3016) D(x): 0.9840 D(G(z)): 0.0622 / 0.0497 Acc: 100.0000 (99.2049)\n",
      "[2/25][34/96] Loss_D: -1.4866 (-1.2262) Loss_G: 7.1694 (4.3142) D(x): 0.9856 D(G(z)): 0.0398 / 0.0065 Acc: 100.0000 (99.2084)\n",
      "[2/25][35/96] Loss_D: -1.3402 (-1.2267) Loss_G: 6.3803 (4.3233) D(x): 0.9463 D(G(z)): 0.0175 / 0.0362 Acc: 100.0000 (99.2119)\n",
      "[2/25][36/96] Loss_D: -1.3346 (-1.2271) Loss_G: 6.5855 (4.3332) D(x): 0.9683 D(G(z)): 0.0187 / 0.0193 Acc: 100.0000 (99.2153)\n",
      "[2/25][37/96] Loss_D: -1.2580 (-1.2273) Loss_G: 5.3773 (4.3377) D(x): 0.9604 D(G(z)): 0.0279 / 0.0381 Acc: 100.0000 (99.2188)\n",
      "[2/25][38/96] Loss_D: -1.3619 (-1.2279) Loss_G: 7.1033 (4.3497) D(x): 0.9771 D(G(z)): 0.0573 / 0.0119 Acc: 98.4375 (99.2154)\n",
      "[2/25][39/96] Loss_D: -1.3691 (-1.2285) Loss_G: 6.0619 (4.3571) D(x): 0.9599 D(G(z)): 0.0133 / 0.0146 Acc: 100.0000 (99.2188)\n",
      "[2/25][40/96] Loss_D: -1.4264 (-1.2293) Loss_G: 6.2574 (4.3652) D(x): 0.9605 D(G(z)): 0.0201 / 0.0168 Acc: 100.0000 (99.2221)\n",
      "[2/25][41/96] Loss_D: -1.4136 (-1.2301) Loss_G: 8.7008 (4.3837) D(x): 0.9766 D(G(z)): 0.0729 / 0.0072 Acc: 100.0000 (99.2254)\n",
      "[2/25][42/96] Loss_D: -1.4542 (-1.2311) Loss_G: 7.8741 (4.3986) D(x): 0.9605 D(G(z)): 0.0022 / 0.0028 Acc: 100.0000 (99.2287)\n",
      "[2/25][43/96] Loss_D: -1.3983 (-1.2318) Loss_G: 5.4748 (4.4032) D(x): 0.9457 D(G(z)): 0.0323 / 0.0313 Acc: 100.0000 (99.2320)\n",
      "[2/25][44/96] Loss_D: -1.5094 (-1.2329) Loss_G: 6.1082 (4.4104) D(x): 0.9778 D(G(z)): 0.0208 / 0.0203 Acc: 100.0000 (99.2352)\n",
      "[2/25][45/96] Loss_D: -1.3777 (-1.2336) Loss_G: 6.3035 (4.4183) D(x): 0.9901 D(G(z)): 0.0294 / 0.0265 Acc: 100.0000 (99.2384)\n",
      "[2/25][46/96] Loss_D: -1.4461 (-1.2344) Loss_G: 7.1682 (4.4298) D(x): 0.9685 D(G(z)): 0.0039 / 0.0067 Acc: 98.4375 (99.2351)\n",
      "[2/25][47/96] Loss_D: -1.5209 (-1.2356) Loss_G: 5.1914 (4.4330) D(x): 0.9750 D(G(z)): 0.0118 / 0.0374 Acc: 100.0000 (99.2383)\n",
      "[2/25][48/96] Loss_D: -1.2613 (-1.2357) Loss_G: 7.2175 (4.4445) D(x): 0.9602 D(G(z)): 0.0731 / 0.0091 Acc: 100.0000 (99.2414)\n",
      "[2/25][49/96] Loss_D: -1.3910 (-1.2364) Loss_G: 5.2334 (4.4478) D(x): 0.9341 D(G(z)): 0.0144 / 0.0544 Acc: 98.4375 (99.2381)\n",
      "[2/25][50/96] Loss_D: -1.3682 (-1.2369) Loss_G: 6.6525 (4.4569) D(x): 0.9849 D(G(z)): 0.0633 / 0.0188 Acc: 100.0000 (99.2413)\n",
      "[2/25][51/96] Loss_D: -1.4509 (-1.2378) Loss_G: 6.7578 (4.4663) D(x): 0.9736 D(G(z)): 0.0159 / 0.0054 Acc: 100.0000 (99.2444)\n",
      "[2/25][52/96] Loss_D: -1.5026 (-1.2389) Loss_G: 6.1609 (4.4732) D(x): 0.9769 D(G(z)): 0.0044 / 0.0264 Acc: 100.0000 (99.2474)\n",
      "[2/25][53/96] Loss_D: -1.3906 (-1.2395) Loss_G: 4.6827 (4.4741) D(x): 0.9467 D(G(z)): 0.0060 / 0.0692 Acc: 100.0000 (99.2505)\n",
      "[2/25][54/96] Loss_D: -1.3954 (-1.2401) Loss_G: 5.9762 (4.4802) D(x): 0.9856 D(G(z)): 0.0552 / 0.0432 Acc: 98.4375 (99.2472)\n",
      "[2/25][55/96] Loss_D: -1.3325 (-1.2405) Loss_G: 7.1944 (4.4911) D(x): 0.9936 D(G(z)): 0.0308 / 0.0130 Acc: 98.4375 (99.2440)\n",
      "[2/25][56/96] Loss_D: -1.3510 (-1.2409) Loss_G: 4.9774 (4.4930) D(x): 0.9289 D(G(z)): 0.0156 / 0.0693 Acc: 98.4375 (99.2407)\n",
      "[2/25][57/96] Loss_D: -1.4919 (-1.2420) Loss_G: 5.9445 (4.4989) D(x): 0.9947 D(G(z)): 0.0281 / 0.0197 Acc: 100.0000 (99.2438)\n",
      "[2/25][58/96] Loss_D: -1.3392 (-1.2423) Loss_G: 4.7249 (4.4998) D(x): 0.9518 D(G(z)): 0.0154 / 0.0673 Acc: 100.0000 (99.2468)\n",
      "[2/25][59/96] Loss_D: -1.3114 (-1.2426) Loss_G: 5.9130 (4.5054) D(x): 0.9896 D(G(z)): 0.0662 / 0.0317 Acc: 98.4375 (99.2436)\n",
      "[2/25][60/96] Loss_D: -1.3722 (-1.2431) Loss_G: 5.1718 (4.5080) D(x): 0.9180 D(G(z)): 0.0088 / 0.0544 Acc: 98.4375 (99.2404)\n",
      "[2/25][61/96] Loss_D: -1.4026 (-1.2438) Loss_G: 7.3833 (4.5193) D(x): 0.9716 D(G(z)): 0.0601 / 0.0189 Acc: 100.0000 (99.2434)\n",
      "[2/25][62/96] Loss_D: -1.3959 (-1.2444) Loss_G: 5.9688 (4.5250) D(x): 0.9476 D(G(z)): 0.0160 / 0.0562 Acc: 100.0000 (99.2463)\n",
      "[2/25][63/96] Loss_D: -1.3426 (-1.2447) Loss_G: 8.7192 (4.5414) D(x): 0.9822 D(G(z)): 0.0876 / 0.0028 Acc: 98.4375 (99.2432)\n",
      "[2/25][64/96] Loss_D: -1.4088 (-1.2454) Loss_G: 7.0247 (4.5510) D(x): 0.9400 D(G(z)): 0.0055 / 0.0141 Acc: 100.0000 (99.2461)\n",
      "[2/25][65/96] Loss_D: -1.4668 (-1.2462) Loss_G: 5.6106 (4.5552) D(x): 0.9432 D(G(z)): 0.0313 / 0.0239 Acc: 100.0000 (99.2490)\n",
      "[2/25][66/96] Loss_D: -1.5346 (-1.2473) Loss_G: 3.8227 (4.5523) D(x): 0.9652 D(G(z)): 0.0241 / 0.1112 Acc: 100.0000 (99.2519)\n",
      "[2/25][67/96] Loss_D: -1.3798 (-1.2479) Loss_G: 8.4246 (4.5672) D(x): 0.9640 D(G(z)): 0.0971 / 0.0133 Acc: 100.0000 (99.2548)\n",
      "[2/25][68/96] Loss_D: -1.2788 (-1.2480) Loss_G: 4.5872 (4.5673) D(x): 0.9076 D(G(z)): 0.0052 / 0.0780 Acc: 100.0000 (99.2577)\n",
      "[2/25][69/96] Loss_D: -1.2905 (-1.2481) Loss_G: 5.9433 (4.5725) D(x): 0.9923 D(G(z)): 0.0465 / 0.0432 Acc: 100.0000 (99.2605)\n",
      "[2/25][70/96] Loss_D: -1.3929 (-1.2487) Loss_G: 7.6609 (4.5843) D(x): 0.9854 D(G(z)): 0.0257 / 0.0114 Acc: 100.0000 (99.2633)\n",
      "[2/25][71/96] Loss_D: -1.3621 (-1.2491) Loss_G: 7.2173 (4.5943) D(x): 0.9811 D(G(z)): 0.0072 / 0.0097 Acc: 100.0000 (99.2661)\n",
      "[2/25][72/96] Loss_D: -1.3866 (-1.2496) Loss_G: 5.5392 (4.5978) D(x): 0.9557 D(G(z)): 0.0093 / 0.0453 Acc: 100.0000 (99.2689)\n",
      "[2/25][73/96] Loss_D: -1.3984 (-1.2502) Loss_G: 6.1188 (4.6035) D(x): 0.9907 D(G(z)): 0.0463 / 0.0313 Acc: 100.0000 (99.2716)\n",
      "[2/25][74/96] Loss_D: -1.3384 (-1.2505) Loss_G: 6.8989 (4.6121) D(x): 0.9519 D(G(z)): 0.0338 / 0.0231 Acc: 100.0000 (99.2743)\n",
      "[2/25][75/96] Loss_D: -1.3483 (-1.2509) Loss_G: 5.7141 (4.6163) D(x): 0.9568 D(G(z)): 0.0146 / 0.0397 Acc: 100.0000 (99.2771)\n",
      "[2/25][76/96] Loss_D: -1.3523 (-1.2513) Loss_G: 3.0915 (4.6106) D(x): 0.9433 D(G(z)): 0.0110 / 0.1527 Acc: 100.0000 (99.2797)\n",
      "[2/25][77/96] Loss_D: -1.2527 (-1.2513) Loss_G: 7.8084 (4.6224) D(x): 0.9976 D(G(z)): 0.1018 / 0.0073 Acc: 95.3125 (99.2650)\n",
      "[2/25][78/96] Loss_D: -1.3704 (-1.2517) Loss_G: 7.4780 (4.6330) D(x): 0.9333 D(G(z)): 0.0053 / 0.0130 Acc: 100.0000 (99.2678)\n",
      "[2/25][79/96] Loss_D: -1.3150 (-1.2519) Loss_G: 6.1271 (4.6385) D(x): 0.9727 D(G(z)): 0.0094 / 0.0292 Acc: 100.0000 (99.2705)\n",
      "[2/25][80/96] Loss_D: -1.2697 (-1.2520) Loss_G: 7.0072 (4.6471) D(x): 0.9881 D(G(z)): 0.0412 / 0.0179 Acc: 100.0000 (99.2731)\n",
      "[2/25][81/96] Loss_D: -1.4660 (-1.2528) Loss_G: 6.6777 (4.6545) D(x): 0.9728 D(G(z)): 0.0041 / 0.0280 Acc: 100.0000 (99.2758)\n",
      "[2/25][82/96] Loss_D: -1.4648 (-1.2536) Loss_G: 7.0588 (4.6633) D(x): 0.9849 D(G(z)): 0.0256 / 0.0168 Acc: 100.0000 (99.2784)\n",
      "[2/25][83/96] Loss_D: -1.4361 (-1.2542) Loss_G: 6.6761 (4.6706) D(x): 0.9472 D(G(z)): 0.0404 / 0.0319 Acc: 100.0000 (99.2810)\n",
      "[2/25][84/96] Loss_D: -1.4270 (-1.2548) Loss_G: 6.0002 (4.6754) D(x): 0.9791 D(G(z)): 0.0024 / 0.0393 Acc: 100.0000 (99.2836)\n",
      "[2/25][85/96] Loss_D: -1.4225 (-1.2554) Loss_G: 6.0513 (4.6803) D(x): 0.9824 D(G(z)): 0.0233 / 0.0154 Acc: 98.4375 (99.2806)\n",
      "[2/25][86/96] Loss_D: -1.4445 (-1.2561) Loss_G: 6.8611 (4.6882) D(x): 0.9888 D(G(z)): 0.0112 / 0.0188 Acc: 98.4375 (99.2776)\n",
      "[2/25][87/96] Loss_D: -1.3813 (-1.2566) Loss_G: 7.4232 (4.6979) D(x): 0.9688 D(G(z)): 0.0537 / 0.0138 Acc: 100.0000 (99.2801)\n",
      "[2/25][88/96] Loss_D: -1.2714 (-1.2566) Loss_G: 7.0369 (4.7062) D(x): 0.9467 D(G(z)): 0.0242 / 0.0220 Acc: 100.0000 (99.2827)\n",
      "[2/25][89/96] Loss_D: -1.4593 (-1.2573) Loss_G: 7.4565 (4.7160) D(x): 0.9796 D(G(z)): 0.0158 / 0.0160 Acc: 100.0000 (99.2852)\n",
      "[2/25][90/96] Loss_D: -1.4433 (-1.2580) Loss_G: 6.1970 (4.7212) D(x): 0.9659 D(G(z)): 0.0193 / 0.0133 Acc: 96.8750 (99.2767)\n",
      "[2/25][91/96] Loss_D: -1.4381 (-1.2586) Loss_G: 6.8195 (4.7286) D(x): 0.9843 D(G(z)): 0.0316 / 0.0118 Acc: 100.0000 (99.2793)\n",
      "[2/25][92/96] Loss_D: -1.3807 (-1.2591) Loss_G: 7.2879 (4.7376) D(x): 0.9731 D(G(z)): 0.0255 / 0.0160 Acc: 100.0000 (99.2818)\n",
      "[2/25][93/96] Loss_D: -1.4425 (-1.2597) Loss_G: 4.8066 (4.7378) D(x): 0.9521 D(G(z)): 0.0048 / 0.0447 Acc: 100.0000 (99.2843)\n",
      "[2/25][94/96] Loss_D: -1.3953 (-1.2602) Loss_G: 8.0649 (4.7494) D(x): 0.9963 D(G(z)): 0.0631 / 0.0022 Acc: 98.4375 (99.2814)\n",
      "[2/25][95/96] Loss_D: -1.5560 (-1.2612) Loss_G: 7.2140 (4.7580) D(x): 0.9689 D(G(z)): 0.0171 / 0.0110 Acc: 100.0000 (99.2839)\n",
      "[3/25][0/96] Loss_D: -1.4755 (-1.2619) Loss_G: 6.9973 (4.7657) D(x): 0.9820 D(G(z)): 0.0055 / 0.0107 Acc: 95.3125 (99.2701)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[3/25][1/96] Loss_D: -1.2763 (-1.2620) Loss_G: 4.2813 (4.7641) D(x): 0.9601 D(G(z)): 0.0171 / 0.0548 Acc: 100.0000 (99.2726)\n",
      "[3/25][2/96] Loss_D: -1.3424 (-1.2623) Loss_G: 5.4388 (4.7664) D(x): 0.9781 D(G(z)): 0.0373 / 0.0506 Acc: 98.4375 (99.2698)\n",
      "[3/25][3/96] Loss_D: -1.3551 (-1.2626) Loss_G: 7.6633 (4.7763) D(x): 0.9866 D(G(z)): 0.0406 / 0.0026 Acc: 100.0000 (99.2723)\n",
      "[3/25][4/96] Loss_D: -1.3248 (-1.2628) Loss_G: 4.7923 (4.7764) D(x): 0.9325 D(G(z)): 0.0066 / 0.0673 Acc: 100.0000 (99.2747)\n",
      "[3/25][5/96] Loss_D: -1.5468 (-1.2638) Loss_G: 6.4930 (4.7822) D(x): 0.9891 D(G(z)): 0.0391 / 0.0063 Acc: 100.0000 (99.2772)\n",
      "[3/25][6/96] Loss_D: -1.3990 (-1.2642) Loss_G: 8.3975 (4.7945) D(x): 0.9783 D(G(z)): 0.0518 / 0.0065 Acc: 100.0000 (99.2797)\n",
      "[3/25][7/96] Loss_D: -1.4700 (-1.2649) Loss_G: 7.3103 (4.8030) D(x): 0.9524 D(G(z)): 0.0067 / 0.0144 Acc: 100.0000 (99.2821)\n",
      "[3/25][8/96] Loss_D: -1.4333 (-1.2655) Loss_G: 4.0146 (4.8003) D(x): 0.9392 D(G(z)): 0.0211 / 0.1055 Acc: 98.4375 (99.2793)\n",
      "[3/25][9/96] Loss_D: -1.3411 (-1.2657) Loss_G: 7.8362 (4.8105) D(x): 0.9878 D(G(z)): 0.0874 / 0.0046 Acc: 98.4375 (99.2764)\n",
      "[3/25][10/96] Loss_D: -1.3122 (-1.2659) Loss_G: 4.0302 (4.8079) D(x): 0.8912 D(G(z)): 0.0058 / 0.0840 Acc: 98.4375 (99.2736)\n",
      "[3/25][11/96] Loss_D: -1.5085 (-1.2667) Loss_G: 7.2026 (4.8159) D(x): 0.9981 D(G(z)): 0.0724 / 0.0174 Acc: 100.0000 (99.2760)\n",
      "[3/25][12/96] Loss_D: -1.4600 (-1.2673) Loss_G: 7.5949 (4.8251) D(x): 0.9649 D(G(z)): 0.0068 / 0.0110 Acc: 100.0000 (99.2784)\n",
      "[3/25][13/96] Loss_D: -1.4875 (-1.2681) Loss_G: 5.8705 (4.8286) D(x): 0.9666 D(G(z)): 0.0044 / 0.0213 Acc: 96.8750 (99.2705)\n",
      "[3/25][14/96] Loss_D: -1.2329 (-1.2680) Loss_G: 9.8861 (4.8452) D(x): 0.9680 D(G(z)): 0.1147 / 0.0003 Acc: 100.0000 (99.2729)\n",
      "[3/25][15/96] Loss_D: -1.3704 (-1.2683) Loss_G: 5.9279 (4.8488) D(x): 0.8898 D(G(z)): 0.0055 / 0.0343 Acc: 100.0000 (99.2753)\n",
      "[3/25][16/96] Loss_D: -1.4254 (-1.2688) Loss_G: 4.7607 (4.8485) D(x): 0.9821 D(G(z)): 0.0319 / 0.0433 Acc: 100.0000 (99.2777)\n",
      "[3/25][17/96] Loss_D: -1.2903 (-1.2689) Loss_G: 7.7931 (4.8581) D(x): 0.9914 D(G(z)): 0.0870 / 0.0133 Acc: 100.0000 (99.2800)\n",
      "[3/25][18/96] Loss_D: -1.3381 (-1.2691) Loss_G: 5.4878 (4.8602) D(x): 0.9042 D(G(z)): 0.0023 / 0.0296 Acc: 98.4375 (99.2773)\n",
      "[3/25][19/96] Loss_D: -1.4998 (-1.2699) Loss_G: 4.4977 (4.8590) D(x): 0.9877 D(G(z)): 0.0159 / 0.0558 Acc: 100.0000 (99.2796)\n",
      "[3/25][20/96] Loss_D: -1.3381 (-1.2701) Loss_G: 9.6703 (4.8746) D(x): 0.9965 D(G(z)): 0.0915 / 0.0016 Acc: 98.4375 (99.2769)\n",
      "[3/25][21/96] Loss_D: -1.3172 (-1.2702) Loss_G: 7.7367 (4.8838) D(x): 0.9141 D(G(z)): 0.0014 / 0.0048 Acc: 98.4375 (99.2742)\n",
      "[3/25][22/96] Loss_D: -1.4644 (-1.2709) Loss_G: 4.0906 (4.8813) D(x): 0.9485 D(G(z)): 0.0074 / 0.0939 Acc: 100.0000 (99.2765)\n",
      "[3/25][23/96] Loss_D: -1.2281 (-1.2707) Loss_G: 8.5531 (4.8930) D(x): 0.9924 D(G(z)): 0.1164 / 0.0078 Acc: 100.0000 (99.2788)\n",
      "[3/25][24/96] Loss_D: -1.3319 (-1.2709) Loss_G: 7.8602 (4.9025) D(x): 0.9190 D(G(z)): 0.0067 / 0.0067 Acc: 98.4375 (99.2762)\n",
      "[3/25][25/96] Loss_D: -1.4459 (-1.2715) Loss_G: 5.4635 (4.9043) D(x): 0.9605 D(G(z)): 0.0057 / 0.0361 Acc: 100.0000 (99.2785)\n",
      "[3/25][26/96] Loss_D: -1.3035 (-1.2716) Loss_G: 5.6125 (4.9066) D(x): 0.9818 D(G(z)): 0.0397 / 0.0221 Acc: 98.4375 (99.2758)\n",
      "[3/25][27/96] Loss_D: -1.5437 (-1.2724) Loss_G: 7.1007 (4.9135) D(x): 0.9969 D(G(z)): 0.0308 / 0.0246 Acc: 100.0000 (99.2781)\n",
      "[3/25][28/96] Loss_D: -1.4614 (-1.2730) Loss_G: 7.6570 (4.9221) D(x): 0.9819 D(G(z)): 0.0038 / 0.0050 Acc: 100.0000 (99.2804)\n",
      "[3/25][29/96] Loss_D: -1.4024 (-1.2734) Loss_G: 4.9480 (4.9222) D(x): 0.9491 D(G(z)): 0.0078 / 0.0742 Acc: 100.0000 (99.2826)\n",
      "[3/25][30/96] Loss_D: -1.3886 (-1.2738) Loss_G: 5.3469 (4.9236) D(x): 0.9795 D(G(z)): 0.0443 / 0.0424 Acc: 100.0000 (99.2849)\n",
      "[3/25][31/96] Loss_D: -1.4434 (-1.2743) Loss_G: 8.0474 (4.9333) D(x): 0.9964 D(G(z)): 0.0357 / 0.0069 Acc: 98.4375 (99.2822)\n",
      "[3/25][32/96] Loss_D: -1.3881 (-1.2747) Loss_G: 5.7937 (4.9360) D(x): 0.9307 D(G(z)): 0.0096 / 0.0118 Acc: 100.0000 (99.2845)\n",
      "[3/25][33/96] Loss_D: -1.3207 (-1.2748) Loss_G: 6.4329 (4.9407) D(x): 0.9851 D(G(z)): 0.0321 / 0.0184 Acc: 100.0000 (99.2867)\n",
      "[3/25][34/96] Loss_D: -1.4540 (-1.2754) Loss_G: 6.4736 (4.9454) D(x): 0.9889 D(G(z)): 0.0342 / 0.0135 Acc: 100.0000 (99.2889)\n",
      "[3/25][35/96] Loss_D: -1.4839 (-1.2760) Loss_G: 6.9311 (4.9515) D(x): 0.9754 D(G(z)): 0.0171 / 0.0062 Acc: 100.0000 (99.2911)\n",
      "[3/25][36/96] Loss_D: -1.3839 (-1.2764) Loss_G: 8.2435 (4.9617) D(x): 0.9708 D(G(z)): 0.0034 / 0.0049 Acc: 98.4375 (99.2885)\n",
      "[3/25][37/96] Loss_D: -1.5693 (-1.2773) Loss_G: 7.0973 (4.9682) D(x): 0.9942 D(G(z)): 0.0175 / 0.0047 Acc: 100.0000 (99.2906)\n",
      "[3/25][38/96] Loss_D: -1.4046 (-1.2776) Loss_G: 6.7118 (4.9735) D(x): 0.9805 D(G(z)): 0.0092 / 0.0129 Acc: 100.0000 (99.2928)\n",
      "[3/25][39/96] Loss_D: -1.4107 (-1.2780) Loss_G: 5.9163 (4.9764) D(x): 0.9925 D(G(z)): 0.0174 / 0.0155 Acc: 98.4375 (99.2902)\n",
      "[3/25][40/96] Loss_D: -1.4476 (-1.2786) Loss_G: 7.2087 (4.9832) D(x): 0.9933 D(G(z)): 0.0226 / 0.0055 Acc: 100.0000 (99.2924)\n",
      "[3/25][41/96] Loss_D: -1.4807 (-1.2792) Loss_G: 5.0280 (4.9833) D(x): 0.9518 D(G(z)): 0.0091 / 0.0469 Acc: 100.0000 (99.2945)\n",
      "[3/25][42/96] Loss_D: -1.3634 (-1.2794) Loss_G: 5.9307 (4.9862) D(x): 0.9645 D(G(z)): 0.0294 / 0.0254 Acc: 98.4375 (99.2919)\n",
      "[3/25][43/96] Loss_D: -1.5476 (-1.2802) Loss_G: 6.7948 (4.9916) D(x): 0.9938 D(G(z)): 0.0308 / 0.0247 Acc: 100.0000 (99.2941)\n",
      "[3/25][44/96] Loss_D: -1.4176 (-1.2807) Loss_G: 6.6895 (4.9967) D(x): 0.9711 D(G(z)): 0.0129 / 0.0044 Acc: 100.0000 (99.2962)\n",
      "[3/25][45/96] Loss_D: -1.4865 (-1.2813) Loss_G: 5.5672 (4.9985) D(x): 0.9848 D(G(z)): 0.0112 / 0.0252 Acc: 100.0000 (99.2983)\n",
      "[3/25][46/96] Loss_D: -1.4389 (-1.2817) Loss_G: 5.7092 (5.0006) D(x): 0.9872 D(G(z)): 0.0149 / 0.0381 Acc: 98.4375 (99.2957)\n",
      "[3/25][47/96] Loss_D: -1.4540 (-1.2823) Loss_G: 5.3867 (5.0017) D(x): 0.9734 D(G(z)): 0.0122 / 0.0331 Acc: 100.0000 (99.2978)\n",
      "[3/25][48/96] Loss_D: -1.4073 (-1.2826) Loss_G: 6.2632 (5.0055) D(x): 0.9816 D(G(z)): 0.0333 / 0.0321 Acc: 100.0000 (99.2999)\n",
      "[3/25][49/96] Loss_D: -1.3927 (-1.2829) Loss_G: 7.7940 (5.0137) D(x): 0.9819 D(G(z)): 0.0172 / 0.0082 Acc: 98.4375 (99.2973)\n",
      "[3/25][50/96] Loss_D: -1.3638 (-1.2832) Loss_G: 7.1354 (5.0200) D(x): 0.9836 D(G(z)): 0.0087 / 0.0160 Acc: 98.4375 (99.2948)\n",
      "[3/25][51/96] Loss_D: -1.3994 (-1.2835) Loss_G: 6.9738 (5.0257) D(x): 0.9673 D(G(z)): 0.0183 / 0.0201 Acc: 100.0000 (99.2969)\n",
      "[3/25][52/96] Loss_D: -1.5538 (-1.2843) Loss_G: 6.9765 (5.0314) D(x): 0.9837 D(G(z)): 0.0226 / 0.0101 Acc: 100.0000 (99.2989)\n",
      "[3/25][53/96] Loss_D: -1.4499 (-1.2848) Loss_G: 6.0444 (5.0344) D(x): 0.9758 D(G(z)): 0.0066 / 0.0148 Acc: 100.0000 (99.3010)\n",
      "[3/25][54/96] Loss_D: -1.4504 (-1.2853) Loss_G: 5.8701 (5.0368) D(x): 0.9799 D(G(z)): 0.0351 / 0.0152 Acc: 100.0000 (99.3030)\n",
      "[3/25][55/96] Loss_D: -1.4220 (-1.2857) Loss_G: 6.6087 (5.0414) D(x): 0.9895 D(G(z)): 0.0164 / 0.0199 Acc: 100.0000 (99.3051)\n",
      "[3/25][56/96] Loss_D: -1.4861 (-1.2863) Loss_G: 7.6324 (5.0489) D(x): 0.9862 D(G(z)): 0.0141 / 0.0039 Acc: 100.0000 (99.3071)\n",
      "[3/25][57/96] Loss_D: -1.4585 (-1.2868) Loss_G: 6.8425 (5.0541) D(x): 0.9802 D(G(z)): 0.0049 / 0.0136 Acc: 100.0000 (99.3091)\n",
      "[3/25][58/96] Loss_D: -1.5197 (-1.2874) Loss_G: 6.5841 (5.0585) D(x): 0.9865 D(G(z)): 0.0122 / 0.0236 Acc: 100.0000 (99.3111)\n",
      "[3/25][59/96] Loss_D: -1.4604 (-1.2879) Loss_G: 5.8482 (5.0608) D(x): 0.9705 D(G(z)): 0.0199 / 0.0243 Acc: 98.4375 (99.3085)\n",
      "[3/25][60/96] Loss_D: -1.3959 (-1.2882) Loss_G: 8.4003 (5.0703) D(x): 0.9937 D(G(z)): 0.0309 / 0.0015 Acc: 100.0000 (99.3105)\n",
      "[3/25][61/96] Loss_D: -1.5134 (-1.2889) Loss_G: 7.5332 (5.0774) D(x): 0.9763 D(G(z)): 0.0064 / 0.0033 Acc: 100.0000 (99.3125)\n",
      "[3/25][62/96] Loss_D: -1.4427 (-1.2893) Loss_G: 6.8477 (5.0824) D(x): 0.9769 D(G(z)): 0.0236 / 0.0185 Acc: 100.0000 (99.3145)\n",
      "[3/25][63/96] Loss_D: -1.3921 (-1.2896) Loss_G: 6.1167 (5.0854) D(x): 0.9526 D(G(z)): 0.0207 / 0.0183 Acc: 100.0000 (99.3164)\n",
      "[3/25][64/96] Loss_D: -1.5048 (-1.2902) Loss_G: 7.1956 (5.0913) D(x): 0.9850 D(G(z)): 0.0350 / 0.0156 Acc: 100.0000 (99.3183)\n",
      "[3/25][65/96] Loss_D: -1.3577 (-1.2904) Loss_G: 8.4592 (5.1009) D(x): 0.9852 D(G(z)): 0.0282 / 0.0023 Acc: 100.0000 (99.3203)\n",
      "[3/25][66/96] Loss_D: -1.4883 (-1.2910) Loss_G: 7.6907 (5.1082) D(x): 0.9794 D(G(z)): 0.0045 / 0.0073 Acc: 98.4375 (99.3178)\n",
      "[3/25][67/96] Loss_D: -1.5196 (-1.2916) Loss_G: 6.1957 (5.1112) D(x): 0.9636 D(G(z)): 0.0146 / 0.0133 Acc: 100.0000 (99.3197)\n",
      "[3/25][68/96] Loss_D: -1.3755 (-1.2918) Loss_G: 8.0475 (5.1194) D(x): 0.9903 D(G(z)): 0.0285 / 0.0064 Acc: 100.0000 (99.3216)\n",
      "[3/25][69/96] Loss_D: -1.4555 (-1.2923) Loss_G: 5.1888 (5.1196) D(x): 0.9507 D(G(z)): 0.0055 / 0.0562 Acc: 100.0000 (99.3235)\n",
      "[3/25][70/96] Loss_D: -1.4455 (-1.2927) Loss_G: 7.5170 (5.1263) D(x): 0.9983 D(G(z)): 0.0578 / 0.0063 Acc: 100.0000 (99.3254)\n",
      "[3/25][71/96] Loss_D: -1.4452 (-1.2932) Loss_G: 6.2858 (5.1295) D(x): 0.9394 D(G(z)): 0.0015 / 0.0162 Acc: 100.0000 (99.3273)\n",
      "[3/25][72/96] Loss_D: -1.4626 (-1.2936) Loss_G: 5.3613 (5.1302) D(x): 0.9797 D(G(z)): 0.0314 / 0.0413 Acc: 100.0000 (99.3291)\n",
      "[3/25][73/96] Loss_D: -1.3843 (-1.2939) Loss_G: 7.5085 (5.1367) D(x): 0.9964 D(G(z)): 0.0309 / 0.0034 Acc: 98.4375 (99.3267)\n",
      "[3/25][74/96] Loss_D: -1.4314 (-1.2943) Loss_G: 7.4964 (5.1432) D(x): 0.9739 D(G(z)): 0.0178 / 0.0046 Acc: 100.0000 (99.3285)\n",
      "[3/25][75/96] Loss_D: -1.3759 (-1.2945) Loss_G: 7.7579 (5.1504) D(x): 0.9779 D(G(z)): 0.0113 / 0.0027 Acc: 98.4375 (99.3261)\n",
      "[3/25][76/96] Loss_D: -1.4832 (-1.2950) Loss_G: 4.8127 (5.1495) D(x): 0.9566 D(G(z)): 0.0035 / 0.0364 Acc: 98.4375 (99.3236)\n",
      "[3/25][77/96] Loss_D: -1.3990 (-1.2953) Loss_G: 6.4658 (5.1531) D(x): 0.9944 D(G(z)): 0.0398 / 0.0280 Acc: 100.0000 (99.3255)\n",
      "[3/25][78/96] Loss_D: -1.5768 (-1.2960) Loss_G: 6.1368 (5.1558) D(x): 0.9885 D(G(z)): 0.0031 / 0.0361 Acc: 98.4375 (99.3231)\n",
      "[3/25][79/96] Loss_D: -1.2780 (-1.2960) Loss_G: 5.6071 (5.1570) D(x): 0.9766 D(G(z)): 0.0156 / 0.0273 Acc: 100.0000 (99.3249)\n",
      "[3/25][80/96] Loss_D: -1.5454 (-1.2967) Loss_G: 7.8821 (5.1644) D(x): 0.9967 D(G(z)): 0.0340 / 0.0068 Acc: 100.0000 (99.3267)\n",
      "[3/25][81/96] Loss_D: -1.3707 (-1.2969) Loss_G: 6.8283 (5.1689) D(x): 0.9581 D(G(z)): 0.0164 / 0.0104 Acc: 96.8750 (99.3201)\n",
      "[3/25][82/96] Loss_D: -1.4026 (-1.2972) Loss_G: 5.2165 (5.1690) D(x): 0.9666 D(G(z)): 0.0090 / 0.0455 Acc: 100.0000 (99.3219)\n",
      "[3/25][83/96] Loss_D: -1.3950 (-1.2974) Loss_G: 8.3048 (5.1774) D(x): 0.9950 D(G(z)): 0.0595 / 0.0023 Acc: 100.0000 (99.3238)\n",
      "[3/25][84/96] Loss_D: -1.2156 (-1.2972) Loss_G: 4.3326 (5.1752) D(x): 0.9052 D(G(z)): 0.0012 / 0.0759 Acc: 98.4375 (99.3214)\n",
      "[3/25][85/96] Loss_D: -1.2477 (-1.2971) Loss_G: 11.7644 (5.1928) D(x): 0.9942 D(G(z)): 0.1314 / 0.0003 Acc: 100.0000 (99.3232)\n",
      "[3/25][86/96] Loss_D: -0.6925 (-1.2955) Loss_G: -0.4266 (5.1778) D(x): 0.6239 D(G(z)): 0.0003 / 0.9897 Acc: 100.0000 (99.3250)\n",
      "[3/25][87/96] Loss_D: 5.0753 (-1.2785) Loss_G: 10.8038 (5.1928) D(x): 1.0000 D(G(z)): 0.9945 / 0.0002 Acc: 100.0000 (99.3268)\n",
      "[3/25][88/96] Loss_D: 3.2158 (-1.2666) Loss_G: 1.1510 (5.1821) D(x): 0.0972 D(G(z)): 0.0001 / 0.3730 Acc: 100.0000 (99.3286)\n",
      "[3/25][89/96] Loss_D: -0.6575 (-1.2650) Loss_G: 0.8050 (5.1705) D(x): 0.8650 D(G(z)): 0.3429 / 0.4822 Acc: 100.0000 (99.3304)\n",
      "[3/25][90/96] Loss_D: -0.2656 (-1.2623) Loss_G: 3.2214 (5.1653) D(x): 0.9181 D(G(z)): 0.5188 / 0.1244 Acc: 100.0000 (99.3321)\n",
      "[3/25][91/96] Loss_D: -0.6003 (-1.2606) Loss_G: 2.9543 (5.1595) D(x): 0.6929 D(G(z)): 0.1190 / 0.1176 Acc: 100.0000 (99.3339)\n",
      "[3/25][92/96] Loss_D: -0.8628 (-1.2596) Loss_G: 2.3146 (5.1520) D(x): 0.7497 D(G(z)): 0.1202 / 0.1180 Acc: 100.0000 (99.3356)\n",
      "[3/25][93/96] Loss_D: -1.2365 (-1.2595) Loss_G: 5.7844 (5.1537) D(x): 0.9583 D(G(z)): 0.1636 / 0.0097 Acc: 100.0000 (99.3374)\n",
      "[3/25][94/96] Loss_D: -1.4553 (-1.2600) Loss_G: 6.7855 (5.1580) D(x): 0.9563 D(G(z)): 0.0216 / 0.0178 Acc: 100.0000 (99.3391)\n",
      "[3/25][95/96] Loss_D: -1.3848 (-1.2603) Loss_G: 6.4780 (5.1614) D(x): 0.9572 D(G(z)): 0.0165 / 0.0057 Acc: 100.0000 (99.3408)\n",
      "[4/25][0/96] Loss_D: -1.4270 (-1.2608) Loss_G: 6.2119 (5.1641) D(x): 0.9806 D(G(z)): 0.0318 / 0.0164 Acc: 98.4375 (99.3385)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[4/25][1/96] Loss_D: -1.4483 (-1.2613) Loss_G: 5.4494 (5.1649) D(x): 0.9502 D(G(z)): 0.0231 / 0.0213 Acc: 100.0000 (99.3402)\n",
      "[4/25][2/96] Loss_D: -1.3936 (-1.2616) Loss_G: 4.8316 (5.1640) D(x): 0.9501 D(G(z)): 0.0588 / 0.0632 Acc: 100.0000 (99.3419)\n",
      "[4/25][3/96] Loss_D: -1.2707 (-1.2616) Loss_G: 6.7028 (5.1680) D(x): 0.9565 D(G(z)): 0.0894 / 0.0114 Acc: 100.0000 (99.3436)\n",
      "[4/25][4/96] Loss_D: -1.4410 (-1.2621) Loss_G: 5.9783 (5.1701) D(x): 0.9294 D(G(z)): 0.0142 / 0.0112 Acc: 98.4375 (99.3413)\n",
      "[4/25][5/96] Loss_D: -1.3116 (-1.2622) Loss_G: 4.8338 (5.1692) D(x): 0.9077 D(G(z)): 0.0289 / 0.0263 Acc: 100.0000 (99.3429)\n",
      "[4/25][6/96] Loss_D: -1.5505 (-1.2629) Loss_G: 5.8682 (5.1710) D(x): 0.9903 D(G(z)): 0.0476 / 0.0159 Acc: 100.0000 (99.3446)\n",
      "[4/25][7/96] Loss_D: -1.4677 (-1.2635) Loss_G: 7.5831 (5.1771) D(x): 0.9916 D(G(z)): 0.0448 / 0.0025 Acc: 100.0000 (99.3463)\n",
      "[4/25][8/96] Loss_D: -1.5198 (-1.2641) Loss_G: 7.8721 (5.1840) D(x): 0.9552 D(G(z)): 0.0034 / 0.0016 Acc: 100.0000 (99.3480)\n",
      "[4/25][9/96] Loss_D: -1.4223 (-1.2645) Loss_G: 6.2235 (5.1866) D(x): 0.9828 D(G(z)): 0.0106 / 0.0127 Acc: 100.0000 (99.3496)\n",
      "[4/25][10/96] Loss_D: -1.4887 (-1.2651) Loss_G: 6.0987 (5.1889) D(x): 0.9682 D(G(z)): 0.0043 / 0.0057 Acc: 100.0000 (99.3513)\n",
      "[4/25][11/96] Loss_D: -1.5108 (-1.2657) Loss_G: 5.9374 (5.1908) D(x): 0.9778 D(G(z)): 0.0343 / 0.0067 Acc: 100.0000 (99.3529)\n",
      "[4/25][12/96] Loss_D: -1.4046 (-1.2661) Loss_G: 5.1532 (5.1907) D(x): 0.9766 D(G(z)): 0.0199 / 0.0179 Acc: 98.4375 (99.3506)\n",
      "[4/25][13/96] Loss_D: -1.4122 (-1.2664) Loss_G: 6.2840 (5.1935) D(x): 0.9827 D(G(z)): 0.0572 / 0.0122 Acc: 100.0000 (99.3522)\n",
      "[4/25][14/96] Loss_D: -1.3013 (-1.2665) Loss_G: 6.7064 (5.1973) D(x): 0.9539 D(G(z)): 0.0279 / 0.0051 Acc: 100.0000 (99.3539)\n",
      "[4/25][15/96] Loss_D: -1.4497 (-1.2670) Loss_G: 5.2162 (5.1973) D(x): 0.9564 D(G(z)): 0.0110 / 0.0151 Acc: 100.0000 (99.3555)\n",
      "[4/25][16/96] Loss_D: -1.4682 (-1.2675) Loss_G: 5.3248 (5.1976) D(x): 0.9879 D(G(z)): 0.0473 / 0.0122 Acc: 100.0000 (99.3571)\n",
      "[4/25][17/96] Loss_D: -1.4923 (-1.2680) Loss_G: 6.0017 (5.1996) D(x): 0.9768 D(G(z)): 0.0119 / 0.0258 Acc: 100.0000 (99.3587)\n",
      "[4/25][18/96] Loss_D: -1.5350 (-1.2687) Loss_G: 5.1591 (5.1995) D(x): 0.9757 D(G(z)): 0.0140 / 0.0219 Acc: 100.0000 (99.3603)\n",
      "[4/25][19/96] Loss_D: -1.4610 (-1.2692) Loss_G: 5.9464 (5.2014) D(x): 0.9866 D(G(z)): 0.0083 / 0.0100 Acc: 100.0000 (99.3619)\n",
      "[4/25][20/96] Loss_D: -1.4295 (-1.2696) Loss_G: 5.4318 (5.2020) D(x): 0.9766 D(G(z)): 0.0247 / 0.0135 Acc: 100.0000 (99.3634)\n",
      "[4/25][21/96] Loss_D: -1.3676 (-1.2698) Loss_G: 5.7332 (5.2033) D(x): 0.9642 D(G(z)): 0.0116 / 0.0163 Acc: 100.0000 (99.3650)\n",
      "[4/25][22/96] Loss_D: -1.4232 (-1.2702) Loss_G: 5.8682 (5.2049) D(x): 0.9872 D(G(z)): 0.0261 / 0.0103 Acc: 96.8750 (99.3589)\n",
      "[4/25][23/96] Loss_D: -1.3988 (-1.2705) Loss_G: 5.6180 (5.2059) D(x): 0.9832 D(G(z)): 0.0193 / 0.0139 Acc: 98.4375 (99.3566)\n",
      "[4/25][24/96] Loss_D: -1.5240 (-1.2711) Loss_G: 6.0353 (5.2079) D(x): 0.9912 D(G(z)): 0.0129 / 0.0058 Acc: 98.4375 (99.3544)\n",
      "[4/25][25/96] Loss_D: -1.3875 (-1.2714) Loss_G: 6.2557 (5.2105) D(x): 0.9868 D(G(z)): 0.0169 / 0.0146 Acc: 98.4375 (99.3521)\n",
      "[4/25][26/96] Loss_D: -1.5197 (-1.2720) Loss_G: 4.9976 (5.2100) D(x): 0.9783 D(G(z)): 0.0166 / 0.0245 Acc: 100.0000 (99.3537)\n",
      "[4/25][27/96] Loss_D: -1.4371 (-1.2724) Loss_G: 5.8225 (5.2115) D(x): 0.9827 D(G(z)): 0.0257 / 0.0057 Acc: 100.0000 (99.3553)\n",
      "[4/25][28/96] Loss_D: -1.5533 (-1.2731) Loss_G: 5.7406 (5.2127) D(x): 0.9640 D(G(z)): 0.0057 / 0.0100 Acc: 98.4375 (99.3531)\n",
      "[4/25][29/96] Loss_D: -1.4790 (-1.2736) Loss_G: 5.1721 (5.2126) D(x): 0.9932 D(G(z)): 0.0136 / 0.0211 Acc: 100.0000 (99.3546)\n",
      "[4/25][30/96] Loss_D: -1.5082 (-1.2742) Loss_G: 5.6571 (5.2137) D(x): 0.9620 D(G(z)): 0.0176 / 0.0229 Acc: 100.0000 (99.3562)\n",
      "[4/25][31/96] Loss_D: -1.3853 (-1.2744) Loss_G: 5.0155 (5.2132) D(x): 0.9801 D(G(z)): 0.0251 / 0.0164 Acc: 98.4375 (99.3540)\n",
      "[4/25][32/96] Loss_D: -1.4273 (-1.2748) Loss_G: 5.1826 (5.2132) D(x): 0.9856 D(G(z)): 0.0363 / 0.0183 Acc: 98.4375 (99.3518)\n",
      "[4/25][33/96] Loss_D: -1.4855 (-1.2753) Loss_G: 5.8553 (5.2147) D(x): 0.9712 D(G(z)): 0.0203 / 0.0208 Acc: 100.0000 (99.3533)\n",
      "[4/25][34/96] Loss_D: -1.4177 (-1.2756) Loss_G: 5.5881 (5.2156) D(x): 0.9718 D(G(z)): 0.0256 / 0.0118 Acc: 98.4375 (99.3511)\n",
      "[4/25][35/96] Loss_D: -1.5279 (-1.2762) Loss_G: 6.5943 (5.2189) D(x): 0.9929 D(G(z)): 0.0112 / 0.0041 Acc: 100.0000 (99.3527)\n",
      "[4/25][36/96] Loss_D: -1.4936 (-1.2767) Loss_G: 6.3328 (5.2215) D(x): 0.9758 D(G(z)): 0.0094 / 0.0108 Acc: 98.4375 (99.3505)\n",
      "[4/25][37/96] Loss_D: -1.4285 (-1.2771) Loss_G: 5.3523 (5.2218) D(x): 0.9786 D(G(z)): 0.0358 / 0.0257 Acc: 98.4375 (99.3483)\n",
      "[4/25][38/96] Loss_D: -1.4859 (-1.2776) Loss_G: 6.3229 (5.2244) D(x): 0.9843 D(G(z)): 0.0264 / 0.0041 Acc: 100.0000 (99.3499)\n",
      "[4/25][39/96] Loss_D: -1.5067 (-1.2781) Loss_G: 5.8349 (5.2259) D(x): 0.9715 D(G(z)): 0.0227 / 0.0135 Acc: 100.0000 (99.3514)\n",
      "[4/25][40/96] Loss_D: -1.3748 (-1.2784) Loss_G: 6.3226 (5.2285) D(x): 0.9669 D(G(z)): 0.0051 / 0.0063 Acc: 100.0000 (99.3529)\n",
      "[4/25][41/96] Loss_D: -1.5527 (-1.2790) Loss_G: 5.8329 (5.2299) D(x): 0.9735 D(G(z)): 0.0194 / 0.0121 Acc: 100.0000 (99.3545)\n",
      "[4/25][42/96] Loss_D: -1.6066 (-1.2798) Loss_G: 4.5867 (5.2284) D(x): 0.9785 D(G(z)): 0.0361 / 0.0251 Acc: 100.0000 (99.3560)\n",
      "[4/25][43/96] Loss_D: -1.3723 (-1.2800) Loss_G: 5.2880 (5.2285) D(x): 0.9759 D(G(z)): 0.0199 / 0.0127 Acc: 100.0000 (99.3575)\n",
      "[4/25][44/96] Loss_D: -1.4840 (-1.2805) Loss_G: 5.5356 (5.2292) D(x): 0.9821 D(G(z)): 0.0262 / 0.0165 Acc: 100.0000 (99.3590)\n",
      "[4/25][45/96] Loss_D: -1.4631 (-1.2809) Loss_G: 5.9723 (5.2309) D(x): 0.9659 D(G(z)): 0.0112 / 0.0049 Acc: 100.0000 (99.3605)\n",
      "[4/25][46/96] Loss_D: -1.3768 (-1.2811) Loss_G: 5.7234 (5.2321) D(x): 0.9734 D(G(z)): 0.0149 / 0.0103 Acc: 100.0000 (99.3619)\n",
      "[4/25][47/96] Loss_D: -1.4681 (-1.2816) Loss_G: 6.1343 (5.2342) D(x): 0.9888 D(G(z)): 0.0179 / 0.0112 Acc: 100.0000 (99.3634)\n",
      "[4/25][48/96] Loss_D: -1.4902 (-1.2820) Loss_G: 5.6171 (5.2351) D(x): 0.9679 D(G(z)): 0.0069 / 0.0154 Acc: 100.0000 (99.3649)\n",
      "[4/25][49/96] Loss_D: -1.4858 (-1.2825) Loss_G: 4.6702 (5.2338) D(x): 0.9744 D(G(z)): 0.0112 / 0.0420 Acc: 100.0000 (99.3664)\n",
      "[4/25][50/96] Loss_D: -1.4759 (-1.2829) Loss_G: 6.0336 (5.2356) D(x): 0.9911 D(G(z)): 0.0416 / 0.0102 Acc: 100.0000 (99.3678)\n",
      "[4/25][51/96] Loss_D: -1.4791 (-1.2834) Loss_G: 5.4366 (5.2361) D(x): 0.9793 D(G(z)): 0.0118 / 0.0165 Acc: 98.4375 (99.3657)\n",
      "[4/25][52/96] Loss_D: -1.3219 (-1.2835) Loss_G: 6.2251 (5.2383) D(x): 0.9891 D(G(z)): 0.0234 / 0.0087 Acc: 100.0000 (99.3671)\n",
      "[4/25][53/96] Loss_D: -1.4070 (-1.2838) Loss_G: 6.3592 (5.2409) D(x): 0.9786 D(G(z)): 0.0176 / 0.0048 Acc: 98.4375 (99.3650)\n",
      "[4/25][54/96] Loss_D: -1.4856 (-1.2842) Loss_G: 6.9213 (5.2447) D(x): 0.9716 D(G(z)): 0.0098 / 0.0033 Acc: 100.0000 (99.3665)\n",
      "[4/25][55/96] Loss_D: -1.5793 (-1.2849) Loss_G: 5.8146 (5.2460) D(x): 0.9822 D(G(z)): 0.0109 / 0.0150 Acc: 100.0000 (99.3679)\n",
      "[4/25][56/96] Loss_D: -1.4038 (-1.2852) Loss_G: 4.9575 (5.2454) D(x): 0.9732 D(G(z)): 0.0166 / 0.0252 Acc: 100.0000 (99.3693)\n",
      "[4/25][57/96] Loss_D: -1.2783 (-1.2852) Loss_G: 6.5029 (5.2482) D(x): 0.9748 D(G(z)): 0.0629 / 0.0102 Acc: 100.0000 (99.3708)\n",
      "[4/25][58/96] Loss_D: -1.3870 (-1.2854) Loss_G: 6.9768 (5.2521) D(x): 0.9726 D(G(z)): 0.0140 / 0.0070 Acc: 100.0000 (99.3722)\n",
      "[4/25][59/96] Loss_D: -1.4376 (-1.2857) Loss_G: 7.5467 (5.2573) D(x): 0.9664 D(G(z)): 0.0098 / 0.0034 Acc: 100.0000 (99.3736)\n",
      "[4/25][60/96] Loss_D: -1.5054 (-1.2862) Loss_G: 6.0459 (5.2590) D(x): 0.9898 D(G(z)): 0.0146 / 0.0099 Acc: 100.0000 (99.3750)\n",
      "[4/25][61/96] Loss_D: -1.4804 (-1.2867) Loss_G: 4.7913 (5.2580) D(x): 0.9886 D(G(z)): 0.0077 / 0.0298 Acc: 98.4375 (99.3729)\n",
      "[4/25][62/96] Loss_D: -1.5984 (-1.2874) Loss_G: 5.1439 (5.2577) D(x): 0.9878 D(G(z)): 0.0123 / 0.0204 Acc: 100.0000 (99.3743)\n",
      "[4/25][63/96] Loss_D: -1.4310 (-1.2877) Loss_G: 4.8754 (5.2569) D(x): 0.9720 D(G(z)): 0.0070 / 0.0233 Acc: 100.0000 (99.3757)\n",
      "[4/25][64/96] Loss_D: -1.2971 (-1.2877) Loss_G: 5.6428 (5.2577) D(x): 0.9772 D(G(z)): 0.0413 / 0.0247 Acc: 98.4375 (99.3736)\n",
      "[4/25][65/96] Loss_D: -1.3852 (-1.2879) Loss_G: 6.3059 (5.2601) D(x): 0.9747 D(G(z)): 0.0081 / 0.0121 Acc: 98.4375 (99.3715)\n",
      "[4/25][66/96] Loss_D: -1.4059 (-1.2882) Loss_G: 5.6299 (5.2609) D(x): 0.9621 D(G(z)): 0.0303 / 0.0143 Acc: 98.4375 (99.3695)\n",
      "[4/25][67/96] Loss_D: -1.5431 (-1.2887) Loss_G: 5.2764 (5.2609) D(x): 0.9899 D(G(z)): 0.0222 / 0.0187 Acc: 100.0000 (99.3709)\n",
      "[4/25][68/96] Loss_D: -1.2939 (-1.2887) Loss_G: 7.8504 (5.2666) D(x): 0.9909 D(G(z)): 0.0428 / 0.0016 Acc: 98.4375 (99.3688)\n",
      "[4/25][69/96] Loss_D: -1.3450 (-1.2889) Loss_G: 6.4136 (5.2692) D(x): 0.9633 D(G(z)): 0.0035 / 0.0132 Acc: 100.0000 (99.3702)\n",
      "[4/25][70/96] Loss_D: -1.4156 (-1.2891) Loss_G: 5.1141 (5.2688) D(x): 0.9619 D(G(z)): 0.0039 / 0.0246 Acc: 100.0000 (99.3716)\n",
      "[4/25][71/96] Loss_D: -1.4711 (-1.2895) Loss_G: 5.2277 (5.2687) D(x): 0.9852 D(G(z)): 0.0116 / 0.0216 Acc: 100.0000 (99.3729)\n",
      "[4/25][72/96] Loss_D: -1.4456 (-1.2899) Loss_G: 5.3644 (5.2689) D(x): 0.9771 D(G(z)): 0.0096 / 0.0194 Acc: 100.0000 (99.3743)\n",
      "[4/25][73/96] Loss_D: -1.4567 (-1.2903) Loss_G: 6.2496 (5.2711) D(x): 0.9969 D(G(z)): 0.0310 / 0.0094 Acc: 98.4375 (99.3723)\n",
      "[4/25][74/96] Loss_D: -1.4460 (-1.2906) Loss_G: 5.9757 (5.2726) D(x): 0.9890 D(G(z)): 0.0595 / 0.0110 Acc: 100.0000 (99.3736)\n",
      "[4/25][75/96] Loss_D: -1.4780 (-1.2910) Loss_G: 7.1747 (5.2768) D(x): 0.9644 D(G(z)): 0.0171 / 0.0018 Acc: 98.4375 (99.3716)\n",
      "[4/25][76/96] Loss_D: -1.3900 (-1.2912) Loss_G: 5.1906 (5.2766) D(x): 0.9334 D(G(z)): 0.0026 / 0.0167 Acc: 100.0000 (99.3730)\n",
      "[4/25][77/96] Loss_D: -1.3538 (-1.2914) Loss_G: 4.5658 (5.2750) D(x): 0.9780 D(G(z)): 0.0247 / 0.0482 Acc: 100.0000 (99.3743)\n",
      "[4/25][78/96] Loss_D: -1.5559 (-1.2919) Loss_G: 5.6443 (5.2758) D(x): 0.9945 D(G(z)): 0.0542 / 0.0119 Acc: 100.0000 (99.3757)\n",
      "[4/25][79/96] Loss_D: -1.5256 (-1.2924) Loss_G: 6.2648 (5.2780) D(x): 0.9860 D(G(z)): 0.0197 / 0.0056 Acc: 100.0000 (99.3770)\n",
      "[4/25][80/96] Loss_D: -1.4311 (-1.2927) Loss_G: 6.9980 (5.2817) D(x): 0.9697 D(G(z)): 0.0037 / 0.0050 Acc: 96.8750 (99.3716)\n",
      "[4/25][81/96] Loss_D: -1.5911 (-1.2934) Loss_G: 6.7000 (5.2847) D(x): 0.9713 D(G(z)): 0.0025 / 0.0041 Acc: 100.0000 (99.3730)\n",
      "[4/25][82/96] Loss_D: -1.4636 (-1.2937) Loss_G: 5.8785 (5.2860) D(x): 0.9837 D(G(z)): 0.0175 / 0.0097 Acc: 98.4375 (99.3710)\n",
      "[4/25][83/96] Loss_D: -1.4758 (-1.2941) Loss_G: 5.1047 (5.2856) D(x): 0.9828 D(G(z)): 0.0057 / 0.0254 Acc: 100.0000 (99.3723)\n",
      "[4/25][84/96] Loss_D: -1.3900 (-1.2943) Loss_G: 6.2263 (5.2876) D(x): 0.9911 D(G(z)): 0.0212 / 0.0075 Acc: 100.0000 (99.3737)\n",
      "[4/25][85/96] Loss_D: -1.5033 (-1.2948) Loss_G: 6.6365 (5.2905) D(x): 0.9882 D(G(z)): 0.0144 / 0.0062 Acc: 100.0000 (99.3750)\n",
      "[4/25][86/96] Loss_D: -1.3581 (-1.2949) Loss_G: 5.1900 (5.2903) D(x): 0.9771 D(G(z)): 0.0070 / 0.0322 Acc: 100.0000 (99.3763)\n",
      "[4/25][87/96] Loss_D: -1.4776 (-1.2953) Loss_G: 5.1406 (5.2899) D(x): 0.9912 D(G(z)): 0.0261 / 0.0313 Acc: 98.4375 (99.3743)\n",
      "[4/25][88/96] Loss_D: -1.2769 (-1.2952) Loss_G: 6.9914 (5.2935) D(x): 0.9845 D(G(z)): 0.0150 / 0.0057 Acc: 95.3125 (99.3658)\n",
      "[4/25][89/96] Loss_D: -1.4124 (-1.2955) Loss_G: 6.9380 (5.2970) D(x): 0.9849 D(G(z)): 0.0093 / 0.0063 Acc: 100.0000 (99.3671)\n",
      "[4/25][90/96] Loss_D: -1.5405 (-1.2960) Loss_G: 5.4204 (5.2973) D(x): 0.9766 D(G(z)): 0.0105 / 0.0077 Acc: 100.0000 (99.3684)\n",
      "[4/25][91/96] Loss_D: -1.4540 (-1.2963) Loss_G: 6.9179 (5.3007) D(x): 0.9868 D(G(z)): 0.0288 / 0.0047 Acc: 100.0000 (99.3697)\n",
      "[4/25][92/96] Loss_D: -1.5830 (-1.2969) Loss_G: 7.6151 (5.3055) D(x): 0.9858 D(G(z)): 0.0100 / 0.0045 Acc: 100.0000 (99.3711)\n",
      "[4/25][93/96] Loss_D: -1.3853 (-1.2971) Loss_G: 5.2257 (5.3054) D(x): 0.9425 D(G(z)): 0.0165 / 0.0261 Acc: 100.0000 (99.3724)\n",
      "[4/25][94/96] Loss_D: -1.5125 (-1.2976) Loss_G: 5.2844 (5.3053) D(x): 0.9869 D(G(z)): 0.0311 / 0.0161 Acc: 100.0000 (99.3737)\n",
      "[4/25][95/96] Loss_D: -1.5798 (-1.2982) Loss_G: 7.0292 (5.3089) D(x): 0.9824 D(G(z)): 0.0076 / 0.0060 Acc: 100.0000 (99.3750)\n",
      "[5/25][0/96] Loss_D: -1.4742 (-1.2985) Loss_G: 6.8656 (5.3121) D(x): 0.9791 D(G(z)): 0.0032 / 0.0044 Acc: 100.0000 (99.3763)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[5/25][1/96] Loss_D: -1.5328 (-1.2990) Loss_G: 6.4882 (5.3146) D(x): 0.9937 D(G(z)): 0.0189 / 0.0084 Acc: 100.0000 (99.3776)\n",
      "[5/25][2/96] Loss_D: -1.3813 (-1.2992) Loss_G: 6.3163 (5.3166) D(x): 0.9907 D(G(z)): 0.0139 / 0.0175 Acc: 100.0000 (99.3789)\n",
      "[5/25][3/96] Loss_D: -1.4637 (-1.2995) Loss_G: 7.1151 (5.3204) D(x): 0.9958 D(G(z)): 0.0143 / 0.0055 Acc: 100.0000 (99.3802)\n",
      "[5/25][4/96] Loss_D: -1.4042 (-1.2997) Loss_G: 7.3464 (5.3245) D(x): 0.9838 D(G(z)): 0.0139 / 0.0038 Acc: 98.4375 (99.3782)\n",
      "[5/25][5/96] Loss_D: -1.4399 (-1.3000) Loss_G: 6.0082 (5.3259) D(x): 0.9517 D(G(z)): 0.0038 / 0.0068 Acc: 100.0000 (99.3795)\n",
      "[5/25][6/96] Loss_D: -1.4299 (-1.3003) Loss_G: 3.7296 (5.3227) D(x): 0.9595 D(G(z)): 0.0053 / 0.0482 Acc: 100.0000 (99.3808)\n",
      "[5/25][7/96] Loss_D: -1.3926 (-1.3005) Loss_G: 7.0184 (5.3261) D(x): 0.9989 D(G(z)): 0.0636 / 0.0043 Acc: 100.0000 (99.3820)\n",
      "[5/25][8/96] Loss_D: -1.4316 (-1.3008) Loss_G: 7.2854 (5.3302) D(x): 0.9943 D(G(z)): 0.0147 / 0.0015 Acc: 100.0000 (99.3833)\n",
      "[5/25][9/96] Loss_D: -1.5139 (-1.3012) Loss_G: 6.3502 (5.3322) D(x): 0.9734 D(G(z)): 0.0029 / 0.0059 Acc: 100.0000 (99.3846)\n",
      "[5/25][10/96] Loss_D: -1.4954 (-1.3016) Loss_G: 6.7952 (5.3352) D(x): 0.9768 D(G(z)): 0.0106 / 0.0072 Acc: 100.0000 (99.3858)\n",
      "[5/25][11/96] Loss_D: -1.5584 (-1.3021) Loss_G: 6.3027 (5.3372) D(x): 0.9932 D(G(z)): 0.0124 / 0.0125 Acc: 100.0000 (99.3871)\n",
      "[5/25][12/96] Loss_D: -1.4487 (-1.3024) Loss_G: 6.5735 (5.3397) D(x): 0.9937 D(G(z)): 0.0123 / 0.0083 Acc: 100.0000 (99.3883)\n",
      "[5/25][13/96] Loss_D: -1.4091 (-1.3026) Loss_G: 6.3325 (5.3417) D(x): 0.9776 D(G(z)): 0.0046 / 0.0074 Acc: 100.0000 (99.3895)\n",
      "[5/25][14/96] Loss_D: -1.5625 (-1.3031) Loss_G: 6.1563 (5.3433) D(x): 0.9919 D(G(z)): 0.0218 / 0.0043 Acc: 96.8750 (99.3845)\n",
      "[5/25][15/96] Loss_D: -1.5531 (-1.3037) Loss_G: 6.0629 (5.3448) D(x): 0.9975 D(G(z)): 0.0067 / 0.0080 Acc: 100.0000 (99.3857)\n",
      "[5/25][16/96] Loss_D: -1.5647 (-1.3042) Loss_G: 6.0707 (5.3463) D(x): 0.9951 D(G(z)): 0.0082 / 0.0079 Acc: 100.0000 (99.3869)\n",
      "[5/25][17/96] Loss_D: -1.3172 (-1.3042) Loss_G: 5.8089 (5.3472) D(x): 0.9691 D(G(z)): 0.0051 / 0.0145 Acc: 100.0000 (99.3882)\n",
      "[5/25][18/96] Loss_D: -1.4773 (-1.3046) Loss_G: 5.4333 (5.3474) D(x): 0.9872 D(G(z)): 0.0089 / 0.0127 Acc: 98.4375 (99.3863)\n",
      "[5/25][19/96] Loss_D: -1.4064 (-1.3048) Loss_G: 7.0829 (5.3508) D(x): 0.9974 D(G(z)): 0.0368 / 0.0032 Acc: 100.0000 (99.3875)\n",
      "[5/25][20/96] Loss_D: -1.5022 (-1.3051) Loss_G: 6.6513 (5.3534) D(x): 0.9766 D(G(z)): 0.0023 / 0.0046 Acc: 100.0000 (99.3887)\n",
      "[5/25][21/96] Loss_D: -1.4947 (-1.3055) Loss_G: 6.7120 (5.3561) D(x): 0.9905 D(G(z)): 0.0107 / 0.0039 Acc: 98.4375 (99.3868)\n",
      "[5/25][22/96] Loss_D: -1.5444 (-1.3060) Loss_G: 6.4679 (5.3583) D(x): 0.9725 D(G(z)): 0.0304 / 0.0065 Acc: 100.0000 (99.3880)\n",
      "[5/25][23/96] Loss_D: -1.5513 (-1.3065) Loss_G: 5.8291 (5.3593) D(x): 0.9846 D(G(z)): 0.0099 / 0.0088 Acc: 98.4375 (99.3862)\n",
      "[5/25][24/96] Loss_D: -1.4051 (-1.3067) Loss_G: 7.9440 (5.3644) D(x): 0.9909 D(G(z)): 0.0316 / 0.0012 Acc: 100.0000 (99.3874)\n",
      "[5/25][25/96] Loss_D: -1.4548 (-1.3070) Loss_G: 5.7655 (5.3652) D(x): 0.9427 D(G(z)): 0.0014 / 0.0186 Acc: 100.0000 (99.3886)\n",
      "[5/25][26/96] Loss_D: -1.4334 (-1.3072) Loss_G: 5.3304 (5.3651) D(x): 0.9890 D(G(z)): 0.0169 / 0.0181 Acc: 100.0000 (99.3898)\n",
      "[5/25][27/96] Loss_D: -1.4991 (-1.3076) Loss_G: 5.0813 (5.3646) D(x): 0.9809 D(G(z)): 0.0109 / 0.0245 Acc: 100.0000 (99.3910)\n",
      "[5/25][28/96] Loss_D: -1.4310 (-1.3078) Loss_G: 4.8543 (5.3636) D(x): 0.9951 D(G(z)): 0.0163 / 0.0368 Acc: 100.0000 (99.3922)\n",
      "[5/25][29/96] Loss_D: -1.4274 (-1.3081) Loss_G: 6.8599 (5.3665) D(x): 0.9972 D(G(z)): 0.0072 / 0.0039 Acc: 98.4375 (99.3903)\n",
      "[5/25][30/96] Loss_D: -1.4688 (-1.3084) Loss_G: 7.5062 (5.3707) D(x): 0.9923 D(G(z)): 0.0326 / 0.0018 Acc: 98.4375 (99.3885)\n",
      "[5/25][31/96] Loss_D: -1.4873 (-1.3087) Loss_G: 5.8006 (5.3715) D(x): 0.9664 D(G(z)): 0.0039 / 0.0083 Acc: 98.4375 (99.3866)\n",
      "[5/25][32/96] Loss_D: -1.4054 (-1.3089) Loss_G: 4.3427 (5.3695) D(x): 0.9633 D(G(z)): 0.0113 / 0.0518 Acc: 100.0000 (99.3878)\n",
      "[5/25][33/96] Loss_D: -1.5181 (-1.3093) Loss_G: 5.7145 (5.3702) D(x): 0.9930 D(G(z)): 0.0058 / 0.0156 Acc: 100.0000 (99.3890)\n",
      "[5/25][34/96] Loss_D: -1.4831 (-1.3097) Loss_G: 5.9028 (5.3712) D(x): 0.9963 D(G(z)): 0.0270 / 0.0088 Acc: 100.0000 (99.3902)\n",
      "[5/25][35/96] Loss_D: -1.4614 (-1.3100) Loss_G: 7.5372 (5.3754) D(x): 0.9962 D(G(z)): 0.0034 / 0.0020 Acc: 98.4375 (99.3883)\n",
      "[5/25][36/96] Loss_D: -1.5153 (-1.3104) Loss_G: 5.3745 (5.3754) D(x): 0.9848 D(G(z)): 0.0146 / 0.0196 Acc: 100.0000 (99.3895)\n",
      "[5/25][37/96] Loss_D: -1.5044 (-1.3107) Loss_G: 6.5743 (5.3777) D(x): 0.9837 D(G(z)): 0.0228 / 0.0051 Acc: 96.8750 (99.3847)\n",
      "[5/25][38/96] Loss_D: -1.4449 (-1.3110) Loss_G: 6.7032 (5.3803) D(x): 0.9777 D(G(z)): 0.0072 / 0.0046 Acc: 98.4375 (99.3828)\n",
      "[5/25][39/96] Loss_D: -1.5746 (-1.3115) Loss_G: 6.5299 (5.3825) D(x): 0.9851 D(G(z)): 0.0170 / 0.0059 Acc: 100.0000 (99.3840)\n",
      "[5/25][40/96] Loss_D: -1.5531 (-1.3120) Loss_G: 7.2576 (5.3861) D(x): 0.9792 D(G(z)): 0.0123 / 0.0023 Acc: 100.0000 (99.3852)\n",
      "[5/25][41/96] Loss_D: -1.4798 (-1.3123) Loss_G: 5.5149 (5.3863) D(x): 0.9766 D(G(z)): 0.0098 / 0.0104 Acc: 98.4375 (99.3834)\n",
      "[5/25][42/96] Loss_D: -1.5492 (-1.3127) Loss_G: 6.4745 (5.3884) D(x): 0.9955 D(G(z)): 0.0230 / 0.0092 Acc: 100.0000 (99.3846)\n",
      "[5/25][43/96] Loss_D: -1.3091 (-1.3127) Loss_G: 1.4699 (5.3809) D(x): 0.9038 D(G(z)): 0.0037 / 0.2881 Acc: 100.0000 (99.3857)\n",
      "[5/25][44/96] Loss_D: -1.1928 (-1.3125) Loss_G: 11.3362 (5.3923) D(x): 0.9997 D(G(z)): 0.2450 / 0.0001 Acc: 100.0000 (99.3869)\n",
      "[5/25][45/96] Loss_D: -0.0583 (-1.3101) Loss_G: -0.4331 (5.3812) D(x): 0.4280 D(G(z)): 0.0001 / 0.9956 Acc: 100.0000 (99.3881)\n",
      "[5/25][46/96] Loss_D: 5.1860 (-1.2978) Loss_G: 3.5724 (5.3778) D(x): 1.0000 D(G(z)): 0.9965 / 0.0869 Acc: 100.0000 (99.3892)\n",
      "[5/25][47/96] Loss_D: -0.1690 (-1.2957) Loss_G: 3.2424 (5.3737) D(x): 0.5479 D(G(z)): 0.1692 / 0.0868 Acc: 100.0000 (99.3904)\n",
      "[5/25][48/96] Loss_D: -0.8512 (-1.2948) Loss_G: 2.6796 (5.3686) D(x): 0.6979 D(G(z)): 0.0814 / 0.1038 Acc: 100.0000 (99.3915)\n",
      "[5/25][49/96] Loss_D: -1.2471 (-1.2947) Loss_G: 4.3787 (5.3668) D(x): 0.9423 D(G(z)): 0.1401 / 0.0245 Acc: 100.0000 (99.3927)\n",
      "[5/25][50/96] Loss_D: -1.4564 (-1.2950) Loss_G: 5.3412 (5.3667) D(x): 0.9609 D(G(z)): 0.0161 / 0.0071 Acc: 100.0000 (99.3938)\n",
      "[5/25][51/96] Loss_D: -1.4269 (-1.2953) Loss_G: 4.7743 (5.3656) D(x): 0.9732 D(G(z)): 0.0267 / 0.0305 Acc: 100.0000 (99.3950)\n",
      "[5/25][52/96] Loss_D: -1.3378 (-1.2954) Loss_G: 3.7063 (5.3625) D(x): 0.9203 D(G(z)): 0.0318 / 0.0655 Acc: 100.0000 (99.3961)\n",
      "[5/25][53/96] Loss_D: -1.4478 (-1.2956) Loss_G: 4.4062 (5.3607) D(x): 0.9647 D(G(z)): 0.0398 / 0.0199 Acc: 98.4375 (99.3943)\n",
      "[5/25][54/96] Loss_D: -1.2771 (-1.2956) Loss_G: 6.1823 (5.3622) D(x): 0.9824 D(G(z)): 0.1178 / 0.0030 Acc: 100.0000 (99.3954)\n",
      "[5/25][55/96] Loss_D: -1.1179 (-1.2953) Loss_G: 4.9798 (5.3615) D(x): 0.8449 D(G(z)): 0.0042 / 0.0219 Acc: 100.0000 (99.3966)\n",
      "[5/25][56/96] Loss_D: -1.3609 (-1.2954) Loss_G: 3.0636 (5.3572) D(x): 0.9329 D(G(z)): 0.0325 / 0.0556 Acc: 100.0000 (99.3977)\n",
      "[5/25][57/96] Loss_D: -1.2675 (-1.2954) Loss_G: 4.6133 (5.3559) D(x): 0.9873 D(G(z)): 0.0905 / 0.0246 Acc: 100.0000 (99.3988)\n",
      "[5/25][58/96] Loss_D: -1.4532 (-1.2956) Loss_G: 5.1218 (5.3554) D(x): 0.9592 D(G(z)): 0.0294 / 0.0121 Acc: 100.0000 (99.3999)\n",
      "[5/25][59/96] Loss_D: -1.4480 (-1.2959) Loss_G: 5.6994 (5.3561) D(x): 0.9393 D(G(z)): 0.0154 / 0.0088 Acc: 100.0000 (99.4010)\n",
      "[5/25][60/96] Loss_D: -1.4584 (-1.2962) Loss_G: 5.2851 (5.3559) D(x): 0.9587 D(G(z)): 0.0098 / 0.0086 Acc: 100.0000 (99.4021)\n",
      "[5/25][61/96] Loss_D: -1.4832 (-1.2966) Loss_G: 4.7627 (5.3548) D(x): 0.9872 D(G(z)): 0.0186 / 0.0181 Acc: 100.0000 (99.4033)\n",
      "[5/25][62/96] Loss_D: -1.4916 (-1.2969) Loss_G: 4.7507 (5.3537) D(x): 0.9720 D(G(z)): 0.0084 / 0.0198 Acc: 100.0000 (99.4044)\n",
      "[5/25][63/96] Loss_D: -1.3739 (-1.2971) Loss_G: 5.2327 (5.3535) D(x): 0.9858 D(G(z)): 0.0282 / 0.0092 Acc: 96.8750 (99.3997)\n",
      "[5/25][64/96] Loss_D: -1.3835 (-1.2972) Loss_G: 5.4018 (5.3536) D(x): 0.9772 D(G(z)): 0.0263 / 0.0183 Acc: 98.4375 (99.3979)\n",
      "[5/25][65/96] Loss_D: -1.4296 (-1.2975) Loss_G: 5.5429 (5.3539) D(x): 0.9888 D(G(z)): 0.0116 / 0.0073 Acc: 98.4375 (99.3962)\n",
      "[5/25][66/96] Loss_D: -1.5521 (-1.2979) Loss_G: 4.7858 (5.3529) D(x): 0.9725 D(G(z)): 0.0101 / 0.0124 Acc: 98.4375 (99.3944)\n",
      "[5/25][67/96] Loss_D: -1.4090 (-1.2981) Loss_G: 5.3681 (5.3529) D(x): 0.9848 D(G(z)): 0.0124 / 0.0099 Acc: 100.0000 (99.3955)\n",
      "[5/25][68/96] Loss_D: -1.4336 (-1.2984) Loss_G: 4.7237 (5.3518) D(x): 0.9800 D(G(z)): 0.0135 / 0.0184 Acc: 100.0000 (99.3966)\n",
      "[5/25][69/96] Loss_D: -1.3987 (-1.2986) Loss_G: 4.6702 (5.3505) D(x): 0.9876 D(G(z)): 0.0371 / 0.0215 Acc: 98.4375 (99.3949)\n",
      "[5/25][70/96] Loss_D: -1.4723 (-1.2989) Loss_G: 5.1985 (5.3503) D(x): 0.9838 D(G(z)): 0.0101 / 0.0118 Acc: 96.8750 (99.3903)\n",
      "[5/25][71/96] Loss_D: -1.3859 (-1.2990) Loss_G: 4.8046 (5.3493) D(x): 0.9744 D(G(z)): 0.0084 / 0.0209 Acc: 98.4375 (99.3886)\n",
      "[5/25][72/96] Loss_D: -1.5594 (-1.2995) Loss_G: 5.0950 (5.3488) D(x): 0.9730 D(G(z)): 0.0450 / 0.0128 Acc: 100.0000 (99.3897)\n",
      "[5/25][73/96] Loss_D: -1.3462 (-1.2996) Loss_G: 5.8645 (5.3498) D(x): 0.9423 D(G(z)): 0.0394 / 0.0056 Acc: 100.0000 (99.3908)\n",
      "[5/25][74/96] Loss_D: -1.4745 (-1.2999) Loss_G: 5.1948 (5.3495) D(x): 0.9631 D(G(z)): 0.0283 / 0.0121 Acc: 100.0000 (99.3919)\n",
      "[5/25][75/96] Loss_D: -1.3795 (-1.3001) Loss_G: 5.1206 (5.3491) D(x): 0.9820 D(G(z)): 0.0206 / 0.0108 Acc: 100.0000 (99.3930)\n",
      "[5/25][76/96] Loss_D: -1.4333 (-1.3003) Loss_G: 5.7700 (5.3498) D(x): 0.9759 D(G(z)): 0.0088 / 0.0082 Acc: 98.4375 (99.3913)\n",
      "[5/25][77/96] Loss_D: -1.5715 (-1.3008) Loss_G: 4.1299 (5.3476) D(x): 0.9723 D(G(z)): 0.0057 / 0.0370 Acc: 100.0000 (99.3924)\n",
      "[5/25][78/96] Loss_D: -1.3132 (-1.3008) Loss_G: 5.0651 (5.3471) D(x): 0.9928 D(G(z)): 0.0586 / 0.0169 Acc: 98.4375 (99.3907)\n",
      "[5/25][79/96] Loss_D: -1.5005 (-1.3012) Loss_G: 6.1656 (5.3486) D(x): 0.9880 D(G(z)): 0.0179 / 0.0045 Acc: 100.0000 (99.3917)\n",
      "[5/25][80/96] Loss_D: -1.4611 (-1.3014) Loss_G: 5.4623 (5.3488) D(x): 0.9477 D(G(z)): 0.0049 / 0.0068 Acc: 100.0000 (99.3928)\n",
      "[5/25][81/96] Loss_D: -1.4053 (-1.3016) Loss_G: 4.5734 (5.3474) D(x): 0.9720 D(G(z)): 0.0146 / 0.0251 Acc: 100.0000 (99.3939)\n",
      "[5/25][82/96] Loss_D: -1.4197 (-1.3018) Loss_G: 3.8929 (5.3448) D(x): 0.9872 D(G(z)): 0.0193 / 0.0336 Acc: 96.8750 (99.3894)\n",
      "[5/25][83/96] Loss_D: -1.4567 (-1.3021) Loss_G: 4.0195 (5.3425) D(x): 0.9790 D(G(z)): 0.0057 / 0.0457 Acc: 98.4375 (99.3877)\n",
      "[5/25][84/96] Loss_D: -1.5250 (-1.3025) Loss_G: 5.2879 (5.3424) D(x): 0.9920 D(G(z)): 0.0134 / 0.0097 Acc: 100.0000 (99.3888)\n",
      "[5/25][85/96] Loss_D: -1.3397 (-1.3026) Loss_G: 6.1123 (5.3437) D(x): 0.9887 D(G(z)): 0.0436 / 0.0079 Acc: 100.0000 (99.3899)\n",
      "[5/25][86/96] Loss_D: -1.3473 (-1.3027) Loss_G: 4.8197 (5.3428) D(x): 0.9511 D(G(z)): 0.0146 / 0.0217 Acc: 100.0000 (99.3910)\n",
      "[5/25][87/96] Loss_D: -1.5348 (-1.3031) Loss_G: 5.6600 (5.3434) D(x): 0.9860 D(G(z)): 0.0404 / 0.0049 Acc: 100.0000 (99.3921)\n",
      "[5/25][88/96] Loss_D: -1.4405 (-1.3033) Loss_G: 5.2162 (5.3432) D(x): 0.9714 D(G(z)): 0.0253 / 0.0264 Acc: 100.0000 (99.3931)\n",
      "[5/25][89/96] Loss_D: -1.4773 (-1.3036) Loss_G: 3.7768 (5.3404) D(x): 0.9334 D(G(z)): 0.0048 / 0.0361 Acc: 100.0000 (99.3942)\n",
      "[5/25][90/96] Loss_D: -1.4291 (-1.3038) Loss_G: 3.9314 (5.3379) D(x): 0.9837 D(G(z)): 0.0072 / 0.0366 Acc: 100.0000 (99.3952)\n",
      "[5/25][91/96] Loss_D: -1.5585 (-1.3043) Loss_G: 4.5859 (5.3366) D(x): 0.9948 D(G(z)): 0.0412 / 0.0181 Acc: 100.0000 (99.3963)\n",
      "[5/25][92/96] Loss_D: -1.4291 (-1.3045) Loss_G: 5.5822 (5.3370) D(x): 0.9792 D(G(z)): 0.0152 / 0.0094 Acc: 100.0000 (99.3974)\n",
      "[5/25][93/96] Loss_D: -1.4363 (-1.3047) Loss_G: 4.5815 (5.3357) D(x): 0.9867 D(G(z)): 0.0314 / 0.0249 Acc: 98.4375 (99.3957)\n",
      "[5/25][94/96] Loss_D: -1.4912 (-1.3050) Loss_G: 5.4634 (5.3360) D(x): 0.9849 D(G(z)): 0.0085 / 0.0071 Acc: 100.0000 (99.3967)\n",
      "[5/25][95/96] Loss_D: -1.5004 (-1.3054) Loss_G: 5.6029 (5.3364) D(x): 0.9868 D(G(z)): 0.0150 / 0.0057 Acc: 100.0000 (99.3978)\n",
      "[6/25][0/96] Loss_D: -1.5133 (-1.3057) Loss_G: 6.7888 (5.3389) D(x): 0.9803 D(G(z)): 0.0110 / 0.0022 Acc: 100.0000 (99.3988)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[6/25][1/96] Loss_D: -1.5229 (-1.3061) Loss_G: 4.3539 (5.3372) D(x): 0.9657 D(G(z)): 0.0134 / 0.0274 Acc: 100.0000 (99.3999)\n",
      "[6/25][2/96] Loss_D: -1.4852 (-1.3064) Loss_G: 3.6023 (5.3342) D(x): 0.9820 D(G(z)): 0.0070 / 0.0505 Acc: 100.0000 (99.4009)\n",
      "[6/25][3/96] Loss_D: -1.5125 (-1.3068) Loss_G: 4.0187 (5.3320) D(x): 0.9785 D(G(z)): 0.0097 / 0.0372 Acc: 100.0000 (99.4019)\n",
      "[6/25][4/96] Loss_D: -1.4283 (-1.3070) Loss_G: 4.0696 (5.3298) D(x): 0.9744 D(G(z)): 0.0257 / 0.0347 Acc: 98.4375 (99.4003)\n",
      "[6/25][5/96] Loss_D: -1.3313 (-1.3070) Loss_G: 6.5455 (5.3319) D(x): 0.9973 D(G(z)): 0.0800 / 0.0037 Acc: 100.0000 (99.4013)\n",
      "[6/25][6/96] Loss_D: -1.5021 (-1.3074) Loss_G: 6.5589 (5.3340) D(x): 0.9539 D(G(z)): 0.0027 / 0.0020 Acc: 100.0000 (99.4023)\n",
      "[6/25][7/96] Loss_D: -1.4455 (-1.3076) Loss_G: 5.4973 (5.3343) D(x): 0.9501 D(G(z)): 0.0101 / 0.0074 Acc: 100.0000 (99.4034)\n",
      "[6/25][8/96] Loss_D: -1.4584 (-1.3079) Loss_G: 3.6817 (5.3314) D(x): 0.9701 D(G(z)): 0.0097 / 0.0353 Acc: 100.0000 (99.4044)\n",
      "[6/25][9/96] Loss_D: -1.3818 (-1.3080) Loss_G: 5.1673 (5.3312) D(x): 0.9823 D(G(z)): 0.0342 / 0.0107 Acc: 100.0000 (99.4054)\n",
      "[6/25][10/96] Loss_D: -1.4829 (-1.3083) Loss_G: 5.9872 (5.3323) D(x): 0.9889 D(G(z)): 0.0337 / 0.0046 Acc: 100.0000 (99.4064)\n",
      "[6/25][11/96] Loss_D: -1.4601 (-1.3085) Loss_G: 6.0301 (5.3335) D(x): 0.9909 D(G(z)): 0.0288 / 0.0047 Acc: 98.4375 (99.4048)\n",
      "[6/25][12/96] Loss_D: -1.4317 (-1.3088) Loss_G: 5.7908 (5.3342) D(x): 0.9841 D(G(z)): 0.0180 / 0.0097 Acc: 100.0000 (99.4058)\n",
      "[6/25][13/96] Loss_D: -1.3918 (-1.3089) Loss_G: 3.7613 (5.3316) D(x): 0.8845 D(G(z)): 0.0066 / 0.0513 Acc: 100.0000 (99.4068)\n",
      "[6/25][14/96] Loss_D: -1.4098 (-1.3091) Loss_G: 4.4376 (5.3301) D(x): 0.9977 D(G(z)): 0.0650 / 0.0221 Acc: 100.0000 (99.4078)\n",
      "[6/25][15/96] Loss_D: -1.4869 (-1.3094) Loss_G: 5.3449 (5.3301) D(x): 0.9885 D(G(z)): 0.0439 / 0.0166 Acc: 98.4375 (99.4061)\n",
      "[6/25][16/96] Loss_D: -1.4299 (-1.3096) Loss_G: 5.9620 (5.3312) D(x): 0.9729 D(G(z)): 0.0077 / 0.0058 Acc: 96.8750 (99.4019)\n",
      "[6/25][17/96] Loss_D: -1.4546 (-1.3098) Loss_G: 6.2848 (5.3328) D(x): 0.9840 D(G(z)): 0.0401 / 0.0030 Acc: 100.0000 (99.4029)\n",
      "[6/25][18/96] Loss_D: -1.2950 (-1.3098) Loss_G: 3.5517 (5.3298) D(x): 0.8929 D(G(z)): 0.0019 / 0.0424 Acc: 100.0000 (99.4039)\n",
      "[6/25][19/96] Loss_D: -1.4125 (-1.3100) Loss_G: 4.9522 (5.3291) D(x): 0.9952 D(G(z)): 0.0745 / 0.0121 Acc: 96.8750 (99.3996)\n",
      "[6/25][20/96] Loss_D: -1.4165 (-1.3101) Loss_G: 5.7647 (5.3299) D(x): 0.9919 D(G(z)): 0.0373 / 0.0052 Acc: 98.4375 (99.3980)\n",
      "[6/25][21/96] Loss_D: -1.5317 (-1.3105) Loss_G: 5.4686 (5.3301) D(x): 0.9550 D(G(z)): 0.0043 / 0.0071 Acc: 98.4375 (99.3964)\n",
      "[6/25][22/96] Loss_D: -1.4133 (-1.3107) Loss_G: 5.0294 (5.3296) D(x): 0.9844 D(G(z)): 0.0079 / 0.0203 Acc: 96.8750 (99.3922)\n",
      "[6/25][23/96] Loss_D: -1.3543 (-1.3108) Loss_G: 3.7776 (5.3270) D(x): 0.9709 D(G(z)): 0.0036 / 0.0547 Acc: 100.0000 (99.3932)\n",
      "[6/25][24/96] Loss_D: -1.3485 (-1.3108) Loss_G: 6.5433 (5.3290) D(x): 0.9927 D(G(z)): 0.0454 / 0.0032 Acc: 100.0000 (99.3942)\n",
      "[6/25][25/96] Loss_D: -1.4291 (-1.3110) Loss_G: 6.1050 (5.3303) D(x): 0.9944 D(G(z)): 0.0176 / 0.0058 Acc: 98.4375 (99.3926)\n",
      "[6/25][26/96] Loss_D: -1.4296 (-1.3112) Loss_G: 5.8779 (5.3312) D(x): 0.9809 D(G(z)): 0.0038 / 0.0066 Acc: 100.0000 (99.3937)\n",
      "[6/25][27/96] Loss_D: -1.5200 (-1.3116) Loss_G: 5.0655 (5.3308) D(x): 0.9806 D(G(z)): 0.0065 / 0.0099 Acc: 100.0000 (99.3947)\n",
      "[6/25][28/96] Loss_D: -1.5478 (-1.3120) Loss_G: 3.5282 (5.3278) D(x): 0.9776 D(G(z)): 0.0068 / 0.0469 Acc: 98.4375 (99.3931)\n",
      "[6/25][29/96] Loss_D: -1.5035 (-1.3123) Loss_G: 5.0004 (5.3273) D(x): 0.9913 D(G(z)): 0.0181 / 0.0151 Acc: 98.4375 (99.3915)\n",
      "[6/25][30/96] Loss_D: -1.5406 (-1.3126) Loss_G: 4.2135 (5.3254) D(x): 0.9957 D(G(z)): 0.0168 / 0.0301 Acc: 100.0000 (99.3925)\n",
      "[6/25][31/96] Loss_D: -1.4965 (-1.3129) Loss_G: 5.4571 (5.3257) D(x): 0.9747 D(G(z)): 0.0330 / 0.0090 Acc: 100.0000 (99.3935)\n",
      "[6/25][32/96] Loss_D: -1.4579 (-1.3132) Loss_G: 4.7062 (5.3246) D(x): 0.9822 D(G(z)): 0.0077 / 0.0179 Acc: 100.0000 (99.3945)\n",
      "[6/25][33/96] Loss_D: -1.4422 (-1.3134) Loss_G: 6.5647 (5.3267) D(x): 0.9796 D(G(z)): 0.0051 / 0.0064 Acc: 100.0000 (99.3955)\n",
      "[6/25][34/96] Loss_D: -1.4318 (-1.3136) Loss_G: 2.6104 (5.3222) D(x): 0.9738 D(G(z)): 0.0224 / 0.1292 Acc: 100.0000 (99.3965)\n",
      "[6/25][35/96] Loss_D: -1.4167 (-1.3138) Loss_G: 3.6972 (5.3196) D(x): 0.9481 D(G(z)): 0.0386 / 0.0415 Acc: 100.0000 (99.3975)\n",
      "[6/25][36/96] Loss_D: -1.5665 (-1.3142) Loss_G: 4.8424 (5.3188) D(x): 0.9880 D(G(z)): 0.0145 / 0.0220 Acc: 100.0000 (99.3985)\n",
      "[6/25][37/96] Loss_D: -1.3959 (-1.3143) Loss_G: 5.3515 (5.3188) D(x): 0.9786 D(G(z)): 0.0429 / 0.0122 Acc: 100.0000 (99.3994)\n",
      "[6/25][38/96] Loss_D: -1.3869 (-1.3144) Loss_G: 4.7200 (5.3179) D(x): 0.9632 D(G(z)): 0.0096 / 0.0278 Acc: 100.0000 (99.4004)\n",
      "[6/25][39/96] Loss_D: -1.3653 (-1.3145) Loss_G: 5.2020 (5.3177) D(x): 0.9881 D(G(z)): 0.0129 / 0.0099 Acc: 98.4375 (99.3988)\n",
      "[6/25][40/96] Loss_D: -1.5472 (-1.3149) Loss_G: 5.5234 (5.3180) D(x): 0.9907 D(G(z)): 0.0056 / 0.0066 Acc: 100.0000 (99.3998)\n",
      "[6/25][41/96] Loss_D: -1.5374 (-1.3152) Loss_G: 6.2838 (5.3196) D(x): 0.9931 D(G(z)): 0.0587 / 0.0025 Acc: 100.0000 (99.4008)\n",
      "[6/25][42/96] Loss_D: -1.4037 (-1.3154) Loss_G: 5.6516 (5.3201) D(x): 0.9615 D(G(z)): 0.0069 / 0.0087 Acc: 100.0000 (99.4018)\n",
      "[6/25][43/96] Loss_D: -1.4550 (-1.3156) Loss_G: 5.2612 (5.3200) D(x): 0.9596 D(G(z)): 0.0146 / 0.0177 Acc: 100.0000 (99.4027)\n",
      "[6/25][44/96] Loss_D: -1.3534 (-1.3157) Loss_G: 5.7947 (5.3208) D(x): 0.9871 D(G(z)): 0.0582 / 0.0079 Acc: 100.0000 (99.4037)\n",
      "[6/25][45/96] Loss_D: -1.4168 (-1.3158) Loss_G: 5.4031 (5.3209) D(x): 0.9664 D(G(z)): 0.0152 / 0.0087 Acc: 100.0000 (99.4046)\n",
      "[6/25][46/96] Loss_D: -1.3359 (-1.3159) Loss_G: 2.9873 (5.3172) D(x): 0.9150 D(G(z)): 0.0085 / 0.0814 Acc: 100.0000 (99.4056)\n",
      "[6/25][47/96] Loss_D: -1.2661 (-1.3158) Loss_G: 7.4845 (5.3206) D(x): 0.9967 D(G(z)): 0.1123 / 0.0022 Acc: 100.0000 (99.4066)\n",
      "[6/25][48/96] Loss_D: -1.4483 (-1.3160) Loss_G: 6.7017 (5.3228) D(x): 0.9599 D(G(z)): 0.0047 / 0.0026 Acc: 100.0000 (99.4075)\n",
      "[6/25][49/96] Loss_D: -1.5331 (-1.3163) Loss_G: 5.4981 (5.3231) D(x): 0.9496 D(G(z)): 0.0009 / 0.0070 Acc: 100.0000 (99.4084)\n",
      "[6/25][50/96] Loss_D: -1.5904 (-1.3168) Loss_G: 5.8413 (5.3240) D(x): 0.9868 D(G(z)): 0.0340 / 0.0071 Acc: 100.0000 (99.4094)\n",
      "[6/25][51/96] Loss_D: -1.4333 (-1.3170) Loss_G: 6.8887 (5.3264) D(x): 0.9838 D(G(z)): 0.0029 / 0.0022 Acc: 100.0000 (99.4103)\n",
      "[6/25][52/96] Loss_D: -1.4534 (-1.3172) Loss_G: 5.5340 (5.3268) D(x): 0.9916 D(G(z)): 0.0067 / 0.0059 Acc: 100.0000 (99.4113)\n",
      "[6/25][53/96] Loss_D: -1.4673 (-1.3174) Loss_G: 5.8088 (5.3275) D(x): 0.9879 D(G(z)): 0.0040 / 0.0065 Acc: 100.0000 (99.4122)\n",
      "[6/25][54/96] Loss_D: -1.5009 (-1.3177) Loss_G: 6.9372 (5.3301) D(x): 0.9972 D(G(z)): 0.0392 / 0.0016 Acc: 98.4375 (99.4107)\n",
      "[6/25][55/96] Loss_D: -1.3781 (-1.3178) Loss_G: 6.2328 (5.3315) D(x): 0.9638 D(G(z)): 0.0042 / 0.0039 Acc: 100.0000 (99.4116)\n",
      "[6/25][56/96] Loss_D: -1.4370 (-1.3180) Loss_G: 5.6786 (5.3321) D(x): 0.9876 D(G(z)): 0.0033 / 0.0104 Acc: 100.0000 (99.4125)\n",
      "[6/25][57/96] Loss_D: -1.5410 (-1.3183) Loss_G: 7.2791 (5.3351) D(x): 0.9967 D(G(z)): 0.0024 / 0.0012 Acc: 98.4375 (99.4110)\n",
      "[6/25][58/96] Loss_D: -1.2919 (-1.3183) Loss_G: 7.3386 (5.3383) D(x): 0.9937 D(G(z)): 0.0842 / 0.0022 Acc: 100.0000 (99.4119)\n",
      "[6/25][59/96] Loss_D: -1.4222 (-1.3185) Loss_G: 9.1181 (5.3442) D(x): 0.9323 D(G(z)): 0.0002 / 0.0002 Acc: 100.0000 (99.4128)\n",
      "[6/25][60/96] Loss_D: -1.4753 (-1.3187) Loss_G: 4.2147 (5.3425) D(x): 0.9647 D(G(z)): 0.0032 / 0.0289 Acc: 100.0000 (99.4138)\n",
      "[6/25][61/96] Loss_D: -1.4566 (-1.3189) Loss_G: 5.5589 (5.3428) D(x): 0.9911 D(G(z)): 0.0415 / 0.0069 Acc: 98.4375 (99.4122)\n",
      "[6/25][62/96] Loss_D: -1.4954 (-1.3192) Loss_G: 5.9254 (5.3437) D(x): 0.9746 D(G(z)): 0.0058 / 0.0048 Acc: 100.0000 (99.4131)\n",
      "[6/25][63/96] Loss_D: -1.4281 (-1.3194) Loss_G: 7.5035 (5.3471) D(x): 0.9920 D(G(z)): 0.0011 / 0.0013 Acc: 100.0000 (99.4141)\n",
      "[6/25][64/96] Loss_D: -1.4293 (-1.3195) Loss_G: 5.6598 (5.3476) D(x): 0.9957 D(G(z)): 0.0188 / 0.0079 Acc: 98.4375 (99.4125)\n",
      "[6/25][65/96] Loss_D: -1.4953 (-1.3198) Loss_G: 5.3971 (5.3477) D(x): 0.9716 D(G(z)): 0.0036 / 0.0080 Acc: 100.0000 (99.4135)\n",
      "[6/25][66/96] Loss_D: -1.4402 (-1.3200) Loss_G: 4.9090 (5.3470) D(x): 0.9988 D(G(z)): 0.0112 / 0.0186 Acc: 100.0000 (99.4144)\n",
      "[6/25][67/96] Loss_D: -1.3560 (-1.3201) Loss_G: 4.2092 (5.3452) D(x): 0.9980 D(G(z)): 0.0165 / 0.0310 Acc: 96.8750 (99.4104)\n",
      "[6/25][68/96] Loss_D: -1.4100 (-1.3202) Loss_G: 5.4330 (5.3453) D(x): 0.9935 D(G(z)): 0.0200 / 0.0103 Acc: 100.0000 (99.4113)\n",
      "[6/25][69/96] Loss_D: -1.4865 (-1.3205) Loss_G: 6.1728 (5.3466) D(x): 0.9844 D(G(z)): 0.0266 / 0.0027 Acc: 98.4375 (99.4098)\n",
      "[6/25][70/96] Loss_D: -1.6019 (-1.3209) Loss_G: 5.7055 (5.3472) D(x): 0.9801 D(G(z)): 0.0023 / 0.0059 Acc: 98.4375 (99.4083)\n",
      "[6/25][71/96] Loss_D: -1.4057 (-1.3210) Loss_G: 5.1851 (5.3469) D(x): 0.9771 D(G(z)): 0.0074 / 0.0120 Acc: 96.8750 (99.4044)\n",
      "[6/25][72/96] Loss_D: -1.5107 (-1.3213) Loss_G: 3.1965 (5.3436) D(x): 0.9723 D(G(z)): 0.0052 / 0.0790 Acc: 100.0000 (99.4053)\n",
      "[6/25][73/96] Loss_D: -1.3864 (-1.3214) Loss_G: 7.2139 (5.3465) D(x): 0.9985 D(G(z)): 0.0165 / 0.0015 Acc: 98.4375 (99.4038)\n",
      "[6/25][74/96] Loss_D: -1.4342 (-1.3216) Loss_G: 5.9095 (5.3474) D(x): 0.9951 D(G(z)): 0.0133 / 0.0045 Acc: 100.0000 (99.4048)\n",
      "[6/25][75/96] Loss_D: -1.5328 (-1.3219) Loss_G: 5.5253 (5.3476) D(x): 0.9861 D(G(z)): 0.0148 / 0.0060 Acc: 100.0000 (99.4057)\n",
      "[6/25][76/96] Loss_D: -1.4900 (-1.3222) Loss_G: 5.5665 (5.3480) D(x): 0.9940 D(G(z)): 0.0039 / 0.0086 Acc: 100.0000 (99.4066)\n",
      "[6/25][77/96] Loss_D: -1.5166 (-1.3225) Loss_G: 5.6573 (5.3484) D(x): 0.9944 D(G(z)): 0.0087 / 0.0067 Acc: 100.0000 (99.4075)\n",
      "[6/25][78/96] Loss_D: -1.4914 (-1.3227) Loss_G: 4.2501 (5.3468) D(x): 0.9742 D(G(z)): 0.0120 / 0.0234 Acc: 100.0000 (99.4084)\n",
      "[6/25][79/96] Loss_D: -1.4651 (-1.3229) Loss_G: 4.9826 (5.3462) D(x): 0.9866 D(G(z)): 0.0052 / 0.0146 Acc: 100.0000 (99.4093)\n",
      "[6/25][80/96] Loss_D: -1.5453 (-1.3233) Loss_G: 4.6783 (5.3452) D(x): 0.9902 D(G(z)): 0.0069 / 0.0225 Acc: 98.4375 (99.4078)\n",
      "[6/25][81/96] Loss_D: -1.5055 (-1.3236) Loss_G: 5.3634 (5.3452) D(x): 0.9926 D(G(z)): 0.0174 / 0.0096 Acc: 100.0000 (99.4087)\n",
      "[6/25][82/96] Loss_D: -1.4822 (-1.3238) Loss_G: 6.0412 (5.3463) D(x): 0.9881 D(G(z)): 0.0055 / 0.0059 Acc: 100.0000 (99.4096)\n",
      "[6/25][83/96] Loss_D: -1.5144 (-1.3241) Loss_G: 5.6429 (5.3467) D(x): 0.9973 D(G(z)): 0.0279 / 0.0047 Acc: 100.0000 (99.4105)\n",
      "[6/25][84/96] Loss_D: -1.5121 (-1.3244) Loss_G: 4.6217 (5.3456) D(x): 0.9586 D(G(z)): 0.0020 / 0.0146 Acc: 100.0000 (99.4114)\n",
      "[6/25][85/96] Loss_D: -1.3948 (-1.3245) Loss_G: 5.7411 (5.3462) D(x): 0.9964 D(G(z)): 0.0076 / 0.0060 Acc: 100.0000 (99.4123)\n",
      "[6/25][86/96] Loss_D: -1.4546 (-1.3247) Loss_G: 7.3015 (5.3492) D(x): 0.9980 D(G(z)): 0.0790 / 0.0019 Acc: 100.0000 (99.4132)\n",
      "[6/25][87/96] Loss_D: -1.3413 (-1.3247) Loss_G: 5.1514 (5.3489) D(x): 0.8909 D(G(z)): 0.0008 / 0.0118 Acc: 98.4375 (99.4117)\n",
      "[6/25][88/96] Loss_D: -1.5065 (-1.3250) Loss_G: 6.7064 (5.3509) D(x): 0.9934 D(G(z)): 0.0020 / 0.0020 Acc: 100.0000 (99.4126)\n",
      "[6/25][89/96] Loss_D: -1.4592 (-1.3252) Loss_G: 2.9547 (5.3473) D(x): 0.9986 D(G(z)): 0.0225 / 0.0706 Acc: 100.0000 (99.4135)\n",
      "[6/25][90/96] Loss_D: -1.3676 (-1.3252) Loss_G: 6.5484 (5.3491) D(x): 0.9955 D(G(z)): 0.0029 / 0.0034 Acc: 100.0000 (99.4144)\n",
      "[6/25][91/96] Loss_D: -1.1470 (-1.3250) Loss_G: 12.2270 (5.3594) D(x): 0.9995 D(G(z)): 0.2351 / 0.0000 Acc: 100.0000 (99.4152)\n",
      "[6/25][92/96] Loss_D: 3.1889 (-1.3182) Loss_G: -0.5304 (5.3506) D(x): 0.0413 D(G(z)): 0.0000 / 0.9992 Acc: 98.4375 (99.4138)\n",
      "[6/25][93/96] Loss_D: 6.0453 (-1.3072) Loss_G: 1.4249 (5.3448) D(x): 1.0000 D(G(z)): 0.9966 / 0.2430 Acc: 98.4375 (99.4123)\n",
      "[6/25][94/96] Loss_D: -0.3585 (-1.3058) Loss_G: 4.5555 (5.3436) D(x): 0.9431 D(G(z)): 0.5701 / 0.0228 Acc: 100.0000 (99.4132)\n",
      "[6/25][95/96] Loss_D: 0.6220 (-1.3030) Loss_G: 1.2373 (5.3375) D(x): 0.2863 D(G(z)): 0.0637 / 0.2228 Acc: 100.0000 (99.4141)\n",
      "[7/25][0/96] Loss_D: -0.5287 (-1.3018) Loss_G: -0.1912 (5.3293) D(x): 0.5990 D(G(z)): 0.2863 / 0.7095 Acc: 98.4375 (99.4126)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[7/25][1/96] Loss_D: 0.0090 (-1.2999) Loss_G: 1.9045 (5.3242) D(x): 0.9242 D(G(z)): 0.6930 / 0.1244 Acc: 98.4375 (99.4112)\n",
      "[7/25][2/96] Loss_D: -0.9613 (-1.2994) Loss_G: 3.0077 (5.3207) D(x): 0.7594 D(G(z)): 0.1438 / 0.0566 Acc: 98.4375 (99.4097)\n",
      "[7/25][3/96] Loss_D: -0.7669 (-1.2986) Loss_G: 1.5698 (5.3152) D(x): 0.6255 D(G(z)): 0.0642 / 0.1782 Acc: 100.0000 (99.4106)\n",
      "[7/25][4/96] Loss_D: -1.1571 (-1.2984) Loss_G: 1.5775 (5.3097) D(x): 0.9093 D(G(z)): 0.1556 / 0.1919 Acc: 100.0000 (99.4115)\n",
      "[7/25][5/96] Loss_D: -0.7525 (-1.2976) Loss_G: 2.7347 (5.3059) D(x): 0.9138 D(G(z)): 0.3602 / 0.0801 Acc: 100.0000 (99.4123)\n",
      "[7/25][6/96] Loss_D: -1.2680 (-1.2975) Loss_G: 2.5215 (5.3018) D(x): 0.8744 D(G(z)): 0.1323 / 0.1172 Acc: 100.0000 (99.4132)\n",
      "[7/25][7/96] Loss_D: -1.0599 (-1.2972) Loss_G: 3.0711 (5.2985) D(x): 0.8684 D(G(z)): 0.1392 / 0.0865 Acc: 100.0000 (99.4141)\n",
      "[7/25][8/96] Loss_D: -1.2939 (-1.2972) Loss_G: 3.3333 (5.2956) D(x): 0.9234 D(G(z)): 0.0818 / 0.0737 Acc: 100.0000 (99.4149)\n",
      "[7/25][9/96] Loss_D: -1.1317 (-1.2969) Loss_G: 2.9216 (5.2921) D(x): 0.8747 D(G(z)): 0.1227 / 0.1014 Acc: 100.0000 (99.4158)\n",
      "[7/25][10/96] Loss_D: -1.1693 (-1.2967) Loss_G: 3.7364 (5.2898) D(x): 0.9102 D(G(z)): 0.1649 / 0.0408 Acc: 100.0000 (99.4166)\n",
      "[7/25][11/96] Loss_D: -1.2888 (-1.2967) Loss_G: 4.3977 (5.2885) D(x): 0.9523 D(G(z)): 0.0560 / 0.0418 Acc: 100.0000 (99.4175)\n",
      "[7/25][12/96] Loss_D: -1.3578 (-1.2968) Loss_G: 3.2339 (5.2855) D(x): 0.8702 D(G(z)): 0.0205 / 0.0764 Acc: 100.0000 (99.4183)\n",
      "[7/25][13/96] Loss_D: -1.2982 (-1.2968) Loss_G: 4.2632 (5.2841) D(x): 0.9491 D(G(z)): 0.1172 / 0.0310 Acc: 98.4375 (99.4169)\n",
      "[7/25][14/96] Loss_D: -1.3691 (-1.2969) Loss_G: 2.7407 (5.2803) D(x): 0.9303 D(G(z)): 0.0290 / 0.1304 Acc: 100.0000 (99.4178)\n",
      "[7/25][15/96] Loss_D: -1.1053 (-1.2966) Loss_G: 3.5804 (5.2779) D(x): 0.9132 D(G(z)): 0.1060 / 0.0905 Acc: 100.0000 (99.4186)\n",
      "[7/25][16/96] Loss_D: -1.2464 (-1.2966) Loss_G: 5.2492 (5.2778) D(x): 0.9542 D(G(z)): 0.1200 / 0.0146 Acc: 98.4375 (99.4172)\n",
      "[7/25][17/96] Loss_D: -1.3478 (-1.2966) Loss_G: 5.6976 (5.2784) D(x): 0.9447 D(G(z)): 0.0133 / 0.0113 Acc: 100.0000 (99.4180)\n",
      "[7/25][18/96] Loss_D: -1.2424 (-1.2966) Loss_G: 4.6404 (5.2775) D(x): 0.9146 D(G(z)): 0.0943 / 0.0304 Acc: 100.0000 (99.4189)\n",
      "[7/25][19/96] Loss_D: -1.3313 (-1.2966) Loss_G: 3.5757 (5.2751) D(x): 0.8971 D(G(z)): 0.0343 / 0.0515 Acc: 100.0000 (99.4197)\n",
      "[7/25][20/96] Loss_D: -1.2087 (-1.2965) Loss_G: 6.0441 (5.2762) D(x): 0.9523 D(G(z)): 0.1762 / 0.0061 Acc: 100.0000 (99.4205)\n",
      "[7/25][21/96] Loss_D: -1.3749 (-1.2966) Loss_G: 3.7579 (5.2740) D(x): 0.8854 D(G(z)): 0.0154 / 0.0374 Acc: 100.0000 (99.4214)\n",
      "[7/25][22/96] Loss_D: -1.5083 (-1.2969) Loss_G: 3.9946 (5.2721) D(x): 0.9625 D(G(z)): 0.0358 / 0.0283 Acc: 100.0000 (99.4222)\n",
      "[7/25][23/96] Loss_D: -1.3659 (-1.2970) Loss_G: 4.3880 (5.2709) D(x): 0.9303 D(G(z)): 0.0351 / 0.0261 Acc: 100.0000 (99.4230)\n",
      "[7/25][24/96] Loss_D: -1.0876 (-1.2967) Loss_G: 6.3613 (5.2724) D(x): 0.9714 D(G(z)): 0.1800 / 0.0060 Acc: 98.4375 (99.4216)\n",
      "[7/25][25/96] Loss_D: -1.2588 (-1.2966) Loss_G: 6.7804 (5.2746) D(x): 0.9213 D(G(z)): 0.0091 / 0.0039 Acc: 100.0000 (99.4225)\n",
      "[7/25][26/96] Loss_D: -1.3244 (-1.2967) Loss_G: 4.2594 (5.2731) D(x): 0.9192 D(G(z)): 0.0098 / 0.0258 Acc: 96.8750 (99.4188)\n",
      "[7/25][27/96] Loss_D: -1.3799 (-1.2968) Loss_G: 5.0589 (5.2728) D(x): 0.9800 D(G(z)): 0.0328 / 0.0116 Acc: 100.0000 (99.4196)\n",
      "[7/25][28/96] Loss_D: -1.4214 (-1.2970) Loss_G: 5.2136 (5.2728) D(x): 0.9868 D(G(z)): 0.0646 / 0.0110 Acc: 100.0000 (99.4205)\n",
      "[7/25][29/96] Loss_D: -1.3988 (-1.2971) Loss_G: 4.7696 (5.2720) D(x): 0.9325 D(G(z)): 0.0216 / 0.0182 Acc: 98.4375 (99.4191)\n",
      "[7/25][30/96] Loss_D: -1.4076 (-1.2973) Loss_G: 5.4930 (5.2724) D(x): 0.9942 D(G(z)): 0.0497 / 0.0078 Acc: 98.4375 (99.4177)\n",
      "[7/25][31/96] Loss_D: -1.3581 (-1.2974) Loss_G: 2.2448 (5.2681) D(x): 0.8593 D(G(z)): 0.0209 / 0.1111 Acc: 100.0000 (99.4185)\n",
      "[7/25][32/96] Loss_D: -1.1501 (-1.2972) Loss_G: 5.4353 (5.2683) D(x): 0.9913 D(G(z)): 0.1848 / 0.0115 Acc: 100.0000 (99.4193)\n",
      "[7/25][33/96] Loss_D: -1.4944 (-1.2974) Loss_G: 7.0631 (5.2708) D(x): 0.9872 D(G(z)): 0.0079 / 0.0015 Acc: 98.4375 (99.4179)\n",
      "[7/25][34/96] Loss_D: -1.2756 (-1.2974) Loss_G: 4.3368 (5.2695) D(x): 0.8251 D(G(z)): 0.0026 / 0.0250 Acc: 100.0000 (99.4188)\n",
      "[7/25][35/96] Loss_D: -1.3238 (-1.2974) Loss_G: 4.6327 (5.2686) D(x): 0.9874 D(G(z)): 0.0846 / 0.0193 Acc: 98.4375 (99.4174)\n",
      "[7/25][36/96] Loss_D: -1.4697 (-1.2977) Loss_G: 4.8141 (5.2680) D(x): 0.9879 D(G(z)): 0.0308 / 0.0113 Acc: 100.0000 (99.4182)\n",
      "[7/25][37/96] Loss_D: -1.2584 (-1.2976) Loss_G: 5.0275 (5.2676) D(x): 0.9907 D(G(z)): 0.0523 / 0.0191 Acc: 100.0000 (99.4190)\n",
      "[7/25][38/96] Loss_D: -1.3170 (-1.2977) Loss_G: 4.8363 (5.2670) D(x): 0.9560 D(G(z)): 0.0227 / 0.0152 Acc: 96.8750 (99.4154)\n",
      "[7/25][39/96] Loss_D: -1.3485 (-1.2977) Loss_G: 4.3444 (5.2657) D(x): 0.9441 D(G(z)): 0.0089 / 0.0319 Acc: 100.0000 (99.4163)\n",
      "[7/25][40/96] Loss_D: -1.4994 (-1.2980) Loss_G: 4.1325 (5.2641) D(x): 0.9618 D(G(z)): 0.0487 / 0.0289 Acc: 100.0000 (99.4171)\n",
      "[7/25][41/96] Loss_D: -1.3114 (-1.2980) Loss_G: 8.2307 (5.2683) D(x): 0.9825 D(G(z)): 0.1212 / 0.0016 Acc: 100.0000 (99.4179)\n",
      "[7/25][42/96] Loss_D: -1.2311 (-1.2979) Loss_G: 5.5965 (5.2688) D(x): 0.8448 D(G(z)): 0.0019 / 0.0102 Acc: 100.0000 (99.4187)\n",
      "[7/25][43/96] Loss_D: -1.4492 (-1.2982) Loss_G: 4.1952 (5.2673) D(x): 0.9497 D(G(z)): 0.0250 / 0.0329 Acc: 100.0000 (99.4195)\n",
      "[7/25][44/96] Loss_D: -1.4456 (-1.2984) Loss_G: 4.4220 (5.2661) D(x): 0.9849 D(G(z)): 0.0205 / 0.0213 Acc: 98.4375 (99.4181)\n",
      "[7/25][45/96] Loss_D: -1.4149 (-1.2985) Loss_G: 6.7003 (5.2681) D(x): 0.9962 D(G(z)): 0.0909 / 0.0029 Acc: 100.0000 (99.4190)\n",
      "[7/25][46/96] Loss_D: -1.5071 (-1.2988) Loss_G: 7.0979 (5.2706) D(x): 0.9789 D(G(z)): 0.0019 / 0.0013 Acc: 100.0000 (99.4198)\n",
      "[7/25][47/96] Loss_D: -1.3127 (-1.2988) Loss_G: 2.2050 (5.2664) D(x): 0.8531 D(G(z)): 0.0070 / 0.1424 Acc: 100.0000 (99.4206)\n",
      "[7/25][48/96] Loss_D: -1.1791 (-1.2987) Loss_G: 6.8284 (5.2685) D(x): 0.9896 D(G(z)): 0.1508 / 0.0026 Acc: 98.4375 (99.4192)\n",
      "[7/25][49/96] Loss_D: -1.4308 (-1.2988) Loss_G: 6.1225 (5.2697) D(x): 0.9234 D(G(z)): 0.0013 / 0.0046 Acc: 100.0000 (99.4200)\n",
      "[7/25][50/96] Loss_D: -1.4193 (-1.2990) Loss_G: 4.3137 (5.2684) D(x): 0.9746 D(G(z)): 0.0071 / 0.0190 Acc: 98.4375 (99.4187)\n",
      "[7/25][51/96] Loss_D: -1.4398 (-1.2992) Loss_G: 4.6303 (5.2675) D(x): 0.9792 D(G(z)): 0.0059 / 0.0206 Acc: 100.0000 (99.4195)\n",
      "[7/25][52/96] Loss_D: -1.4359 (-1.2994) Loss_G: 4.4103 (5.2663) D(x): 0.9911 D(G(z)): 0.0227 / 0.0219 Acc: 100.0000 (99.4203)\n",
      "[7/25][53/96] Loss_D: -1.3917 (-1.2995) Loss_G: 5.4132 (5.2665) D(x): 0.9684 D(G(z)): 0.0365 / 0.0124 Acc: 100.0000 (99.4211)\n",
      "[7/25][54/96] Loss_D: -1.3869 (-1.2996) Loss_G: 4.9333 (5.2661) D(x): 0.9911 D(G(z)): 0.0241 / 0.0137 Acc: 100.0000 (99.4219)\n",
      "[7/25][55/96] Loss_D: -1.4350 (-1.2998) Loss_G: 4.4653 (5.2650) D(x): 0.9639 D(G(z)): 0.0051 / 0.0186 Acc: 100.0000 (99.4226)\n",
      "[7/25][56/96] Loss_D: -1.4586 (-1.3000) Loss_G: 5.2689 (5.2650) D(x): 0.9499 D(G(z)): 0.0129 / 0.0083 Acc: 100.0000 (99.4234)\n",
      "[7/25][57/96] Loss_D: -1.4561 (-1.3003) Loss_G: 5.3049 (5.2650) D(x): 0.9941 D(G(z)): 0.0493 / 0.0101 Acc: 100.0000 (99.4242)\n",
      "[7/25][58/96] Loss_D: -1.4733 (-1.3005) Loss_G: 5.1412 (5.2649) D(x): 0.9900 D(G(z)): 0.0302 / 0.0108 Acc: 98.4375 (99.4229)\n",
      "[7/25][59/96] Loss_D: -1.4019 (-1.3006) Loss_G: 3.1880 (5.2620) D(x): 0.9412 D(G(z)): 0.0142 / 0.0539 Acc: 100.0000 (99.4237)\n",
      "[7/25][60/96] Loss_D: -1.4361 (-1.3008) Loss_G: 5.5463 (5.2624) D(x): 0.9797 D(G(z)): 0.0258 / 0.0058 Acc: 98.4375 (99.4223)\n",
      "[7/25][61/96] Loss_D: -1.4976 (-1.3011) Loss_G: 4.6893 (5.2616) D(x): 0.9703 D(G(z)): 0.0162 / 0.0150 Acc: 100.0000 (99.4231)\n",
      "[7/25][62/96] Loss_D: -1.4247 (-1.3013) Loss_G: 4.0892 (5.2600) D(x): 0.9829 D(G(z)): 0.0102 / 0.0308 Acc: 100.0000 (99.4239)\n",
      "[7/25][63/96] Loss_D: -1.4852 (-1.3015) Loss_G: 4.4782 (5.2590) D(x): 0.9860 D(G(z)): 0.0412 / 0.0162 Acc: 98.4375 (99.4226)\n",
      "[7/25][64/96] Loss_D: -1.4695 (-1.3017) Loss_G: 5.9348 (5.2599) D(x): 0.9488 D(G(z)): 0.0043 / 0.0047 Acc: 100.0000 (99.4233)\n",
      "[7/25][65/96] Loss_D: -1.5060 (-1.3020) Loss_G: 3.8358 (5.2580) D(x): 0.9834 D(G(z)): 0.0363 / 0.0424 Acc: 100.0000 (99.4241)\n",
      "[7/25][66/96] Loss_D: -1.3097 (-1.3020) Loss_G: 5.2142 (5.2579) D(x): 0.9917 D(G(z)): 0.0375 / 0.0091 Acc: 100.0000 (99.4249)\n",
      "[7/25][67/96] Loss_D: -1.4957 (-1.3023) Loss_G: 6.2308 (5.2592) D(x): 0.9684 D(G(z)): 0.0021 / 0.0037 Acc: 100.0000 (99.4257)\n",
      "[7/25][68/96] Loss_D: -1.3573 (-1.3024) Loss_G: 2.4833 (5.2555) D(x): 0.9490 D(G(z)): 0.0071 / 0.1364 Acc: 100.0000 (99.4265)\n",
      "[7/25][69/96] Loss_D: -1.3880 (-1.3025) Loss_G: 2.7419 (5.2521) D(x): 0.9773 D(G(z)): 0.0235 / 0.0886 Acc: 100.0000 (99.4272)\n",
      "[7/25][70/96] Loss_D: -1.3836 (-1.3026) Loss_G: 7.1770 (5.2547) D(x): 0.9974 D(G(z)): 0.1197 / 0.0013 Acc: 100.0000 (99.4280)\n",
      "[7/25][71/96] Loss_D: -1.1252 (-1.3023) Loss_G: 1.8574 (5.2501) D(x): 0.7760 D(G(z)): 0.0020 / 0.1999 Acc: 100.0000 (99.4288)\n",
      "[7/25][72/96] Loss_D: -1.2479 (-1.3023) Loss_G: 6.6525 (5.2520) D(x): 0.9981 D(G(z)): 0.1916 / 0.0021 Acc: 100.0000 (99.4295)\n",
      "[7/25][73/96] Loss_D: -1.5193 (-1.3026) Loss_G: 7.5669 (5.2551) D(x): 0.9615 D(G(z)): 0.0015 / 0.0008 Acc: 100.0000 (99.4303)\n",
      "[7/25][74/96] Loss_D: -1.1444 (-1.3024) Loss_G: 1.8565 (5.2505) D(x): 0.8165 D(G(z)): 0.0007 / 0.1838 Acc: 98.4375 (99.4290)\n",
      "[7/25][75/96] Loss_D: -1.3948 (-1.3025) Loss_G: 3.7440 (5.2485) D(x): 0.9962 D(G(z)): 0.0922 / 0.0341 Acc: 100.0000 (99.4297)\n",
      "[7/25][76/96] Loss_D: -1.3585 (-1.3025) Loss_G: 6.2702 (5.2499) D(x): 0.9971 D(G(z)): 0.0960 / 0.0026 Acc: 100.0000 (99.4305)\n",
      "[7/25][77/96] Loss_D: -1.5298 (-1.3029) Loss_G: 8.1735 (5.2538) D(x): 0.9732 D(G(z)): 0.0027 / 0.0004 Acc: 98.4375 (99.4292)\n",
      "[7/25][78/96] Loss_D: -1.4854 (-1.3031) Loss_G: 6.4696 (5.2554) D(x): 0.9461 D(G(z)): 0.0003 / 0.0022 Acc: 100.0000 (99.4299)\n",
      "[7/25][79/96] Loss_D: -1.4319 (-1.3033) Loss_G: 5.1035 (5.2552) D(x): 0.9826 D(G(z)): 0.0030 / 0.0145 Acc: 98.4375 (99.4286)\n",
      "[7/25][80/96] Loss_D: -1.5005 (-1.3035) Loss_G: 3.1535 (5.2524) D(x): 0.9882 D(G(z)): 0.0031 / 0.0537 Acc: 96.8750 (99.4252)\n",
      "[7/25][81/96] Loss_D: -1.4794 (-1.3038) Loss_G: 5.1574 (5.2523) D(x): 0.9960 D(G(z)): 0.0444 / 0.0102 Acc: 100.0000 (99.4260)\n",
      "[7/25][82/96] Loss_D: -1.4410 (-1.3039) Loss_G: 4.7890 (5.2517) D(x): 0.9847 D(G(z)): 0.0114 / 0.0177 Acc: 100.0000 (99.4267)\n",
      "[7/25][83/96] Loss_D: -1.4922 (-1.3042) Loss_G: 4.8697 (5.2512) D(x): 0.9969 D(G(z)): 0.0194 / 0.0116 Acc: 100.0000 (99.4275)\n",
      "[7/25][84/96] Loss_D: -1.4899 (-1.3044) Loss_G: 5.4368 (5.2514) D(x): 0.9798 D(G(z)): 0.0050 / 0.0070 Acc: 100.0000 (99.4283)\n",
      "[7/25][85/96] Loss_D: -1.3510 (-1.3045) Loss_G: 5.1651 (5.2513) D(x): 0.9676 D(G(z)): 0.0046 / 0.0094 Acc: 100.0000 (99.4290)\n",
      "[7/25][86/96] Loss_D: -1.3685 (-1.3046) Loss_G: 4.7612 (5.2507) D(x): 0.9841 D(G(z)): 0.0346 / 0.0128 Acc: 96.8750 (99.4256)\n",
      "[7/25][87/96] Loss_D: -1.6410 (-1.3050) Loss_G: 5.7522 (5.2513) D(x): 0.9924 D(G(z)): 0.0074 / 0.0044 Acc: 100.0000 (99.4264)\n",
      "[7/25][88/96] Loss_D: -1.4510 (-1.3052) Loss_G: 5.5697 (5.2517) D(x): 0.9924 D(G(z)): 0.0249 / 0.0066 Acc: 100.0000 (99.4272)\n",
      "[7/25][89/96] Loss_D: -1.4800 (-1.3054) Loss_G: 6.5139 (5.2534) D(x): 0.9719 D(G(z)): 0.0054 / 0.0020 Acc: 100.0000 (99.4279)\n",
      "[7/25][90/96] Loss_D: -1.3769 (-1.3055) Loss_G: 4.2490 (5.2521) D(x): 0.9602 D(G(z)): 0.0205 / 0.0201 Acc: 98.4375 (99.4266)\n",
      "[7/25][91/96] Loss_D: -1.3777 (-1.3056) Loss_G: 4.0851 (5.2505) D(x): 0.9891 D(G(z)): 0.0171 / 0.0247 Acc: 100.0000 (99.4274)\n",
      "[7/25][92/96] Loss_D: -1.5206 (-1.3059) Loss_G: 3.4483 (5.2482) D(x): 0.9926 D(G(z)): 0.0292 / 0.0383 Acc: 96.8750 (99.4240)\n",
      "[7/25][93/96] Loss_D: -1.4709 (-1.3061) Loss_G: 3.7527 (5.2462) D(x): 0.9822 D(G(z)): 0.0099 / 0.0309 Acc: 100.0000 (99.4248)\n",
      "[7/25][94/96] Loss_D: -1.3766 (-1.3062) Loss_G: 5.6417 (5.2468) D(x): 0.9853 D(G(z)): 0.0140 / 0.0045 Acc: 100.0000 (99.4255)\n",
      "[7/25][95/96] Loss_D: -1.5268 (-1.3065) Loss_G: 5.6072 (5.2472) D(x): 0.9927 D(G(z)): 0.0360 / 0.0059 Acc: 100.0000 (99.4263)\n",
      "[8/25][0/96] Loss_D: -1.4759 (-1.3067) Loss_G: 4.9622 (5.2469) D(x): 0.9575 D(G(z)): 0.0097 / 0.0102 Acc: 100.0000 (99.4270)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[8/25][1/96] Loss_D: -1.4598 (-1.3069) Loss_G: 4.2120 (5.2455) D(x): 0.9796 D(G(z)): 0.0033 / 0.0192 Acc: 100.0000 (99.4278)\n",
      "[8/25][2/96] Loss_D: -1.3987 (-1.3070) Loss_G: 5.6163 (5.2460) D(x): 0.9854 D(G(z)): 0.0505 / 0.0061 Acc: 100.0000 (99.4285)\n",
      "[8/25][3/96] Loss_D: -1.5035 (-1.3073) Loss_G: 5.5085 (5.2463) D(x): 0.9921 D(G(z)): 0.0040 / 0.0073 Acc: 98.4375 (99.4272)\n",
      "[8/25][4/96] Loss_D: -1.1049 (-1.3070) Loss_G: 4.6741 (5.2456) D(x): 0.8842 D(G(z)): 0.0596 / 0.0164 Acc: 100.0000 (99.4280)\n",
      "[8/25][5/96] Loss_D: -1.4795 (-1.3073) Loss_G: 5.2508 (5.2456) D(x): 0.9880 D(G(z)): 0.0620 / 0.0094 Acc: 100.0000 (99.4287)\n",
      "[8/25][6/96] Loss_D: -1.4496 (-1.3074) Loss_G: 4.3360 (5.2444) D(x): 0.9399 D(G(z)): 0.0151 / 0.0135 Acc: 100.0000 (99.4294)\n",
      "[8/25][7/96] Loss_D: -1.5883 (-1.3078) Loss_G: 5.8710 (5.2452) D(x): 0.9894 D(G(z)): 0.0375 / 0.0048 Acc: 98.4375 (99.4282)\n",
      "[8/25][8/96] Loss_D: -1.5428 (-1.3081) Loss_G: 5.6240 (5.2457) D(x): 0.9844 D(G(z)): 0.0037 / 0.0052 Acc: 100.0000 (99.4289)\n",
      "[8/25][9/96] Loss_D: -1.3942 (-1.3082) Loss_G: 5.1541 (5.2456) D(x): 0.9870 D(G(z)): 0.0141 / 0.0103 Acc: 100.0000 (99.4296)\n",
      "[8/25][10/96] Loss_D: -1.4943 (-1.3085) Loss_G: 5.4475 (5.2459) D(x): 0.9834 D(G(z)): 0.0181 / 0.0073 Acc: 100.0000 (99.4304)\n",
      "[8/25][11/96] Loss_D: -1.5600 (-1.3088) Loss_G: 4.0578 (5.2443) D(x): 0.9656 D(G(z)): 0.0109 / 0.0226 Acc: 98.4375 (99.4291)\n",
      "[8/25][12/96] Loss_D: -1.3827 (-1.3089) Loss_G: 7.8843 (5.2477) D(x): 0.9908 D(G(z)): 0.0534 / 0.0005 Acc: 100.0000 (99.4298)\n",
      "[8/25][13/96] Loss_D: -1.5026 (-1.3091) Loss_G: 3.8534 (5.2459) D(x): 0.9483 D(G(z)): 0.0022 / 0.0416 Acc: 100.0000 (99.4305)\n",
      "[8/25][14/96] Loss_D: -1.3904 (-1.3092) Loss_G: 5.3479 (5.2461) D(x): 0.9876 D(G(z)): 0.0033 / 0.0063 Acc: 100.0000 (99.4313)\n",
      "[8/25][15/96] Loss_D: -1.4899 (-1.3095) Loss_G: 5.1732 (5.2460) D(x): 0.9933 D(G(z)): 0.0032 / 0.0096 Acc: 98.4375 (99.4300)\n",
      "[8/25][16/96] Loss_D: -1.5176 (-1.3097) Loss_G: 4.9315 (5.2456) D(x): 0.9979 D(G(z)): 0.0261 / 0.0138 Acc: 100.0000 (99.4307)\n",
      "[8/25][17/96] Loss_D: -1.5243 (-1.3100) Loss_G: 6.4336 (5.2471) D(x): 0.9940 D(G(z)): 0.0256 / 0.0017 Acc: 98.4375 (99.4295)\n",
      "[8/25][18/96] Loss_D: -1.5045 (-1.3102) Loss_G: 5.9337 (5.2480) D(x): 0.9804 D(G(z)): 0.0064 / 0.0038 Acc: 100.0000 (99.4302)\n",
      "[8/25][19/96] Loss_D: -1.3866 (-1.3103) Loss_G: 5.3794 (5.2481) D(x): 0.9735 D(G(z)): 0.0076 / 0.0109 Acc: 98.4375 (99.4289)\n",
      "[8/25][20/96] Loss_D: -1.4257 (-1.3105) Loss_G: 4.5010 (5.2472) D(x): 0.9790 D(G(z)): 0.0054 / 0.0229 Acc: 100.0000 (99.4297)\n",
      "[8/25][21/96] Loss_D: -1.5485 (-1.3108) Loss_G: 3.1632 (5.2445) D(x): 0.9934 D(G(z)): 0.0072 / 0.0464 Acc: 98.4375 (99.4284)\n",
      "[8/25][22/96] Loss_D: -1.5660 (-1.3111) Loss_G: 6.5642 (5.2462) D(x): 0.9905 D(G(z)): 0.0023 / 0.0016 Acc: 100.0000 (99.4291)\n",
      "[8/25][23/96] Loss_D: -1.3282 (-1.3111) Loss_G: 8.0368 (5.2497) D(x): 0.9981 D(G(z)): 0.1146 / 0.0008 Acc: 100.0000 (99.4298)\n",
      "[8/25][24/96] Loss_D: -0.9402 (-1.3107) Loss_G: 0.4441 (5.2437) D(x): 0.6924 D(G(z)): 0.0000 / 0.5399 Acc: 100.0000 (99.4306)\n",
      "[8/25][25/96] Loss_D: 2.7830 (-1.3055) Loss_G: 14.6865 (5.2556) D(x): 1.0000 D(G(z)): 0.9691 / 0.0000 Acc: 100.0000 (99.4313)\n",
      "[8/25][26/96] Loss_D: 3.2929 (-1.2997) Loss_G: 5.1878 (5.2555) D(x): 0.0409 D(G(z)): 0.0000 / 0.0551 Acc: 100.0000 (99.4320)\n",
      "[8/25][27/96] Loss_D: -1.2104 (-1.2996) Loss_G: 1.5731 (5.2508) D(x): 0.9265 D(G(z)): 0.0552 / 0.3908 Acc: 100.0000 (99.4327)\n",
      "[8/25][28/96] Loss_D: -0.7754 (-1.2990) Loss_G: 3.2565 (5.2483) D(x): 0.9927 D(G(z)): 0.2629 / 0.1518 Acc: 100.0000 (99.4334)\n",
      "[8/25][29/96] Loss_D: -0.9946 (-1.2986) Loss_G: 4.8685 (5.2479) D(x): 0.9666 D(G(z)): 0.2560 / 0.0195 Acc: 100.0000 (99.4341)\n",
      "[8/25][30/96] Loss_D: -1.2647 (-1.2985) Loss_G: 6.6885 (5.2497) D(x): 0.8756 D(G(z)): 0.0379 / 0.0029 Acc: 100.0000 (99.4348)\n",
      "[8/25][31/96] Loss_D: -1.3757 (-1.2986) Loss_G: 4.5852 (5.2488) D(x): 0.9246 D(G(z)): 0.0267 / 0.0198 Acc: 100.0000 (99.4355)\n",
      "[8/25][32/96] Loss_D: -1.4988 (-1.2989) Loss_G: 5.5138 (5.2492) D(x): 0.9674 D(G(z)): 0.0204 / 0.0073 Acc: 98.4375 (99.4343)\n",
      "[8/25][33/96] Loss_D: -1.3567 (-1.2990) Loss_G: 3.9720 (5.2476) D(x): 0.9861 D(G(z)): 0.0300 / 0.0282 Acc: 100.0000 (99.4350)\n",
      "[8/25][34/96] Loss_D: -1.4510 (-1.2991) Loss_G: 4.5025 (5.2467) D(x): 0.9677 D(G(z)): 0.0269 / 0.0118 Acc: 98.4375 (99.4338)\n",
      "[8/25][35/96] Loss_D: -1.4681 (-1.2994) Loss_G: 4.1844 (5.2453) D(x): 0.9923 D(G(z)): 0.0233 / 0.0273 Acc: 98.4375 (99.4325)\n",
      "[8/25][36/96] Loss_D: -1.4175 (-1.2995) Loss_G: 4.2692 (5.2441) D(x): 0.9878 D(G(z)): 0.0209 / 0.0320 Acc: 100.0000 (99.4332)\n",
      "[8/25][37/96] Loss_D: -1.4545 (-1.2997) Loss_G: 4.7536 (5.2435) D(x): 0.9907 D(G(z)): 0.0200 / 0.0117 Acc: 100.0000 (99.4339)\n",
      "[8/25][38/96] Loss_D: -1.4755 (-1.2999) Loss_G: 5.1430 (5.2434) D(x): 0.9893 D(G(z)): 0.0520 / 0.0078 Acc: 98.4375 (99.4327)\n",
      "[8/25][39/96] Loss_D: -1.5139 (-1.3002) Loss_G: 4.6586 (5.2427) D(x): 0.9939 D(G(z)): 0.0109 / 0.0169 Acc: 100.0000 (99.4334)\n",
      "[8/25][40/96] Loss_D: -1.4995 (-1.3004) Loss_G: 4.8616 (5.2422) D(x): 0.9794 D(G(z)): 0.0099 / 0.0139 Acc: 98.4375 (99.4322)\n",
      "[8/25][41/96] Loss_D: -1.5187 (-1.3007) Loss_G: 3.2959 (5.2398) D(x): 0.9436 D(G(z)): 0.0235 / 0.0691 Acc: 100.0000 (99.4329)\n",
      "[8/25][42/96] Loss_D: -1.4066 (-1.3008) Loss_G: 6.0972 (5.2408) D(x): 0.9847 D(G(z)): 0.1170 / 0.0025 Acc: 100.0000 (99.4336)\n",
      "[8/25][43/96] Loss_D: -1.4992 (-1.3011) Loss_G: 5.6903 (5.2414) D(x): 0.9516 D(G(z)): 0.0104 / 0.0036 Acc: 98.4375 (99.4323)\n",
      "[8/25][44/96] Loss_D: -1.2825 (-1.3010) Loss_G: 4.8238 (5.2409) D(x): 0.9375 D(G(z)): 0.0076 / 0.0102 Acc: 98.4375 (99.4311)\n",
      "[8/25][45/96] Loss_D: -1.3823 (-1.3011) Loss_G: 4.1055 (5.2395) D(x): 0.9756 D(G(z)): 0.0132 / 0.0186 Acc: 100.0000 (99.4318)\n",
      "[8/25][46/96] Loss_D: -1.3157 (-1.3012) Loss_G: 4.3251 (5.2384) D(x): 0.9702 D(G(z)): 0.1132 / 0.0179 Acc: 100.0000 (99.4325)\n",
      "[8/25][47/96] Loss_D: -1.5290 (-1.3014) Loss_G: 3.9026 (5.2367) D(x): 0.9645 D(G(z)): 0.0104 / 0.0203 Acc: 98.4375 (99.4313)\n",
      "[8/25][48/96] Loss_D: -1.2460 (-1.3014) Loss_G: 3.4461 (5.2345) D(x): 0.9161 D(G(z)): 0.0716 / 0.0497 Acc: 100.0000 (99.4320)\n",
      "[8/25][49/96] Loss_D: -1.3518 (-1.3014) Loss_G: 5.0353 (5.2343) D(x): 0.9415 D(G(z)): 0.0621 / 0.0085 Acc: 100.0000 (99.4327)\n",
      "[8/25][50/96] Loss_D: -1.3097 (-1.3014) Loss_G: 3.8878 (5.2327) D(x): 0.9141 D(G(z)): 0.0265 / 0.0287 Acc: 100.0000 (99.4334)\n",
      "[8/25][51/96] Loss_D: -1.2288 (-1.3014) Loss_G: 6.2870 (5.2339) D(x): 0.9486 D(G(z)): 0.1742 / 0.0030 Acc: 100.0000 (99.4341)\n",
      "[8/25][52/96] Loss_D: -1.1705 (-1.3012) Loss_G: 3.7882 (5.2322) D(x): 0.8136 D(G(z)): 0.0183 / 0.0334 Acc: 98.4375 (99.4329)\n",
      "[8/25][53/96] Loss_D: -1.4771 (-1.3014) Loss_G: 2.3734 (5.2287) D(x): 0.9503 D(G(z)): 0.0579 / 0.1221 Acc: 100.0000 (99.4335)\n",
      "[8/25][54/96] Loss_D: -1.4112 (-1.3015) Loss_G: 4.0364 (5.2272) D(x): 0.9524 D(G(z)): 0.0750 / 0.0290 Acc: 100.0000 (99.4342)\n",
      "[8/25][55/96] Loss_D: -1.3024 (-1.3015) Loss_G: 5.1237 (5.2271) D(x): 0.9918 D(G(z)): 0.0588 / 0.0081 Acc: 100.0000 (99.4349)\n",
      "[8/25][56/96] Loss_D: -1.2619 (-1.3015) Loss_G: 4.7460 (5.2265) D(x): 0.9148 D(G(z)): 0.0680 / 0.0140 Acc: 98.4375 (99.4337)\n",
      "[8/25][57/96] Loss_D: -1.4664 (-1.3017) Loss_G: 5.8446 (5.2273) D(x): 0.9293 D(G(z)): 0.0026 / 0.0039 Acc: 98.4375 (99.4325)\n",
      "[8/25][58/96] Loss_D: -1.4764 (-1.3019) Loss_G: 2.9744 (5.2246) D(x): 0.9654 D(G(z)): 0.0136 / 0.0816 Acc: 100.0000 (99.4332)\n",
      "[8/25][59/96] Loss_D: -1.3450 (-1.3020) Loss_G: 3.3089 (5.2223) D(x): 0.9900 D(G(z)): 0.0338 / 0.0469 Acc: 100.0000 (99.4339)\n",
      "[8/25][60/96] Loss_D: -1.3322 (-1.3020) Loss_G: 4.9775 (5.2220) D(x): 0.9914 D(G(z)): 0.0504 / 0.0091 Acc: 100.0000 (99.4346)\n",
      "[8/25][61/96] Loss_D: -1.5309 (-1.3023) Loss_G: 4.7432 (5.2214) D(x): 0.9754 D(G(z)): 0.0058 / 0.0176 Acc: 100.0000 (99.4352)\n",
      "[8/25][62/96] Loss_D: -1.4619 (-1.3025) Loss_G: 6.3349 (5.2227) D(x): 0.9843 D(G(z)): 0.0463 / 0.0025 Acc: 100.0000 (99.4359)\n",
      "[8/25][63/96] Loss_D: -1.3759 (-1.3025) Loss_G: 4.9494 (5.2224) D(x): 0.9348 D(G(z)): 0.0061 / 0.0134 Acc: 100.0000 (99.4366)\n",
      "[8/25][64/96] Loss_D: -1.4048 (-1.3027) Loss_G: 4.7257 (5.2218) D(x): 0.9878 D(G(z)): 0.0044 / 0.0182 Acc: 100.0000 (99.4373)\n",
      "[8/25][65/96] Loss_D: -1.5570 (-1.3030) Loss_G: 3.5971 (5.2198) D(x): 0.9921 D(G(z)): 0.0128 / 0.0325 Acc: 100.0000 (99.4379)\n",
      "[8/25][66/96] Loss_D: -1.3295 (-1.3030) Loss_G: 4.9285 (5.2195) D(x): 0.9967 D(G(z)): 0.0395 / 0.0122 Acc: 98.4375 (99.4368)\n",
      "[8/25][67/96] Loss_D: -1.4362 (-1.3032) Loss_G: 5.3917 (5.2197) D(x): 0.9699 D(G(z)): 0.0158 / 0.0113 Acc: 98.4375 (99.4356)\n",
      "[8/25][68/96] Loss_D: -1.4065 (-1.3033) Loss_G: 4.7686 (5.2192) D(x): 0.9751 D(G(z)): 0.0193 / 0.0121 Acc: 100.0000 (99.4362)\n",
      "[8/25][69/96] Loss_D: -1.4277 (-1.3034) Loss_G: 3.6355 (5.2173) D(x): 0.9898 D(G(z)): 0.0146 / 0.0337 Acc: 100.0000 (99.4369)\n",
      "[8/25][70/96] Loss_D: -1.4591 (-1.3036) Loss_G: 5.5666 (5.2177) D(x): 0.9868 D(G(z)): 0.0115 / 0.0053 Acc: 100.0000 (99.4376)\n",
      "[8/25][71/96] Loss_D: -1.4070 (-1.3037) Loss_G: 3.7251 (5.2159) D(x): 0.9849 D(G(z)): 0.0413 / 0.0301 Acc: 98.4375 (99.4364)\n",
      "[8/25][72/96] Loss_D: -1.4301 (-1.3039) Loss_G: 4.9963 (5.2157) D(x): 0.9806 D(G(z)): 0.0029 / 0.0096 Acc: 100.0000 (99.4371)\n",
      "[8/25][73/96] Loss_D: -1.4152 (-1.3040) Loss_G: 4.9889 (5.2154) D(x): 0.9845 D(G(z)): 0.0062 / 0.0100 Acc: 98.4375 (99.4359)\n",
      "[8/25][74/96] Loss_D: -1.4936 (-1.3043) Loss_G: 3.9866 (5.2139) D(x): 0.9847 D(G(z)): 0.0061 / 0.0223 Acc: 98.4375 (99.4347)\n",
      "[8/25][75/96] Loss_D: -1.4300 (-1.3044) Loss_G: 2.7783 (5.2110) D(x): 0.9683 D(G(z)): 0.0055 / 0.0865 Acc: 100.0000 (99.4354)\n",
      "[8/25][76/96] Loss_D: -1.4799 (-1.3046) Loss_G: 5.4054 (5.2113) D(x): 0.9951 D(G(z)): 0.0827 / 0.0055 Acc: 100.0000 (99.4360)\n",
      "[8/25][77/96] Loss_D: -1.4548 (-1.3048) Loss_G: 5.0226 (5.2110) D(x): 0.9701 D(G(z)): 0.0134 / 0.0119 Acc: 100.0000 (99.4367)\n",
      "[8/25][78/96] Loss_D: -1.3744 (-1.3049) Loss_G: 4.5556 (5.2103) D(x): 0.9602 D(G(z)): 0.0160 / 0.0161 Acc: 100.0000 (99.4374)\n",
      "[8/25][79/96] Loss_D: -1.4112 (-1.3050) Loss_G: 3.7581 (5.2086) D(x): 0.9739 D(G(z)): 0.0147 / 0.0392 Acc: 100.0000 (99.4380)\n",
      "[8/25][80/96] Loss_D: -1.3284 (-1.3050) Loss_G: 3.8759 (5.2070) D(x): 0.9688 D(G(z)): 0.0132 / 0.0295 Acc: 98.4375 (99.4368)\n",
      "[8/25][81/96] Loss_D: -1.4480 (-1.3052) Loss_G: 3.6884 (5.2052) D(x): 0.9890 D(G(z)): 0.0385 / 0.0330 Acc: 100.0000 (99.4375)\n",
      "[8/25][82/96] Loss_D: -1.4654 (-1.3054) Loss_G: 5.1944 (5.2052) D(x): 0.9938 D(G(z)): 0.0411 / 0.0081 Acc: 100.0000 (99.4382)\n",
      "[8/25][83/96] Loss_D: -1.3816 (-1.3055) Loss_G: 2.7613 (5.2023) D(x): 0.9529 D(G(z)): 0.0042 / 0.1044 Acc: 100.0000 (99.4388)\n",
      "[8/25][84/96] Loss_D: -1.4495 (-1.3056) Loss_G: 5.8367 (5.2031) D(x): 0.9771 D(G(z)): 0.0632 / 0.0031 Acc: 98.4375 (99.4376)\n",
      "[8/25][85/96] Loss_D: -1.4601 (-1.3058) Loss_G: 4.4609 (5.2022) D(x): 0.9721 D(G(z)): 0.0089 / 0.0165 Acc: 98.4375 (99.4365)\n",
      "[8/25][86/96] Loss_D: -1.3446 (-1.3059) Loss_G: 5.1859 (5.2022) D(x): 0.9615 D(G(z)): 0.0180 / 0.0094 Acc: 100.0000 (99.4371)\n",
      "[8/25][87/96] Loss_D: -1.4406 (-1.3060) Loss_G: 3.6168 (5.2003) D(x): 0.9825 D(G(z)): 0.0094 / 0.0332 Acc: 100.0000 (99.4378)\n",
      "[8/25][88/96] Loss_D: -1.5187 (-1.3063) Loss_G: 4.2078 (5.1992) D(x): 0.9954 D(G(z)): 0.0225 / 0.0215 Acc: 100.0000 (99.4384)\n",
      "[8/25][89/96] Loss_D: -1.3927 (-1.3064) Loss_G: 4.8660 (5.1988) D(x): 0.9942 D(G(z)): 0.0226 / 0.0156 Acc: 100.0000 (99.4391)\n",
      "[8/25][90/96] Loss_D: -1.4426 (-1.3065) Loss_G: 5.6780 (5.1993) D(x): 0.9737 D(G(z)): 0.0136 / 0.0050 Acc: 100.0000 (99.4398)\n",
      "[8/25][91/96] Loss_D: -1.4532 (-1.3067) Loss_G: 4.1436 (5.1981) D(x): 0.9919 D(G(z)): 0.0073 / 0.0209 Acc: 98.4375 (99.4386)\n",
      "[8/25][92/96] Loss_D: -1.4102 (-1.3068) Loss_G: 5.3646 (5.1983) D(x): 0.9932 D(G(z)): 0.0198 / 0.0066 Acc: 98.4375 (99.4374)\n",
      "[8/25][93/96] Loss_D: -1.4932 (-1.3070) Loss_G: 5.6139 (5.1988) D(x): 0.9742 D(G(z)): 0.0113 / 0.0076 Acc: 100.0000 (99.4381)\n",
      "[8/25][94/96] Loss_D: -1.3078 (-1.3070) Loss_G: 3.6435 (5.1970) D(x): 0.9586 D(G(z)): 0.0041 / 0.0361 Acc: 98.4375 (99.4369)\n",
      "[8/25][95/96] Loss_D: -1.3610 (-1.3071) Loss_G: 4.2822 (5.1959) D(x): 0.9919 D(G(z)): 0.0657 / 0.0189 Acc: 98.4375 (99.4358)\n",
      "[9/25][0/96] Loss_D: -1.3280 (-1.3071) Loss_G: 5.4774 (5.1963) D(x): 0.9741 D(G(z)): 0.0033 / 0.0052 Acc: 100.0000 (99.4364)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[9/25][1/96] Loss_D: -1.5468 (-1.3074) Loss_G: 5.5513 (5.1967) D(x): 0.9956 D(G(z)): 0.0258 / 0.0040 Acc: 100.0000 (99.4371)\n",
      "[9/25][2/96] Loss_D: -1.4097 (-1.3075) Loss_G: 4.6496 (5.1960) D(x): 0.9646 D(G(z)): 0.0061 / 0.0169 Acc: 98.4375 (99.4359)\n",
      "[9/25][3/96] Loss_D: -1.3868 (-1.3076) Loss_G: 5.6717 (5.1966) D(x): 0.9771 D(G(z)): 0.0358 / 0.0056 Acc: 100.0000 (99.4366)\n",
      "[9/25][4/96] Loss_D: -1.3337 (-1.3076) Loss_G: 4.3668 (5.1956) D(x): 0.9593 D(G(z)): 0.0141 / 0.0211 Acc: 100.0000 (99.4372)\n",
      "[9/25][5/96] Loss_D: -1.4594 (-1.3078) Loss_G: 5.9082 (5.1964) D(x): 0.9934 D(G(z)): 0.0175 / 0.0036 Acc: 100.0000 (99.4379)\n",
      "[9/25][6/96] Loss_D: -1.3869 (-1.3079) Loss_G: 4.7962 (5.1960) D(x): 0.9932 D(G(z)): 0.0121 / 0.0129 Acc: 100.0000 (99.4385)\n",
      "[9/25][7/96] Loss_D: -1.4534 (-1.3081) Loss_G: 3.4212 (5.1939) D(x): 0.9958 D(G(z)): 0.0109 / 0.0465 Acc: 100.0000 (99.4391)\n",
      "[9/25][8/96] Loss_D: -1.4808 (-1.3083) Loss_G: 5.3979 (5.1942) D(x): 0.9918 D(G(z)): 0.0251 / 0.0071 Acc: 100.0000 (99.4398)\n",
      "[9/25][9/96] Loss_D: -1.3527 (-1.3083) Loss_G: 6.0202 (5.1951) D(x): 0.9782 D(G(z)): 0.0295 / 0.0032 Acc: 98.4375 (99.4386)\n",
      "[9/25][10/96] Loss_D: -1.4952 (-1.3085) Loss_G: 4.9101 (5.1948) D(x): 0.9701 D(G(z)): 0.0016 / 0.0108 Acc: 100.0000 (99.4393)\n",
      "[9/25][11/96] Loss_D: -1.4947 (-1.3087) Loss_G: 4.5486 (5.1941) D(x): 0.9908 D(G(z)): 0.0114 / 0.0131 Acc: 98.4375 (99.4381)\n",
      "[9/25][12/96] Loss_D: -1.4940 (-1.3090) Loss_G: 5.8527 (5.1948) D(x): 0.9973 D(G(z)): 0.0033 / 0.0041 Acc: 100.0000 (99.4388)\n",
      "[9/25][13/96] Loss_D: -1.3941 (-1.3091) Loss_G: 4.9847 (5.1946) D(x): 0.9941 D(G(z)): 0.0062 / 0.0092 Acc: 96.8750 (99.4359)\n",
      "[9/25][14/96] Loss_D: -1.4363 (-1.3092) Loss_G: 4.4719 (5.1938) D(x): 0.9780 D(G(z)): 0.0085 / 0.0189 Acc: 100.0000 (99.4365)\n",
      "[9/25][15/96] Loss_D: -1.5108 (-1.3094) Loss_G: 5.3831 (5.1940) D(x): 0.9951 D(G(z)): 0.0621 / 0.0058 Acc: 100.0000 (99.4371)\n",
      "[9/25][16/96] Loss_D: -1.5317 (-1.3097) Loss_G: 8.6210 (5.1979) D(x): 0.9872 D(G(z)): 0.0019 / 0.0002 Acc: 100.0000 (99.4378)\n",
      "[9/25][17/96] Loss_D: -1.2083 (-1.3096) Loss_G: 3.7801 (5.1962) D(x): 0.9019 D(G(z)): 0.0015 / 0.0461 Acc: 98.4375 (99.4366)\n",
      "[9/25][18/96] Loss_D: -1.3916 (-1.3097) Loss_G: 3.6162 (5.1945) D(x): 0.9931 D(G(z)): 0.0646 / 0.0342 Acc: 100.0000 (99.4373)\n",
      "[9/25][19/96] Loss_D: -1.4485 (-1.3098) Loss_G: 4.7382 (5.1939) D(x): 0.9919 D(G(z)): 0.0268 / 0.0147 Acc: 100.0000 (99.4379)\n",
      "[9/25][20/96] Loss_D: -1.3892 (-1.3099) Loss_G: 4.3243 (5.1930) D(x): 0.9858 D(G(z)): 0.0025 / 0.0238 Acc: 100.0000 (99.4386)\n",
      "[9/25][21/96] Loss_D: -1.4094 (-1.3100) Loss_G: 3.5385 (5.1911) D(x): 0.9640 D(G(z)): 0.0047 / 0.0343 Acc: 100.0000 (99.4392)\n",
      "[9/25][22/96] Loss_D: -1.4527 (-1.3102) Loss_G: 4.3197 (5.1901) D(x): 0.9978 D(G(z)): 0.0136 / 0.0184 Acc: 96.8750 (99.4363)\n",
      "[9/25][23/96] Loss_D: -1.4689 (-1.3104) Loss_G: 5.1742 (5.1901) D(x): 0.9964 D(G(z)): 0.0116 / 0.0090 Acc: 98.4375 (99.4352)\n",
      "[9/25][24/96] Loss_D: -1.5015 (-1.3106) Loss_G: 6.1455 (5.1912) D(x): 0.9883 D(G(z)): 0.0022 / 0.0026 Acc: 100.0000 (99.4358)\n",
      "[9/25][25/96] Loss_D: -1.3657 (-1.3106) Loss_G: 8.8122 (5.1952) D(x): 0.9928 D(G(z)): 0.1212 / 0.0002 Acc: 100.0000 (99.4364)\n",
      "[9/25][26/96] Loss_D: -1.2851 (-1.3106) Loss_G: 10.0027 (5.2006) D(x): 0.9209 D(G(z)): 0.0001 / 0.0001 Acc: 100.0000 (99.4371)\n",
      "[9/25][27/96] Loss_D: -0.8228 (-1.3101) Loss_G: -0.3711 (5.1944) D(x): 0.6476 D(G(z)): 0.0002 / 0.9961 Acc: 100.0000 (99.4377)\n",
      "[9/25][28/96] Loss_D: 2.6133 (-1.3057) Loss_G: 14.4280 (5.2047) D(x): 1.0000 D(G(z)): 0.9636 / 0.0000 Acc: 100.0000 (99.4383)\n",
      "[9/25][29/96] Loss_D: 3.2345 (-1.3006) Loss_G: 5.0549 (5.2046) D(x): 0.0232 D(G(z)): 0.0000 / 0.0083 Acc: 98.4375 (99.4372)\n",
      "[9/25][30/96] Loss_D: -1.4946 (-1.3008) Loss_G: 0.2193 (5.1990) D(x): 0.9953 D(G(z)): 0.0294 / 0.5914 Acc: 98.4375 (99.4361)\n",
      "[9/25][31/96] Loss_D: 0.3569 (-1.2990) Loss_G: 9.0227 (5.2033) D(x): 0.9999 D(G(z)): 0.6935 / 0.0003 Acc: 100.0000 (99.4367)\n",
      "[9/25][32/96] Loss_D: -1.2710 (-1.2989) Loss_G: 9.0325 (5.2075) D(x): 0.8354 D(G(z)): 0.0004 / 0.0002 Acc: 100.0000 (99.4374)\n",
      "[9/25][33/96] Loss_D: -0.4939 (-1.2980) Loss_G: 4.5354 (5.2068) D(x): 0.4937 D(G(z)): 0.0000 / 0.0161 Acc: 100.0000 (99.4380)\n",
      "[9/25][34/96] Loss_D: -1.4631 (-1.2982) Loss_G: 0.5589 (5.2016) D(x): 0.9962 D(G(z)): 0.0445 / 0.4175 Acc: 100.0000 (99.4386)\n",
      "[9/25][35/96] Loss_D: 0.4672 (-1.2962) Loss_G: 8.9121 (5.2057) D(x): 0.9998 D(G(z)): 0.7564 / 0.0003 Acc: 100.0000 (99.4392)\n",
      "[9/25][36/96] Loss_D: -1.5928 (-1.2966) Loss_G: 10.6133 (5.2117) D(x): 0.9569 D(G(z)): 0.0003 / 0.0000 Acc: 100.0000 (99.4399)\n",
      "[9/25][37/96] Loss_D: -0.3458 (-1.2955) Loss_G: 3.7727 (5.2101) D(x): 0.4118 D(G(z)): 0.0001 / 0.0393 Acc: 98.4375 (99.4387)\n",
      "[9/25][38/96] Loss_D: -1.5521 (-1.2958) Loss_G: 2.3546 (5.2070) D(x): 0.9969 D(G(z)): 0.0072 / 0.1291 Acc: 100.0000 (99.4394)\n",
      "[9/25][39/96] Loss_D: -1.1071 (-1.2956) Loss_G: 2.0161 (5.2034) D(x): 0.9993 D(G(z)): 0.1800 / 0.1644 Acc: 98.4375 (99.4383)\n",
      "[9/25][40/96] Loss_D: -0.9984 (-1.2953) Loss_G: 7.4162 (5.2059) D(x): 0.9990 D(G(z)): 0.3735 / 0.0009 Acc: 98.4375 (99.4372)\n",
      "[9/25][41/96] Loss_D: -1.3415 (-1.2953) Loss_G: 9.3468 (5.2105) D(x): 0.9028 D(G(z)): 0.0003 / 0.0001 Acc: 100.0000 (99.4378)\n",
      "[9/25][42/96] Loss_D: -0.9648 (-1.2950) Loss_G: 5.1154 (5.2104) D(x): 0.6294 D(G(z)): 0.0001 / 0.0071 Acc: 100.0000 (99.4384)\n",
      "[9/25][43/96] Loss_D: -1.4221 (-1.2951) Loss_G: 2.1779 (5.2070) D(x): 0.9933 D(G(z)): 0.0234 / 0.1245 Acc: 100.0000 (99.4390)\n",
      "[9/25][44/96] Loss_D: -1.3012 (-1.2951) Loss_G: 3.7721 (5.2054) D(x): 0.9981 D(G(z)): 0.1485 / 0.0296 Acc: 100.0000 (99.4396)\n",
      "[9/25][45/96] Loss_D: -1.2265 (-1.2950) Loss_G: 6.3778 (5.2067) D(x): 0.9985 D(G(z)): 0.1597 / 0.0025 Acc: 100.0000 (99.4402)\n",
      "[9/25][46/96] Loss_D: -1.4451 (-1.2952) Loss_G: 7.2797 (5.2090) D(x): 0.9869 D(G(z)): 0.0101 / 0.0011 Acc: 100.0000 (99.4409)\n",
      "[9/25][47/96] Loss_D: -1.2399 (-1.2951) Loss_G: 3.3447 (5.2070) D(x): 0.8401 D(G(z)): 0.0047 / 0.0464 Acc: 98.4375 (99.4398)\n",
      "[9/25][48/96] Loss_D: -1.3867 (-1.2952) Loss_G: 3.2045 (5.2048) D(x): 0.9771 D(G(z)): 0.0260 / 0.0546 Acc: 100.0000 (99.4404)\n",
      "[9/25][49/96] Loss_D: -1.4766 (-1.2954) Loss_G: 3.1145 (5.2025) D(x): 0.9724 D(G(z)): 0.0519 / 0.0555 Acc: 100.0000 (99.4410)\n",
      "[9/25][50/96] Loss_D: -1.4485 (-1.2956) Loss_G: 4.7588 (5.2020) D(x): 0.9738 D(G(z)): 0.0370 / 0.0198 Acc: 100.0000 (99.4416)\n",
      "[9/25][51/96] Loss_D: -1.4079 (-1.2957) Loss_G: 3.1608 (5.1998) D(x): 0.9781 D(G(z)): 0.0355 / 0.0479 Acc: 96.8750 (99.4388)\n",
      "[9/25][52/96] Loss_D: -1.2829 (-1.2957) Loss_G: 5.3956 (5.2000) D(x): 0.9542 D(G(z)): 0.0677 / 0.0072 Acc: 96.8750 (99.4360)\n",
      "[9/25][53/96] Loss_D: -1.4565 (-1.2959) Loss_G: 4.5273 (5.1992) D(x): 0.9712 D(G(z)): 0.0580 / 0.0175 Acc: 100.0000 (99.4366)\n",
      "[9/25][54/96] Loss_D: -1.3307 (-1.2959) Loss_G: 4.4826 (5.1985) D(x): 0.9233 D(G(z)): 0.0345 / 0.0137 Acc: 100.0000 (99.4372)\n",
      "[9/25][55/96] Loss_D: -1.2326 (-1.2959) Loss_G: 4.1703 (5.1973) D(x): 0.8461 D(G(z)): 0.1654 / 0.0228 Acc: 100.0000 (99.4378)\n",
      "[9/25][56/96] Loss_D: -1.2258 (-1.2958) Loss_G: 4.3294 (5.1964) D(x): 0.8958 D(G(z)): 0.0687 / 0.0261 Acc: 98.4375 (99.4368)\n",
      "[9/25][57/96] Loss_D: -1.3685 (-1.2959) Loss_G: 4.1906 (5.1953) D(x): 0.9470 D(G(z)): 0.0842 / 0.0185 Acc: 100.0000 (99.4374)\n",
      "[9/25][58/96] Loss_D: -1.0731 (-1.2956) Loss_G: 3.8603 (5.1939) D(x): 0.8951 D(G(z)): 0.0998 / 0.0332 Acc: 98.4375 (99.4363)\n",
      "[9/25][59/96] Loss_D: -1.4147 (-1.2957) Loss_G: 3.1556 (5.1917) D(x): 0.9425 D(G(z)): 0.0191 / 0.0463 Acc: 100.0000 (99.4369)\n",
      "[9/25][60/96] Loss_D: -1.4945 (-1.2960) Loss_G: 5.0166 (5.1915) D(x): 0.9907 D(G(z)): 0.0516 / 0.0103 Acc: 100.0000 (99.4375)\n",
      "[9/25][61/96] Loss_D: -1.4813 (-1.2962) Loss_G: 5.9209 (5.1923) D(x): 0.9837 D(G(z)): 0.0019 / 0.0035 Acc: 98.4375 (99.4364)\n",
      "[9/25][62/96] Loss_D: -1.5428 (-1.2964) Loss_G: 4.8034 (5.1918) D(x): 0.9751 D(G(z)): 0.0199 / 0.0100 Acc: 100.0000 (99.4370)\n",
      "[9/25][63/96] Loss_D: -1.5735 (-1.2967) Loss_G: 4.9544 (5.1916) D(x): 0.9666 D(G(z)): 0.0705 / 0.0094 Acc: 100.0000 (99.4376)\n",
      "[9/25][64/96] Loss_D: -1.6136 (-1.2971) Loss_G: 6.3032 (5.1928) D(x): 0.9711 D(G(z)): 0.0120 / 0.0024 Acc: 100.0000 (99.4382)\n",
      "[9/25][65/96] Loss_D: -1.4845 (-1.2973) Loss_G: 5.2137 (5.1928) D(x): 0.9596 D(G(z)): 0.0443 / 0.0078 Acc: 100.0000 (99.4388)\n",
      "[9/25][66/96] Loss_D: -1.4233 (-1.2974) Loss_G: 5.8289 (5.1935) D(x): 0.9749 D(G(z)): 0.0161 / 0.0042 Acc: 100.0000 (99.4394)\n",
      "[9/25][67/96] Loss_D: -1.3766 (-1.2975) Loss_G: 4.9895 (5.1933) D(x): 0.9519 D(G(z)): 0.0067 / 0.0119 Acc: 100.0000 (99.4400)\n",
      "[9/25][68/96] Loss_D: -1.3966 (-1.2976) Loss_G: 3.9193 (5.1919) D(x): 0.9536 D(G(z)): 0.0353 / 0.0342 Acc: 100.0000 (99.4406)\n",
      "[9/25][69/96] Loss_D: -1.3441 (-1.2976) Loss_G: 3.9456 (5.1906) D(x): 0.9396 D(G(z)): 0.0391 / 0.0343 Acc: 100.0000 (99.4412)\n",
      "[9/25][70/96] Loss_D: -1.4375 (-1.2978) Loss_G: 5.2355 (5.1906) D(x): 0.9926 D(G(z)): 0.1035 / 0.0069 Acc: 100.0000 (99.4418)\n",
      "[9/25][71/96] Loss_D: -1.3936 (-1.2979) Loss_G: 6.1238 (5.1916) D(x): 0.9488 D(G(z)): 0.0034 / 0.0034 Acc: 100.0000 (99.4424)\n",
      "[9/25][72/96] Loss_D: -1.4268 (-1.2980) Loss_G: 6.6125 (5.1931) D(x): 0.9507 D(G(z)): 0.0029 / 0.0023 Acc: 100.0000 (99.4430)\n",
      "[9/25][73/96] Loss_D: -1.5057 (-1.2983) Loss_G: 4.5403 (5.1924) D(x): 0.9845 D(G(z)): 0.0088 / 0.0152 Acc: 100.0000 (99.4436)\n",
      "[9/25][74/96] Loss_D: -1.4070 (-1.2984) Loss_G: 3.4297 (5.1906) D(x): 0.9876 D(G(z)): 0.0170 / 0.0364 Acc: 98.4375 (99.4426)\n",
      "[9/25][75/96] Loss_D: -1.3595 (-1.2984) Loss_G: 4.9093 (5.1903) D(x): 0.9933 D(G(z)): 0.0497 / 0.0186 Acc: 100.0000 (99.4432)\n",
      "[9/25][76/96] Loss_D: -1.4236 (-1.2986) Loss_G: 5.2883 (5.1904) D(x): 0.9887 D(G(z)): 0.0594 / 0.0074 Acc: 100.0000 (99.4437)\n",
      "[9/25][77/96] Loss_D: -1.5042 (-1.2988) Loss_G: 6.6977 (5.1920) D(x): 0.9645 D(G(z)): 0.0045 / 0.0018 Acc: 100.0000 (99.4443)\n",
      "[9/25][78/96] Loss_D: -1.4741 (-1.2990) Loss_G: 4.9859 (5.1917) D(x): 0.9241 D(G(z)): 0.0035 / 0.0088 Acc: 98.4375 (99.4433)\n",
      "[9/25][79/96] Loss_D: -1.5296 (-1.2992) Loss_G: 5.1043 (5.1916) D(x): 0.9823 D(G(z)): 0.0573 / 0.0117 Acc: 100.0000 (99.4439)\n",
      "[9/25][80/96] Loss_D: -1.3525 (-1.2993) Loss_G: 4.1810 (5.1906) D(x): 0.9205 D(G(z)): 0.0037 / 0.0156 Acc: 100.0000 (99.4444)\n",
      "[9/25][81/96] Loss_D: -1.4105 (-1.2994) Loss_G: 4.3249 (5.1897) D(x): 0.9988 D(G(z)): 0.0662 / 0.0142 Acc: 98.4375 (99.4434)\n",
      "[9/25][82/96] Loss_D: -1.5246 (-1.2996) Loss_G: 5.6329 (5.1901) D(x): 0.9838 D(G(z)): 0.0044 / 0.0047 Acc: 100.0000 (99.4440)\n",
      "[9/25][83/96] Loss_D: -1.4258 (-1.2998) Loss_G: 5.3974 (5.1904) D(x): 0.9870 D(G(z)): 0.0283 / 0.0068 Acc: 98.4375 (99.4429)\n",
      "[9/25][84/96] Loss_D: -1.4641 (-1.2999) Loss_G: 4.7480 (5.1899) D(x): 0.9928 D(G(z)): 0.0126 / 0.0146 Acc: 98.4375 (99.4418)\n",
      "[9/25][85/96] Loss_D: -1.4126 (-1.3001) Loss_G: 6.3335 (5.1911) D(x): 0.9934 D(G(z)): 0.0026 / 0.0033 Acc: 100.0000 (99.4424)\n",
      "[9/25][86/96] Loss_D: -1.3628 (-1.3001) Loss_G: 4.9995 (5.1909) D(x): 0.9798 D(G(z)): 0.0069 / 0.0094 Acc: 100.0000 (99.4430)\n",
      "[9/25][87/96] Loss_D: -1.4762 (-1.3003) Loss_G: 5.7114 (5.1914) D(x): 0.9899 D(G(z)): 0.0130 / 0.0042 Acc: 98.4375 (99.4420)\n",
      "[9/25][88/96] Loss_D: -1.4074 (-1.3004) Loss_G: 5.4961 (5.1918) D(x): 0.9750 D(G(z)): 0.0036 / 0.0084 Acc: 98.4375 (99.4409)\n",
      "[9/25][89/96] Loss_D: -1.3925 (-1.3005) Loss_G: 6.7911 (5.1934) D(x): 0.9962 D(G(z)): 0.0525 / 0.0014 Acc: 98.4375 (99.4399)\n",
      "[9/25][90/96] Loss_D: -1.3412 (-1.3006) Loss_G: 2.9070 (5.1910) D(x): 0.9183 D(G(z)): 0.0149 / 0.0719 Acc: 100.0000 (99.4404)\n",
      "[9/25][91/96] Loss_D: -1.4241 (-1.3007) Loss_G: 4.3939 (5.1902) D(x): 0.9926 D(G(z)): 0.0474 / 0.0183 Acc: 100.0000 (99.4410)\n",
      "[9/25][92/96] Loss_D: -1.4803 (-1.3009) Loss_G: 6.2334 (5.1913) D(x): 0.9850 D(G(z)): 0.0039 / 0.0024 Acc: 100.0000 (99.4416)\n",
      "[9/25][93/96] Loss_D: -1.5393 (-1.3011) Loss_G: 5.6955 (5.1918) D(x): 0.9977 D(G(z)): 0.0052 / 0.0034 Acc: 100.0000 (99.4422)\n",
      "[9/25][94/96] Loss_D: -1.4716 (-1.3013) Loss_G: 4.5971 (5.1912) D(x): 0.9772 D(G(z)): 0.0044 / 0.0108 Acc: 100.0000 (99.4428)\n",
      "[9/25][95/96] Loss_D: -1.4046 (-1.3014) Loss_G: 6.4642 (5.1925) D(x): 0.9950 D(G(z)): 0.0415 / 0.0026 Acc: 100.0000 (99.4434)\n",
      "[10/25][0/96] Loss_D: -1.5088 (-1.3016) Loss_G: 6.2560 (5.1936) D(x): 0.9658 D(G(z)): 0.0018 / 0.0022 Acc: 100.0000 (99.4439)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[10/25][1/96] Loss_D: -1.4927 (-1.3018) Loss_G: 4.4839 (5.1929) D(x): 0.9902 D(G(z)): 0.0126 / 0.0169 Acc: 98.4375 (99.4429)\n",
      "[10/25][2/96] Loss_D: -1.4489 (-1.3020) Loss_G: 4.5095 (5.1922) D(x): 0.9955 D(G(z)): 0.0149 / 0.0163 Acc: 100.0000 (99.4435)\n",
      "[10/25][3/96] Loss_D: -1.4480 (-1.3021) Loss_G: 4.5008 (5.1915) D(x): 0.9897 D(G(z)): 0.0100 / 0.0196 Acc: 100.0000 (99.4440)\n",
      "[10/25][4/96] Loss_D: -1.5444 (-1.3024) Loss_G: 6.0269 (5.1923) D(x): 0.9822 D(G(z)): 0.0151 / 0.0030 Acc: 100.0000 (99.4446)\n",
      "[10/25][5/96] Loss_D: -1.4408 (-1.3025) Loss_G: 5.7586 (5.1929) D(x): 0.9748 D(G(z)): 0.0312 / 0.0055 Acc: 100.0000 (99.4452)\n",
      "[10/25][6/96] Loss_D: -1.3791 (-1.3026) Loss_G: 5.4410 (5.1932) D(x): 0.9463 D(G(z)): 0.0394 / 0.0061 Acc: 98.4375 (99.4442)\n",
      "[10/25][7/96] Loss_D: -1.4993 (-1.3028) Loss_G: 4.0062 (5.1919) D(x): 0.9953 D(G(z)): 0.0113 / 0.0188 Acc: 100.0000 (99.4447)\n",
      "[10/25][8/96] Loss_D: -1.4726 (-1.3030) Loss_G: 5.9709 (5.1928) D(x): 0.9948 D(G(z)): 0.0064 / 0.0050 Acc: 100.0000 (99.4453)\n",
      "[10/25][9/96] Loss_D: -1.4209 (-1.3031) Loss_G: 5.0932 (5.1926) D(x): 0.9892 D(G(z)): 0.0617 / 0.0127 Acc: 98.4375 (99.4443)\n",
      "[10/25][10/96] Loss_D: -1.4380 (-1.3032) Loss_G: 6.5639 (5.1941) D(x): 0.9715 D(G(z)): 0.0008 / 0.0021 Acc: 100.0000 (99.4448)\n",
      "[10/25][11/96] Loss_D: -1.3240 (-1.3033) Loss_G: 6.5114 (5.1954) D(x): 0.9637 D(G(z)): 0.0115 / 0.0028 Acc: 98.4375 (99.4438)\n",
      "[10/25][12/96] Loss_D: -1.5580 (-1.3035) Loss_G: 3.5069 (5.1937) D(x): 0.9890 D(G(z)): 0.0088 / 0.0365 Acc: 100.0000 (99.4444)\n",
      "[10/25][13/96] Loss_D: -1.4787 (-1.3037) Loss_G: 5.5883 (5.1941) D(x): 0.9755 D(G(z)): 0.0113 / 0.0082 Acc: 100.0000 (99.4449)\n",
      "[10/25][14/96] Loss_D: -1.5067 (-1.3039) Loss_G: 5.0914 (5.1940) D(x): 0.9954 D(G(z)): 0.0193 / 0.0085 Acc: 100.0000 (99.4455)\n",
      "[10/25][15/96] Loss_D: -1.4189 (-1.3040) Loss_G: 4.0570 (5.1928) D(x): 0.9684 D(G(z)): 0.0581 / 0.0261 Acc: 100.0000 (99.4461)\n",
      "[10/25][16/96] Loss_D: -1.4216 (-1.3041) Loss_G: 7.1315 (5.1948) D(x): 0.9903 D(G(z)): 0.0059 / 0.0017 Acc: 100.0000 (99.4466)\n",
      "[10/25][17/96] Loss_D: -1.5087 (-1.3044) Loss_G: 4.8909 (5.1945) D(x): 0.9489 D(G(z)): 0.0076 / 0.0128 Acc: 100.0000 (99.4472)\n",
      "[10/25][18/96] Loss_D: -1.5237 (-1.3046) Loss_G: 3.8457 (5.1931) D(x): 0.9957 D(G(z)): 0.0183 / 0.0233 Acc: 100.0000 (99.4478)\n",
      "[10/25][19/96] Loss_D: -1.4073 (-1.3047) Loss_G: 4.1391 (5.1920) D(x): 0.9801 D(G(z)): 0.0096 / 0.0189 Acc: 100.0000 (99.4483)\n",
      "[10/25][20/96] Loss_D: -1.2811 (-1.3047) Loss_G: 4.6531 (5.1915) D(x): 0.9908 D(G(z)): 0.0489 / 0.0160 Acc: 100.0000 (99.4489)\n",
      "[10/25][21/96] Loss_D: -1.4268 (-1.3048) Loss_G: 4.2167 (5.1905) D(x): 0.9929 D(G(z)): 0.0035 / 0.0190 Acc: 98.4375 (99.4479)\n",
      "[10/25][22/96] Loss_D: -1.4743 (-1.3050) Loss_G: 5.5722 (5.1909) D(x): 0.9893 D(G(z)): 0.0691 / 0.0048 Acc: 100.0000 (99.4484)\n",
      "[10/25][23/96] Loss_D: -0.9321 (-1.3046) Loss_G: -0.1273 (5.1855) D(x): 0.6793 D(G(z)): 0.0173 / 0.7377 Acc: 98.4375 (99.4474)\n",
      "[10/25][24/96] Loss_D: 1.1024 (-1.3021) Loss_G: 13.8355 (5.1943) D(x): 0.9993 D(G(z)): 0.8737 / 0.0000 Acc: 100.0000 (99.4480)\n",
      "[10/25][25/96] Loss_D: 5.9595 (-1.2948) Loss_G: 5.2015 (5.1943) D(x): 0.0016 D(G(z)): 0.0000 / 0.0093 Acc: 100.0000 (99.4485)\n",
      "[10/25][26/96] Loss_D: -1.5083 (-1.2950) Loss_G: -0.0047 (5.1890) D(x): 0.9791 D(G(z)): 0.0153 / 0.6843 Acc: 98.4375 (99.4475)\n",
      "[10/25][27/96] Loss_D: -0.0147 (-1.2937) Loss_G: 6.2382 (5.1901) D(x): 0.9993 D(G(z)): 0.5988 / 0.0042 Acc: 98.4375 (99.4465)\n",
      "[10/25][28/96] Loss_D: -1.3526 (-1.2937) Loss_G: 7.5577 (5.1925) D(x): 0.8971 D(G(z)): 0.0077 / 0.0010 Acc: 100.0000 (99.4470)\n",
      "[10/25][29/96] Loss_D: -0.9607 (-1.2934) Loss_G: 6.1959 (5.1935) D(x): 0.6945 D(G(z)): 0.0015 / 0.0033 Acc: 100.0000 (99.4476)\n",
      "[10/25][30/96] Loss_D: -1.4145 (-1.2935) Loss_G: 2.6700 (5.1909) D(x): 0.9838 D(G(z)): 0.0061 / 0.0875 Acc: 98.4375 (99.4466)\n",
      "[10/25][31/96] Loss_D: -1.3277 (-1.2936) Loss_G: 4.7774 (5.1905) D(x): 0.9995 D(G(z)): 0.1392 / 0.0137 Acc: 100.0000 (99.4471)\n",
      "[10/25][32/96] Loss_D: -1.5826 (-1.2939) Loss_G: 5.9170 (5.1912) D(x): 0.9994 D(G(z)): 0.0098 / 0.0037 Acc: 100.0000 (99.4477)\n",
      "[10/25][33/96] Loss_D: -1.3918 (-1.2940) Loss_G: 5.6447 (5.1917) D(x): 0.9980 D(G(z)): 0.0123 / 0.0053 Acc: 98.4375 (99.4467)\n",
      "[10/25][34/96] Loss_D: -1.5116 (-1.2942) Loss_G: 6.0462 (5.1926) D(x): 0.9985 D(G(z)): 0.0025 / 0.0039 Acc: 100.0000 (99.4472)\n",
      "[10/25][35/96] Loss_D: -1.4829 (-1.2944) Loss_G: 5.0728 (5.1924) D(x): 0.9971 D(G(z)): 0.0139 / 0.0064 Acc: 100.0000 (99.4478)\n",
      "[10/25][36/96] Loss_D: -1.5549 (-1.2946) Loss_G: 5.4141 (5.1927) D(x): 0.9980 D(G(z)): 0.0055 / 0.0059 Acc: 100.0000 (99.4483)\n",
      "[10/25][37/96] Loss_D: -1.5382 (-1.2949) Loss_G: 5.2920 (5.1928) D(x): 0.9969 D(G(z)): 0.0055 / 0.0065 Acc: 98.4375 (99.4473)\n",
      "[10/25][38/96] Loss_D: -1.5482 (-1.2951) Loss_G: 5.1583 (5.1927) D(x): 0.9932 D(G(z)): 0.0073 / 0.0086 Acc: 100.0000 (99.4479)\n",
      "[10/25][39/96] Loss_D: -1.4437 (-1.2953) Loss_G: 6.5753 (5.1941) D(x): 0.9978 D(G(z)): 0.0072 / 0.0019 Acc: 100.0000 (99.4484)\n",
      "[10/25][40/96] Loss_D: -1.5551 (-1.2955) Loss_G: 4.5401 (5.1934) D(x): 0.9980 D(G(z)): 0.0203 / 0.0116 Acc: 98.4375 (99.4474)\n",
      "[10/25][41/96] Loss_D: -1.4174 (-1.2957) Loss_G: 5.0048 (5.1933) D(x): 0.9954 D(G(z)): 0.0298 / 0.0083 Acc: 100.0000 (99.4480)\n",
      "[10/25][42/96] Loss_D: -1.4173 (-1.2958) Loss_G: 4.1966 (5.1923) D(x): 0.9884 D(G(z)): 0.0082 / 0.0217 Acc: 96.8750 (99.4454)\n",
      "[10/25][43/96] Loss_D: -1.5272 (-1.2960) Loss_G: 4.6571 (5.1917) D(x): 0.9917 D(G(z)): 0.0108 / 0.0113 Acc: 98.4375 (99.4444)\n",
      "[10/25][44/96] Loss_D: -1.5093 (-1.2962) Loss_G: 3.9216 (5.1905) D(x): 0.9840 D(G(z)): 0.0191 / 0.0247 Acc: 100.0000 (99.4450)\n",
      "[10/25][45/96] Loss_D: -1.4335 (-1.2964) Loss_G: 4.8563 (5.1901) D(x): 0.9873 D(G(z)): 0.0357 / 0.0099 Acc: 98.4375 (99.4440)\n",
      "[10/25][46/96] Loss_D: -1.4317 (-1.2965) Loss_G: 4.0342 (5.1890) D(x): 0.9305 D(G(z)): 0.0074 / 0.0223 Acc: 100.0000 (99.4445)\n",
      "[10/25][47/96] Loss_D: -1.5172 (-1.2967) Loss_G: 1.9280 (5.1858) D(x): 0.9815 D(G(z)): 0.0106 / 0.1364 Acc: 100.0000 (99.4451)\n",
      "[10/25][48/96] Loss_D: -1.4484 (-1.2969) Loss_G: 3.1191 (5.1837) D(x): 0.9928 D(G(z)): 0.0553 / 0.0586 Acc: 100.0000 (99.4456)\n",
      "[10/25][49/96] Loss_D: -1.1049 (-1.2967) Loss_G: 10.8408 (5.1893) D(x): 0.9862 D(G(z)): 0.3061 / 0.0000 Acc: 100.0000 (99.4462)\n",
      "[10/25][50/96] Loss_D: 0.1218 (-1.2953) Loss_G: 3.7051 (5.1878) D(x): 0.2596 D(G(z)): 0.0000 / 0.0271 Acc: 100.0000 (99.4467)\n",
      "[10/25][51/96] Loss_D: -1.3072 (-1.2953) Loss_G: 0.2147 (5.1829) D(x): 0.9989 D(G(z)): 0.1163 / 0.5830 Acc: 100.0000 (99.4473)\n",
      "[10/25][52/96] Loss_D: -0.0119 (-1.2940) Loss_G: 10.0335 (5.1877) D(x): 0.9998 D(G(z)): 0.6887 / 0.0001 Acc: 100.0000 (99.4478)\n",
      "[10/25][53/96] Loss_D: -1.6221 (-1.2943) Loss_G: 12.1928 (5.1946) D(x): 0.9717 D(G(z)): 0.0001 / 0.0000 Acc: 100.0000 (99.4483)\n",
      "[10/25][54/96] Loss_D: -0.8315 (-1.2939) Loss_G: 8.9565 (5.1983) D(x): 0.6846 D(G(z)): 0.0000 / 0.0003 Acc: 96.8750 (99.4458)\n",
      "[10/25][55/96] Loss_D: -1.4882 (-1.2941) Loss_G: 5.4358 (5.1986) D(x): 0.9526 D(G(z)): 0.0003 / 0.0139 Acc: 98.4375 (99.4448)\n",
      "[10/25][56/96] Loss_D: -1.1035 (-1.2939) Loss_G: 6.1625 (5.1995) D(x): 0.9961 D(G(z)): 0.2249 / 0.0031 Acc: 100.0000 (99.4454)\n",
      "[10/25][57/96] Loss_D: -1.3386 (-1.2939) Loss_G: 7.7882 (5.2021) D(x): 0.9426 D(G(z)): 0.0042 / 0.0006 Acc: 100.0000 (99.4459)\n",
      "[10/25][58/96] Loss_D: -1.4981 (-1.2941) Loss_G: 5.6967 (5.2025) D(x): 0.9370 D(G(z)): 0.0033 / 0.0036 Acc: 98.4375 (99.4449)\n",
      "[10/25][59/96] Loss_D: -1.4445 (-1.2943) Loss_G: 4.1619 (5.2015) D(x): 0.9023 D(G(z)): 0.0059 / 0.0213 Acc: 100.0000 (99.4455)\n",
      "[10/25][60/96] Loss_D: -1.4070 (-1.2944) Loss_G: 4.3365 (5.2007) D(x): 0.9971 D(G(z)): 0.0268 / 0.0198 Acc: 100.0000 (99.4460)\n",
      "[10/25][61/96] Loss_D: -1.4135 (-1.2945) Loss_G: 3.6019 (5.1991) D(x): 0.9957 D(G(z)): 0.0828 / 0.0368 Acc: 100.0000 (99.4466)\n",
      "[10/25][62/96] Loss_D: -1.4767 (-1.2947) Loss_G: 4.4237 (5.1983) D(x): 0.9867 D(G(z)): 0.0367 / 0.0167 Acc: 100.0000 (99.4471)\n",
      "[10/25][63/96] Loss_D: -1.4724 (-1.2949) Loss_G: 5.2917 (5.1984) D(x): 0.9762 D(G(z)): 0.0089 / 0.0072 Acc: 100.0000 (99.4476)\n",
      "[10/25][64/96] Loss_D: -1.4285 (-1.2950) Loss_G: 6.2926 (5.1995) D(x): 0.9858 D(G(z)): 0.0099 / 0.0030 Acc: 100.0000 (99.4482)\n",
      "[10/25][65/96] Loss_D: -1.3726 (-1.2951) Loss_G: 5.4337 (5.1997) D(x): 0.9864 D(G(z)): 0.0041 / 0.0074 Acc: 96.8750 (99.4457)\n",
      "[10/25][66/96] Loss_D: -1.5213 (-1.2953) Loss_G: 3.8218 (5.1984) D(x): 0.9942 D(G(z)): 0.0040 / 0.0296 Acc: 98.4375 (99.4447)\n",
      "[10/25][67/96] Loss_D: -1.4639 (-1.2954) Loss_G: 4.7837 (5.1980) D(x): 0.9878 D(G(z)): 0.0079 / 0.0100 Acc: 100.0000 (99.4452)\n",
      "[10/25][68/96] Loss_D: -1.4169 (-1.2956) Loss_G: 5.5702 (5.1984) D(x): 0.9625 D(G(z)): 0.0255 / 0.0064 Acc: 100.0000 (99.4458)\n",
      "[10/25][69/96] Loss_D: -1.2729 (-1.2955) Loss_G: 5.1453 (5.1983) D(x): 0.9886 D(G(z)): 0.0404 / 0.0108 Acc: 96.8750 (99.4433)\n",
      "[10/25][70/96] Loss_D: -1.3980 (-1.2956) Loss_G: 5.3954 (5.1985) D(x): 0.9857 D(G(z)): 0.0226 / 0.0092 Acc: 100.0000 (99.4438)\n",
      "[10/25][71/96] Loss_D: -1.4978 (-1.2958) Loss_G: 5.3866 (5.1987) D(x): 0.9917 D(G(z)): 0.0236 / 0.0060 Acc: 100.0000 (99.4443)\n",
      "[10/25][72/96] Loss_D: -1.4325 (-1.2960) Loss_G: 5.5276 (5.1990) D(x): 0.9907 D(G(z)): 0.0111 / 0.0066 Acc: 100.0000 (99.4449)\n",
      "[10/25][73/96] Loss_D: -1.4082 (-1.2961) Loss_G: 3.5215 (5.1974) D(x): 0.9162 D(G(z)): 0.0096 / 0.0344 Acc: 100.0000 (99.4454)\n",
      "[10/25][74/96] Loss_D: -1.3784 (-1.2962) Loss_G: 2.7776 (5.1950) D(x): 0.9912 D(G(z)): 0.0601 / 0.0864 Acc: 100.0000 (99.4460)\n",
      "[10/25][75/96] Loss_D: -1.3490 (-1.2962) Loss_G: 5.0442 (5.1949) D(x): 0.9728 D(G(z)): 0.0281 / 0.0146 Acc: 100.0000 (99.4465)\n",
      "[10/25][76/96] Loss_D: -1.4711 (-1.2964) Loss_G: 4.7952 (5.1945) D(x): 0.9933 D(G(z)): 0.0213 / 0.0086 Acc: 98.4375 (99.4455)\n",
      "[10/25][77/96] Loss_D: -1.5013 (-1.2966) Loss_G: 4.0099 (5.1934) D(x): 0.9710 D(G(z)): 0.0147 / 0.0253 Acc: 100.0000 (99.4461)\n",
      "[10/25][78/96] Loss_D: -1.5487 (-1.2968) Loss_G: 4.3482 (5.1925) D(x): 0.9873 D(G(z)): 0.0293 / 0.0168 Acc: 100.0000 (99.4466)\n",
      "[10/25][79/96] Loss_D: -1.4356 (-1.2969) Loss_G: 5.5815 (5.1929) D(x): 0.9887 D(G(z)): 0.0208 / 0.0052 Acc: 100.0000 (99.4471)\n",
      "[10/25][80/96] Loss_D: -1.5061 (-1.2971) Loss_G: 4.9407 (5.1927) D(x): 0.9730 D(G(z)): 0.0118 / 0.0110 Acc: 100.0000 (99.4476)\n",
      "[10/25][81/96] Loss_D: -1.4196 (-1.2973) Loss_G: 4.3338 (5.1919) D(x): 0.9342 D(G(z)): 0.0029 / 0.0181 Acc: 100.0000 (99.4482)\n",
      "[10/25][82/96] Loss_D: -1.4372 (-1.2974) Loss_G: 2.7817 (5.1895) D(x): 0.9833 D(G(z)): 0.0289 / 0.0838 Acc: 100.0000 (99.4487)\n",
      "[10/25][83/96] Loss_D: -1.3609 (-1.2975) Loss_G: 2.5290 (5.1870) D(x): 0.9572 D(G(z)): 0.0279 / 0.1178 Acc: 100.0000 (99.4492)\n",
      "[10/25][84/96] Loss_D: -1.2234 (-1.2974) Loss_G: 3.8391 (5.1857) D(x): 0.9871 D(G(z)): 0.1593 / 0.0318 Acc: 100.0000 (99.4498)\n",
      "[10/25][85/96] Loss_D: -1.4111 (-1.2975) Loss_G: 6.6206 (5.1871) D(x): 0.9712 D(G(z)): 0.0118 / 0.0016 Acc: 98.4375 (99.4488)\n",
      "[10/25][86/96] Loss_D: -1.4075 (-1.2976) Loss_G: 5.3047 (5.1872) D(x): 0.9202 D(G(z)): 0.0024 / 0.0050 Acc: 100.0000 (99.4493)\n",
      "[10/25][87/96] Loss_D: -1.3486 (-1.2977) Loss_G: 3.1301 (5.1852) D(x): 0.9138 D(G(z)): 0.0285 / 0.0507 Acc: 100.0000 (99.4498)\n",
      "[10/25][88/96] Loss_D: -1.5145 (-1.2979) Loss_G: 3.3036 (5.1834) D(x): 0.9878 D(G(z)): 0.0182 / 0.0518 Acc: 100.0000 (99.4504)\n",
      "[10/25][89/96] Loss_D: -1.3133 (-1.2979) Loss_G: 6.0546 (5.1843) D(x): 0.9990 D(G(z)): 0.1194 / 0.0031 Acc: 100.0000 (99.4509)\n",
      "[10/25][90/96] Loss_D: -1.4026 (-1.2980) Loss_G: 5.6796 (5.1847) D(x): 0.9720 D(G(z)): 0.0046 / 0.0041 Acc: 100.0000 (99.4514)\n",
      "[10/25][91/96] Loss_D: -1.2597 (-1.2979) Loss_G: 2.3268 (5.1820) D(x): 0.8604 D(G(z)): 0.0237 / 0.1132 Acc: 98.4375 (99.4505)\n",
      "[10/25][92/96] Loss_D: -1.5582 (-1.2982) Loss_G: 3.5456 (5.1805) D(x): 0.9926 D(G(z)): 0.0122 / 0.0334 Acc: 100.0000 (99.4510)\n",
      "[10/25][93/96] Loss_D: -1.3059 (-1.2982) Loss_G: 3.1083 (5.1785) D(x): 0.9803 D(G(z)): 0.0715 / 0.0647 Acc: 98.4375 (99.4500)\n",
      "[10/25][94/96] Loss_D: -1.3822 (-1.2983) Loss_G: 2.8795 (5.1763) D(x): 0.9969 D(G(z)): 0.0348 / 0.0781 Acc: 100.0000 (99.4505)\n",
      "[10/25][95/96] Loss_D: -1.4488 (-1.2984) Loss_G: 5.1388 (5.1763) D(x): 0.9938 D(G(z)): 0.0133 / 0.0071 Acc: 98.4375 (99.4496)\n",
      "[11/25][0/96] Loss_D: -1.4743 (-1.2986) Loss_G: 5.1122 (5.1762) D(x): 0.9848 D(G(z)): 0.0096 / 0.0073 Acc: 96.8750 (99.4471)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[11/25][1/96] Loss_D: -1.4354 (-1.2987) Loss_G: 6.2287 (5.1772) D(x): 0.9669 D(G(z)): 0.0120 / 0.0027 Acc: 100.0000 (99.4477)\n",
      "[11/25][2/96] Loss_D: -1.5061 (-1.2989) Loss_G: 3.8612 (5.1760) D(x): 0.9934 D(G(z)): 0.0180 / 0.0290 Acc: 98.4375 (99.4467)\n",
      "[11/25][3/96] Loss_D: -1.3502 (-1.2990) Loss_G: 2.9741 (5.1739) D(x): 0.9765 D(G(z)): 0.0289 / 0.0720 Acc: 100.0000 (99.4472)\n",
      "[11/25][4/96] Loss_D: -1.4940 (-1.2991) Loss_G: 4.5078 (5.1733) D(x): 0.9880 D(G(z)): 0.0092 / 0.0176 Acc: 98.4375 (99.4463)\n",
      "[11/25][5/96] Loss_D: -1.4086 (-1.2992) Loss_G: 7.3461 (5.1753) D(x): 0.9922 D(G(z)): 0.0053 / 0.0015 Acc: 98.4375 (99.4453)\n",
      "[11/25][6/96] Loss_D: -1.3085 (-1.2993) Loss_G: 4.7761 (5.1749) D(x): 0.9516 D(G(z)): 0.0911 / 0.0121 Acc: 100.0000 (99.4458)\n",
      "[11/25][7/96] Loss_D: -1.4578 (-1.2994) Loss_G: 4.0169 (5.1739) D(x): 0.9345 D(G(z)): 0.0015 / 0.0247 Acc: 100.0000 (99.4464)\n",
      "[11/25][8/96] Loss_D: -1.5241 (-1.2996) Loss_G: 6.9760 (5.1755) D(x): 0.9786 D(G(z)): 0.0136 / 0.0012 Acc: 100.0000 (99.4469)\n",
      "[11/25][9/96] Loss_D: -1.4079 (-1.2997) Loss_G: 4.7372 (5.1751) D(x): 0.9929 D(G(z)): 0.0638 / 0.0135 Acc: 100.0000 (99.4474)\n",
      "[11/25][10/96] Loss_D: -1.3322 (-1.2997) Loss_G: 7.2521 (5.1771) D(x): 0.9639 D(G(z)): 0.0044 / 0.0021 Acc: 100.0000 (99.4479)\n",
      "[11/25][11/96] Loss_D: -1.4637 (-1.2999) Loss_G: 4.9861 (5.1769) D(x): 0.9974 D(G(z)): 0.0178 / 0.0090 Acc: 98.4375 (99.4470)\n",
      "[11/25][12/96] Loss_D: -1.4894 (-1.3001) Loss_G: 6.1292 (5.1778) D(x): 0.9836 D(G(z)): 0.0093 / 0.0036 Acc: 98.4375 (99.4460)\n",
      "[11/25][13/96] Loss_D: -1.4371 (-1.3002) Loss_G: 4.6609 (5.1773) D(x): 0.9573 D(G(z)): 0.0024 / 0.0119 Acc: 98.4375 (99.4451)\n",
      "[11/25][14/96] Loss_D: -1.5808 (-1.3005) Loss_G: 2.9210 (5.1752) D(x): 0.9796 D(G(z)): 0.0220 / 0.0614 Acc: 100.0000 (99.4456)\n",
      "[11/25][15/96] Loss_D: -1.3586 (-1.3005) Loss_G: 5.6171 (5.1756) D(x): 0.9923 D(G(z)): 0.0476 / 0.0064 Acc: 100.0000 (99.4461)\n",
      "[11/25][16/96] Loss_D: -1.3647 (-1.3006) Loss_G: 4.1300 (5.1746) D(x): 0.9936 D(G(z)): 0.0320 / 0.0268 Acc: 100.0000 (99.4466)\n",
      "[11/25][17/96] Loss_D: -1.2668 (-1.3005) Loss_G: 2.6787 (5.1723) D(x): 0.8790 D(G(z)): 0.0076 / 0.0772 Acc: 98.4375 (99.4457)\n",
      "[11/25][18/96] Loss_D: -1.2094 (-1.3005) Loss_G: 6.8336 (5.1739) D(x): 0.9910 D(G(z)): 0.1843 / 0.0024 Acc: 100.0000 (99.4462)\n",
      "[11/25][19/96] Loss_D: -1.3999 (-1.3006) Loss_G: 6.6003 (5.1752) D(x): 0.9250 D(G(z)): 0.0009 / 0.0019 Acc: 100.0000 (99.4467)\n",
      "[11/25][20/96] Loss_D: -1.3937 (-1.3006) Loss_G: 4.6722 (5.1747) D(x): 0.9512 D(G(z)): 0.0076 / 0.0122 Acc: 100.0000 (99.4472)\n",
      "[11/25][21/96] Loss_D: -1.5109 (-1.3008) Loss_G: 5.5314 (5.1750) D(x): 0.9737 D(G(z)): 0.0047 / 0.0069 Acc: 100.0000 (99.4478)\n",
      "[11/25][22/96] Loss_D: -1.5034 (-1.3010) Loss_G: 3.2397 (5.1733) D(x): 0.9927 D(G(z)): 0.0048 / 0.0482 Acc: 100.0000 (99.4483)\n",
      "[11/25][23/96] Loss_D: -1.4589 (-1.3012) Loss_G: 4.8389 (5.1729) D(x): 0.9839 D(G(z)): 0.0085 / 0.0130 Acc: 100.0000 (99.4488)\n",
      "[11/25][24/96] Loss_D: -1.4819 (-1.3013) Loss_G: 2.8376 (5.1708) D(x): 0.9975 D(G(z)): 0.0661 / 0.0582 Acc: 100.0000 (99.4493)\n",
      "[11/25][25/96] Loss_D: -1.4600 (-1.3015) Loss_G: 7.3882 (5.1728) D(x): 0.9850 D(G(z)): 0.0088 / 0.0008 Acc: 100.0000 (99.4498)\n",
      "[11/25][26/96] Loss_D: -1.3478 (-1.3015) Loss_G: 6.1928 (5.1738) D(x): 0.9575 D(G(z)): 0.0050 / 0.0030 Acc: 100.0000 (99.4503)\n",
      "[11/25][27/96] Loss_D: -1.3569 (-1.3016) Loss_G: 4.5901 (5.1732) D(x): 0.9563 D(G(z)): 0.0023 / 0.0186 Acc: 100.0000 (99.4508)\n",
      "[11/25][28/96] Loss_D: -1.4122 (-1.3017) Loss_G: 5.5613 (5.1736) D(x): 0.9964 D(G(z)): 0.0754 / 0.0046 Acc: 100.0000 (99.4513)\n",
      "[11/25][29/96] Loss_D: -1.5199 (-1.3019) Loss_G: 3.1140 (5.1717) D(x): 0.9932 D(G(z)): 0.0043 / 0.0534 Acc: 100.0000 (99.4518)\n",
      "[11/25][30/96] Loss_D: -1.5957 (-1.3022) Loss_G: 6.2304 (5.1727) D(x): 0.9724 D(G(z)): 0.0006 / 0.0024 Acc: 100.0000 (99.4523)\n",
      "[11/25][31/96] Loss_D: -1.6574 (-1.3025) Loss_G: 5.1173 (5.1726) D(x): 0.9776 D(G(z)): 0.0069 / 0.0074 Acc: 100.0000 (99.4528)\n",
      "[11/25][32/96] Loss_D: -1.3863 (-1.3026) Loss_G: 3.6563 (5.1712) D(x): 0.9914 D(G(z)): 0.0258 / 0.0289 Acc: 100.0000 (99.4533)\n",
      "[11/25][33/96] Loss_D: -1.3796 (-1.3026) Loss_G: 7.5371 (5.1734) D(x): 0.9947 D(G(z)): 0.1676 / 0.0007 Acc: 98.4375 (99.4524)\n",
      "[11/25][34/96] Loss_D: -1.3809 (-1.3027) Loss_G: 7.1785 (5.1752) D(x): 0.8628 D(G(z)): 0.0001 / 0.0011 Acc: 98.4375 (99.4515)\n",
      "[11/25][35/96] Loss_D: -1.5120 (-1.3029) Loss_G: 5.6539 (5.1757) D(x): 0.9403 D(G(z)): 0.0004 / 0.0039 Acc: 96.8750 (99.4491)\n",
      "[11/25][36/96] Loss_D: -1.5052 (-1.3031) Loss_G: 3.8320 (5.1744) D(x): 0.9879 D(G(z)): 0.0275 / 0.0214 Acc: 100.0000 (99.4496)\n",
      "[11/25][37/96] Loss_D: -1.4428 (-1.3032) Loss_G: 3.2996 (5.1727) D(x): 0.9677 D(G(z)): 0.0207 / 0.0528 Acc: 100.0000 (99.4501)\n",
      "[11/25][38/96] Loss_D: -1.4725 (-1.3034) Loss_G: 3.3645 (5.1711) D(x): 0.9928 D(G(z)): 0.0042 / 0.0424 Acc: 100.0000 (99.4506)\n",
      "[11/25][39/96] Loss_D: -1.4240 (-1.3035) Loss_G: 5.8783 (5.1717) D(x): 0.9815 D(G(z)): 0.0246 / 0.0045 Acc: 98.4375 (99.4497)\n",
      "[11/25][40/96] Loss_D: -1.5212 (-1.3037) Loss_G: 5.7573 (5.1723) D(x): 0.9956 D(G(z)): 0.0055 / 0.0041 Acc: 100.0000 (99.4502)\n",
      "[11/25][41/96] Loss_D: -1.3391 (-1.3037) Loss_G: 6.3614 (5.1733) D(x): 0.9923 D(G(z)): 0.0729 / 0.0026 Acc: 96.8750 (99.4479)\n",
      "[11/25][42/96] Loss_D: -1.5942 (-1.3040) Loss_G: 7.8008 (5.1757) D(x): 0.9854 D(G(z)): 0.0001 / 0.0005 Acc: 98.4375 (99.4469)\n",
      "[11/25][43/96] Loss_D: -1.4529 (-1.3041) Loss_G: 3.0010 (5.1738) D(x): 0.9330 D(G(z)): 0.0029 / 0.0628 Acc: 98.4375 (99.4460)\n",
      "[11/25][44/96] Loss_D: -1.5275 (-1.3043) Loss_G: 5.4129 (5.1740) D(x): 0.9902 D(G(z)): 0.0093 / 0.0070 Acc: 98.4375 (99.4451)\n",
      "[11/25][45/96] Loss_D: -1.3929 (-1.3044) Loss_G: 5.2256 (5.1740) D(x): 0.9821 D(G(z)): 0.0105 / 0.0065 Acc: 100.0000 (99.4456)\n",
      "[11/25][46/96] Loss_D: -1.4329 (-1.3045) Loss_G: 5.4769 (5.1743) D(x): 0.9839 D(G(z)): 0.0036 / 0.0052 Acc: 98.4375 (99.4447)\n",
      "[11/25][47/96] Loss_D: -1.3227 (-1.3045) Loss_G: 6.4475 (5.1755) D(x): 0.9843 D(G(z)): 0.1272 / 0.0018 Acc: 98.4375 (99.4438)\n",
      "[11/25][48/96] Loss_D: -1.4344 (-1.3046) Loss_G: 5.7476 (5.1760) D(x): 0.9149 D(G(z)): 0.0022 / 0.0037 Acc: 100.0000 (99.4443)\n",
      "[11/25][49/96] Loss_D: -1.4796 (-1.3048) Loss_G: 5.2919 (5.1761) D(x): 0.9667 D(G(z)): 0.0005 / 0.0077 Acc: 100.0000 (99.4448)\n",
      "[11/25][50/96] Loss_D: -1.6256 (-1.3051) Loss_G: 7.1014 (5.1778) D(x): 0.9881 D(G(z)): 0.0029 / 0.0015 Acc: 100.0000 (99.4453)\n",
      "[11/25][51/96] Loss_D: -1.5021 (-1.3053) Loss_G: 5.2133 (5.1778) D(x): 0.9874 D(G(z)): 0.0007 / 0.0078 Acc: 100.0000 (99.4458)\n",
      "[11/25][52/96] Loss_D: -1.3608 (-1.3053) Loss_G: 6.1206 (5.1787) D(x): 0.9987 D(G(z)): 0.0644 / 0.0031 Acc: 98.4375 (99.4449)\n",
      "[11/25][53/96] Loss_D: -1.4140 (-1.3054) Loss_G: 5.9462 (5.1794) D(x): 0.9962 D(G(z)): 0.0040 / 0.0044 Acc: 100.0000 (99.4454)\n",
      "[11/25][54/96] Loss_D: -1.5522 (-1.3056) Loss_G: 8.1547 (5.1821) D(x): 0.9775 D(G(z)): 0.0024 / 0.0004 Acc: 100.0000 (99.4459)\n",
      "[11/25][55/96] Loss_D: -1.3928 (-1.3057) Loss_G: 5.5749 (5.1824) D(x): 0.9876 D(G(z)): 0.0159 / 0.0063 Acc: 100.0000 (99.4464)\n",
      "[11/25][56/96] Loss_D: -1.4437 (-1.3058) Loss_G: 4.6833 (5.1820) D(x): 0.9318 D(G(z)): 0.0132 / 0.0120 Acc: 100.0000 (99.4469)\n",
      "[11/25][57/96] Loss_D: -1.5154 (-1.3060) Loss_G: 3.7303 (5.1807) D(x): 0.9970 D(G(z)): 0.0272 / 0.0316 Acc: 98.4375 (99.4460)\n",
      "[11/25][58/96] Loss_D: -1.4645 (-1.3062) Loss_G: 5.1552 (5.1806) D(x): 0.9886 D(G(z)): 0.0229 / 0.0072 Acc: 100.0000 (99.4465)\n",
      "[11/25][59/96] Loss_D: -1.4892 (-1.3063) Loss_G: 6.4322 (5.1818) D(x): 0.9929 D(G(z)): 0.0036 / 0.0019 Acc: 100.0000 (99.4470)\n",
      "[11/25][60/96] Loss_D: -1.4541 (-1.3065) Loss_G: 4.4216 (5.1811) D(x): 0.9643 D(G(z)): 0.0080 / 0.0203 Acc: 98.4375 (99.4461)\n",
      "[11/25][61/96] Loss_D: -1.4157 (-1.3066) Loss_G: 4.2435 (5.1802) D(x): 0.9983 D(G(z)): 0.0044 / 0.0232 Acc: 100.0000 (99.4466)\n",
      "[11/25][62/96] Loss_D: -1.4560 (-1.3067) Loss_G: 6.3502 (5.1813) D(x): 0.9984 D(G(z)): 0.0109 / 0.0026 Acc: 100.0000 (99.4471)\n",
      "[11/25][63/96] Loss_D: -1.3389 (-1.3067) Loss_G: 7.9695 (5.1838) D(x): 0.9973 D(G(z)): 0.1043 / 0.0007 Acc: 100.0000 (99.4475)\n",
      "[11/25][64/96] Loss_D: -1.4416 (-1.3068) Loss_G: 6.2817 (5.1848) D(x): 0.9011 D(G(z)): 0.0003 / 0.0027 Acc: 100.0000 (99.4480)\n",
      "[11/25][65/96] Loss_D: -1.4890 (-1.3070) Loss_G: 4.4877 (5.1841) D(x): 0.9620 D(G(z)): 0.0017 / 0.0134 Acc: 98.4375 (99.4471)\n",
      "[11/25][66/96] Loss_D: -1.4718 (-1.3071) Loss_G: 4.7457 (5.1837) D(x): 0.9978 D(G(z)): 0.0100 / 0.0147 Acc: 100.0000 (99.4476)\n",
      "[11/25][67/96] Loss_D: -1.4679 (-1.3073) Loss_G: 5.9554 (5.1844) D(x): 0.9969 D(G(z)): 0.0131 / 0.0043 Acc: 100.0000 (99.4481)\n",
      "[11/25][68/96] Loss_D: -1.4499 (-1.3074) Loss_G: 4.4940 (5.1838) D(x): 0.9942 D(G(z)): 0.0127 / 0.0203 Acc: 96.8750 (99.4458)\n",
      "[11/25][69/96] Loss_D: -1.4986 (-1.3076) Loss_G: 8.0751 (5.1864) D(x): 0.9988 D(G(z)): 0.0154 / 0.0008 Acc: 100.0000 (99.4463)\n",
      "[11/25][70/96] Loss_D: -1.3930 (-1.3077) Loss_G: 4.6209 (5.1859) D(x): 0.9963 D(G(z)): 0.0375 / 0.0151 Acc: 100.0000 (99.4468)\n",
      "[11/25][71/96] Loss_D: -1.5663 (-1.3079) Loss_G: 7.0011 (5.1875) D(x): 0.9776 D(G(z)): 0.0006 / 0.0013 Acc: 100.0000 (99.4473)\n",
      "[11/25][72/96] Loss_D: -1.5446 (-1.3081) Loss_G: 8.3832 (5.1903) D(x): 0.9834 D(G(z)): 0.0007 / 0.0005 Acc: 100.0000 (99.4478)\n",
      "[11/25][73/96] Loss_D: -1.4122 (-1.3082) Loss_G: 5.9327 (5.1910) D(x): 0.9809 D(G(z)): 0.0054 / 0.0038 Acc: 100.0000 (99.4483)\n",
      "[11/25][74/96] Loss_D: -1.6214 (-1.3085) Loss_G: 9.0194 (5.1944) D(x): 0.9966 D(G(z)): 0.0002 / 0.0001 Acc: 100.0000 (99.4488)\n",
      "[11/25][75/96] Loss_D: -1.5348 (-1.3087) Loss_G: 2.7656 (5.1922) D(x): 0.9821 D(G(z)): 0.0087 / 0.0995 Acc: 100.0000 (99.4493)\n",
      "[11/25][76/96] Loss_D: -1.4454 (-1.3088) Loss_G: 6.8405 (5.1937) D(x): 0.9910 D(G(z)): 0.0138 / 0.0016 Acc: 98.4375 (99.4484)\n",
      "[11/25][77/96] Loss_D: -1.5088 (-1.3090) Loss_G: 5.4591 (5.1939) D(x): 0.9948 D(G(z)): 0.0015 / 0.0061 Acc: 100.0000 (99.4489)\n",
      "[11/25][78/96] Loss_D: -1.5389 (-1.3092) Loss_G: 5.4944 (5.1942) D(x): 0.9940 D(G(z)): 0.0019 / 0.0079 Acc: 100.0000 (99.4493)\n",
      "[11/25][79/96] Loss_D: -1.4268 (-1.3093) Loss_G: 7.7658 (5.1964) D(x): 0.9898 D(G(z)): 0.0790 / 0.0007 Acc: 98.4375 (99.4484)\n",
      "[11/25][80/96] Loss_D: -0.7583 (-1.3088) Loss_G: -0.0468 (5.1918) D(x): 0.6070 D(G(z)): 0.0001 / 0.7177 Acc: 100.0000 (99.4489)\n",
      "[11/25][81/96] Loss_D: 3.4642 (-1.3046) Loss_G: 15.0195 (5.2005) D(x): 1.0000 D(G(z)): 0.9761 / 0.0000 Acc: 100.0000 (99.4494)\n",
      "[11/25][82/96] Loss_D: 0.0518 (-1.3034) Loss_G: 1.4245 (5.1971) D(x): 0.3189 D(G(z)): 0.0000 / 0.2758 Acc: 100.0000 (99.4499)\n",
      "[11/25][83/96] Loss_D: -0.9184 (-1.3031) Loss_G: 3.5916 (5.1957) D(x): 0.9880 D(G(z)): 0.3156 / 0.0532 Acc: 100.0000 (99.4504)\n",
      "[11/25][84/96] Loss_D: -1.5013 (-1.3032) Loss_G: 5.9218 (5.1964) D(x): 0.9811 D(G(z)): 0.0209 / 0.0048 Acc: 100.0000 (99.4509)\n",
      "[11/25][85/96] Loss_D: -1.4426 (-1.3034) Loss_G: 6.4471 (5.1975) D(x): 0.9642 D(G(z)): 0.0031 / 0.0029 Acc: 100.0000 (99.4513)\n",
      "[11/25][86/96] Loss_D: -1.6041 (-1.3036) Loss_G: 7.2887 (5.1993) D(x): 0.9822 D(G(z)): 0.0019 / 0.0010 Acc: 100.0000 (99.4518)\n",
      "[11/25][87/96] Loss_D: -1.5260 (-1.3038) Loss_G: 6.9021 (5.2008) D(x): 0.9828 D(G(z)): 0.0027 / 0.0011 Acc: 100.0000 (99.4523)\n",
      "[11/25][88/96] Loss_D: -1.4022 (-1.3039) Loss_G: 5.4944 (5.2010) D(x): 0.9734 D(G(z)): 0.0078 / 0.0065 Acc: 100.0000 (99.4528)\n",
      "[11/25][89/96] Loss_D: -1.5695 (-1.3041) Loss_G: 4.2253 (5.2002) D(x): 0.9923 D(G(z)): 0.0451 / 0.0152 Acc: 100.0000 (99.4533)\n",
      "[11/25][90/96] Loss_D: -1.5015 (-1.3043) Loss_G: 6.3079 (5.2012) D(x): 0.9725 D(G(z)): 0.0160 / 0.0024 Acc: 100.0000 (99.4537)\n",
      "[11/25][91/96] Loss_D: -1.4666 (-1.3044) Loss_G: 5.8043 (5.2017) D(x): 0.9896 D(G(z)): 0.0071 / 0.0038 Acc: 100.0000 (99.4542)\n",
      "[11/25][92/96] Loss_D: -1.5520 (-1.3047) Loss_G: 5.8632 (5.2023) D(x): 0.9918 D(G(z)): 0.0021 / 0.0038 Acc: 100.0000 (99.4547)\n",
      "[11/25][93/96] Loss_D: -1.4457 (-1.3048) Loss_G: 5.4932 (5.2025) D(x): 0.9656 D(G(z)): 0.0028 / 0.0055 Acc: 100.0000 (99.4552)\n",
      "[11/25][94/96] Loss_D: -1.3809 (-1.3049) Loss_G: 4.9537 (5.2023) D(x): 0.9918 D(G(z)): 0.0014 / 0.0114 Acc: 98.4375 (99.4543)\n",
      "[11/25][95/96] Loss_D: -1.4050 (-1.3049) Loss_G: 5.0166 (5.2021) D(x): 0.9873 D(G(z)): 0.0034 / 0.0102 Acc: 100.0000 (99.4548)\n",
      "[12/25][0/96] Loss_D: -1.6228 (-1.3052) Loss_G: 5.4850 (5.2024) D(x): 0.9959 D(G(z)): 0.0135 / 0.0062 Acc: 100.0000 (99.4552)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[12/25][1/96] Loss_D: -1.4167 (-1.3053) Loss_G: 4.8485 (5.2021) D(x): 0.9917 D(G(z)): 0.0270 / 0.0111 Acc: 100.0000 (99.4557)\n",
      "[12/25][2/96] Loss_D: -1.4287 (-1.3054) Loss_G: 5.2565 (5.2021) D(x): 0.9858 D(G(z)): 0.0093 / 0.0143 Acc: 100.0000 (99.4562)\n",
      "[12/25][3/96] Loss_D: -1.4536 (-1.3055) Loss_G: 4.7348 (5.2017) D(x): 0.9898 D(G(z)): 0.0196 / 0.0131 Acc: 98.4375 (99.4553)\n",
      "[12/25][4/96] Loss_D: -1.3636 (-1.3056) Loss_G: 5.0754 (5.2016) D(x): 0.9897 D(G(z)): 0.0658 / 0.0118 Acc: 100.0000 (99.4558)\n",
      "[12/25][5/96] Loss_D: -1.5156 (-1.3058) Loss_G: 7.4540 (5.2036) D(x): 0.9865 D(G(z)): 0.0018 / 0.0010 Acc: 98.4375 (99.4549)\n",
      "[12/25][6/96] Loss_D: -1.5287 (-1.3060) Loss_G: 6.3016 (5.2045) D(x): 0.9686 D(G(z)): 0.0007 / 0.0026 Acc: 100.0000 (99.4553)\n",
      "[12/25][7/96] Loss_D: -1.3792 (-1.3060) Loss_G: 3.5410 (5.2031) D(x): 0.9406 D(G(z)): 0.0020 / 0.0372 Acc: 96.8750 (99.4531)\n",
      "[12/25][8/96] Loss_D: -1.4202 (-1.3061) Loss_G: 4.0435 (5.2021) D(x): 0.9645 D(G(z)): 0.0104 / 0.0316 Acc: 98.4375 (99.4523)\n",
      "[12/25][9/96] Loss_D: -1.3479 (-1.3062) Loss_G: 5.1352 (5.2020) D(x): 0.9980 D(G(z)): 0.0386 / 0.0090 Acc: 100.0000 (99.4527)\n",
      "[12/25][10/96] Loss_D: -1.3575 (-1.3062) Loss_G: 4.6599 (5.2015) D(x): 0.9852 D(G(z)): 0.0175 / 0.0139 Acc: 100.0000 (99.4532)\n",
      "[12/25][11/96] Loss_D: -1.3706 (-1.3063) Loss_G: 5.4542 (5.2018) D(x): 0.9880 D(G(z)): 0.0077 / 0.0060 Acc: 100.0000 (99.4537)\n",
      "[12/25][12/96] Loss_D: -1.3405 (-1.3063) Loss_G: 4.4806 (5.2011) D(x): 0.9732 D(G(z)): 0.0213 / 0.0188 Acc: 100.0000 (99.4541)\n",
      "[12/25][13/96] Loss_D: -1.5076 (-1.3065) Loss_G: 4.8235 (5.2008) D(x): 0.9891 D(G(z)): 0.0120 / 0.0103 Acc: 98.4375 (99.4533)\n",
      "[12/25][14/96] Loss_D: -1.4964 (-1.3066) Loss_G: 4.6589 (5.2004) D(x): 0.9711 D(G(z)): 0.0194 / 0.0149 Acc: 100.0000 (99.4537)\n",
      "[12/25][15/96] Loss_D: -1.4274 (-1.3067) Loss_G: 6.0884 (5.2011) D(x): 0.9915 D(G(z)): 0.0576 / 0.0029 Acc: 100.0000 (99.4542)\n",
      "[12/25][16/96] Loss_D: -1.2642 (-1.3067) Loss_G: 5.6098 (5.2015) D(x): 0.9249 D(G(z)): 0.0251 / 0.0051 Acc: 96.8750 (99.4520)\n",
      "[12/25][17/96] Loss_D: -1.4596 (-1.3068) Loss_G: 6.2364 (5.2024) D(x): 0.9929 D(G(z)): 0.0308 / 0.0035 Acc: 100.0000 (99.4525)\n",
      "[12/25][18/96] Loss_D: -1.4725 (-1.3070) Loss_G: 4.7493 (5.2020) D(x): 0.9913 D(G(z)): 0.0030 / 0.0133 Acc: 100.0000 (99.4529)\n",
      "[12/25][19/96] Loss_D: -1.3477 (-1.3070) Loss_G: 1.5636 (5.1989) D(x): 0.8905 D(G(z)): 0.0012 / 0.2180 Acc: 98.4375 (99.4521)\n",
      "[12/25][20/96] Loss_D: -1.4542 (-1.3071) Loss_G: 3.9424 (5.1978) D(x): 0.9923 D(G(z)): 0.0905 / 0.0295 Acc: 100.0000 (99.4525)\n",
      "[12/25][21/96] Loss_D: -1.4827 (-1.3073) Loss_G: 4.0448 (5.1968) D(x): 0.9895 D(G(z)): 0.0063 / 0.0285 Acc: 100.0000 (99.4530)\n",
      "[12/25][22/96] Loss_D: -1.5222 (-1.3075) Loss_G: 5.8024 (5.1973) D(x): 0.9808 D(G(z)): 0.0049 / 0.0044 Acc: 100.0000 (99.4535)\n",
      "[12/25][23/96] Loss_D: -1.4197 (-1.3076) Loss_G: 4.5364 (5.1968) D(x): 0.9808 D(G(z)): 0.0274 / 0.0119 Acc: 100.0000 (99.4539)\n",
      "[12/25][24/96] Loss_D: -1.3790 (-1.3076) Loss_G: 2.9820 (5.1949) D(x): 0.9576 D(G(z)): 0.0129 / 0.0691 Acc: 98.4375 (99.4531)\n",
      "[12/25][25/96] Loss_D: -1.4391 (-1.3077) Loss_G: 4.0046 (5.1939) D(x): 0.9665 D(G(z)): 0.0248 / 0.0242 Acc: 100.0000 (99.4535)\n",
      "[12/25][26/96] Loss_D: -1.4953 (-1.3079) Loss_G: 2.8696 (5.1919) D(x): 0.9891 D(G(z)): 0.0380 / 0.0631 Acc: 100.0000 (99.4540)\n",
      "[12/25][27/96] Loss_D: -1.4273 (-1.3080) Loss_G: 4.7232 (5.1915) D(x): 0.9966 D(G(z)): 0.0476 / 0.0124 Acc: 98.4375 (99.4531)\n",
      "[12/25][28/96] Loss_D: -1.3272 (-1.3080) Loss_G: 5.2943 (5.1916) D(x): 0.9060 D(G(z)): 0.0473 / 0.0056 Acc: 98.4375 (99.4523)\n",
      "[12/25][29/96] Loss_D: -1.4531 (-1.3081) Loss_G: 5.6480 (5.1920) D(x): 0.9822 D(G(z)): 0.0125 / 0.0059 Acc: 100.0000 (99.4527)\n",
      "[12/25][30/96] Loss_D: -1.3808 (-1.3082) Loss_G: 5.4012 (5.1921) D(x): 0.9784 D(G(z)): 0.0322 / 0.0061 Acc: 98.4375 (99.4519)\n",
      "[12/25][31/96] Loss_D: -1.4732 (-1.3083) Loss_G: 5.2032 (5.1922) D(x): 0.9861 D(G(z)): 0.0122 / 0.0075 Acc: 100.0000 (99.4523)\n",
      "[12/25][32/96] Loss_D: -1.4375 (-1.3084) Loss_G: 5.8716 (5.1927) D(x): 0.9869 D(G(z)): 0.0310 / 0.0058 Acc: 100.0000 (99.4528)\n",
      "[12/25][33/96] Loss_D: -1.4062 (-1.3085) Loss_G: 3.1367 (5.1910) D(x): 0.9209 D(G(z)): 0.0189 / 0.0489 Acc: 98.4375 (99.4519)\n",
      "[12/25][34/96] Loss_D: -1.5632 (-1.3087) Loss_G: 4.2023 (5.1902) D(x): 0.9897 D(G(z)): 0.0189 / 0.0194 Acc: 100.0000 (99.4524)\n",
      "[12/25][35/96] Loss_D: -1.4413 (-1.3088) Loss_G: 3.0477 (5.1884) D(x): 0.9906 D(G(z)): 0.0171 / 0.0630 Acc: 98.4375 (99.4515)\n",
      "[12/25][36/96] Loss_D: -1.3976 (-1.3089) Loss_G: 4.1401 (5.1875) D(x): 0.9958 D(G(z)): 0.0342 / 0.0247 Acc: 100.0000 (99.4520)\n",
      "[12/25][37/96] Loss_D: -1.4362 (-1.3090) Loss_G: 3.3024 (5.1859) D(x): 0.9472 D(G(z)): 0.0200 / 0.0442 Acc: 100.0000 (99.4525)\n",
      "[12/25][38/96] Loss_D: -1.4934 (-1.3092) Loss_G: 3.2806 (5.1843) D(x): 0.9863 D(G(z)): 0.0370 / 0.0472 Acc: 100.0000 (99.4529)\n",
      "[12/25][39/96] Loss_D: -1.4595 (-1.3093) Loss_G: 6.3010 (5.1852) D(x): 0.9646 D(G(z)): 0.0931 / 0.0044 Acc: 100.0000 (99.4534)\n",
      "[12/25][40/96] Loss_D: -1.2792 (-1.3093) Loss_G: 1.0591 (5.1818) D(x): 0.8470 D(G(z)): 0.0054 / 0.3266 Acc: 100.0000 (99.4538)\n",
      "[12/25][41/96] Loss_D: -1.3317 (-1.3093) Loss_G: 0.9011 (5.1782) D(x): 0.9910 D(G(z)): 0.0181 / 0.3679 Acc: 100.0000 (99.4543)\n",
      "[12/25][42/96] Loss_D: -1.2944 (-1.3093) Loss_G: 8.0191 (5.1806) D(x): 0.9965 D(G(z)): 0.1858 / 0.0005 Acc: 100.0000 (99.4548)\n",
      "[12/25][43/96] Loss_D: -1.3898 (-1.3094) Loss_G: 7.0651 (5.1821) D(x): 0.8926 D(G(z)): 0.0009 / 0.0019 Acc: 100.0000 (99.4552)\n",
      "[12/25][44/96] Loss_D: -1.2654 (-1.3093) Loss_G: 1.1285 (5.1788) D(x): 0.8438 D(G(z)): 0.0058 / 0.2886 Acc: 100.0000 (99.4557)\n",
      "[12/25][45/96] Loss_D: -1.1023 (-1.3092) Loss_G: 10.4516 (5.1832) D(x): 0.9969 D(G(z)): 0.2779 / 0.0000 Acc: 96.8750 (99.4535)\n",
      "[12/25][46/96] Loss_D: -1.1486 (-1.3090) Loss_G: 5.2057 (5.1832) D(x): 0.7783 D(G(z)): 0.0002 / 0.0075 Acc: 100.0000 (99.4540)\n",
      "[12/25][47/96] Loss_D: -1.5213 (-1.3092) Loss_G: 4.6882 (5.1828) D(x): 0.9934 D(G(z)): 0.0033 / 0.0116 Acc: 100.0000 (99.4544)\n",
      "[12/25][48/96] Loss_D: -1.4577 (-1.3093) Loss_G: 4.1635 (5.1819) D(x): 0.9985 D(G(z)): 0.0930 / 0.0209 Acc: 100.0000 (99.4549)\n",
      "[12/25][49/96] Loss_D: -1.5002 (-1.3095) Loss_G: 5.9288 (5.1825) D(x): 0.9916 D(G(z)): 0.0068 / 0.0042 Acc: 100.0000 (99.4553)\n",
      "[12/25][50/96] Loss_D: -1.4222 (-1.3096) Loss_G: 5.7000 (5.1830) D(x): 0.9813 D(G(z)): 0.0115 / 0.0042 Acc: 100.0000 (99.4558)\n",
      "[12/25][51/96] Loss_D: -1.4572 (-1.3097) Loss_G: 6.3489 (5.1839) D(x): 0.9887 D(G(z)): 0.0152 / 0.0028 Acc: 100.0000 (99.4562)\n",
      "[12/25][52/96] Loss_D: -1.5617 (-1.3099) Loss_G: 5.2243 (5.1840) D(x): 0.9874 D(G(z)): 0.0035 / 0.0077 Acc: 100.0000 (99.4567)\n",
      "[12/25][53/96] Loss_D: -1.5346 (-1.3101) Loss_G: 6.3754 (5.1850) D(x): 0.9886 D(G(z)): 0.0007 / 0.0021 Acc: 98.4375 (99.4558)\n",
      "[12/25][54/96] Loss_D: -1.4254 (-1.3102) Loss_G: 6.0847 (5.1857) D(x): 0.9749 D(G(z)): 0.0011 / 0.0035 Acc: 100.0000 (99.4563)\n",
      "[12/25][55/96] Loss_D: -1.4527 (-1.3103) Loss_G: 5.1028 (5.1856) D(x): 0.9890 D(G(z)): 0.0337 / 0.0090 Acc: 98.4375 (99.4555)\n",
      "[12/25][56/96] Loss_D: -1.3393 (-1.3103) Loss_G: 5.2871 (5.1857) D(x): 0.9966 D(G(z)): 0.0446 / 0.0101 Acc: 100.0000 (99.4559)\n",
      "[12/25][57/96] Loss_D: -1.3652 (-1.3104) Loss_G: 7.9030 (5.1880) D(x): 0.9666 D(G(z)): 0.0056 / 0.0005 Acc: 100.0000 (99.4564)\n",
      "[12/25][58/96] Loss_D: -1.4885 (-1.3105) Loss_G: 5.6768 (5.1884) D(x): 0.9811 D(G(z)): 0.0070 / 0.0042 Acc: 100.0000 (99.4568)\n",
      "[12/25][59/96] Loss_D: -1.5332 (-1.3107) Loss_G: 7.5821 (5.1903) D(x): 0.9901 D(G(z)): 0.0035 / 0.0008 Acc: 100.0000 (99.4573)\n",
      "[12/25][60/96] Loss_D: -1.4084 (-1.3108) Loss_G: 4.5509 (5.1898) D(x): 0.9776 D(G(z)): 0.0053 / 0.0168 Acc: 100.0000 (99.4577)\n",
      "[12/25][61/96] Loss_D: -1.4682 (-1.3109) Loss_G: 6.4017 (5.1908) D(x): 0.9873 D(G(z)): 0.0029 / 0.0024 Acc: 98.4375 (99.4569)\n",
      "[12/25][62/96] Loss_D: -1.3832 (-1.3110) Loss_G: 6.4054 (5.1918) D(x): 0.9977 D(G(z)): 0.0051 / 0.0020 Acc: 100.0000 (99.4573)\n",
      "[12/25][63/96] Loss_D: -1.4820 (-1.3111) Loss_G: 4.6372 (5.1914) D(x): 0.9996 D(G(z)): 0.0452 / 0.0147 Acc: 100.0000 (99.4578)\n",
      "[12/25][64/96] Loss_D: -1.3797 (-1.3112) Loss_G: 6.2501 (5.1922) D(x): 0.9754 D(G(z)): 0.0006 / 0.0031 Acc: 98.4375 (99.4569)\n",
      "[12/25][65/96] Loss_D: -1.5354 (-1.3114) Loss_G: 3.8516 (5.1911) D(x): 0.9669 D(G(z)): 0.0074 / 0.0260 Acc: 100.0000 (99.4574)\n",
      "[12/25][66/96] Loss_D: -1.4461 (-1.3115) Loss_G: 3.4320 (5.1897) D(x): 0.9980 D(G(z)): 0.0428 / 0.0409 Acc: 100.0000 (99.4578)\n",
      "[12/25][67/96] Loss_D: -1.4365 (-1.3116) Loss_G: 3.8516 (5.1886) D(x): 0.9857 D(G(z)): 0.0168 / 0.0262 Acc: 100.0000 (99.4582)\n",
      "[12/25][68/96] Loss_D: -1.4238 (-1.3117) Loss_G: 4.7225 (5.1882) D(x): 0.9812 D(G(z)): 0.0091 / 0.0153 Acc: 100.0000 (99.4587)\n",
      "[12/25][69/96] Loss_D: -1.3494 (-1.3117) Loss_G: 7.1517 (5.1898) D(x): 0.9884 D(G(z)): 0.0278 / 0.0012 Acc: 100.0000 (99.4591)\n",
      "[12/25][70/96] Loss_D: -1.4833 (-1.3118) Loss_G: 5.8834 (5.1904) D(x): 0.9441 D(G(z)): 0.0013 / 0.0046 Acc: 100.0000 (99.4596)\n",
      "[12/25][71/96] Loss_D: -1.5870 (-1.3121) Loss_G: 3.8692 (5.1893) D(x): 0.9985 D(G(z)): 0.0053 / 0.0254 Acc: 100.0000 (99.4600)\n",
      "[12/25][72/96] Loss_D: -1.3744 (-1.3121) Loss_G: 7.1637 (5.1909) D(x): 0.9979 D(G(z)): 0.0285 / 0.0012 Acc: 100.0000 (99.4605)\n",
      "[12/25][73/96] Loss_D: -1.4089 (-1.3122) Loss_G: 7.4642 (5.1928) D(x): 0.9945 D(G(z)): 0.0716 / 0.0008 Acc: 100.0000 (99.4609)\n",
      "[12/25][74/96] Loss_D: -1.5286 (-1.3124) Loss_G: 4.6779 (5.1923) D(x): 0.9456 D(G(z)): 0.0007 / 0.0122 Acc: 98.4375 (99.4601)\n",
      "[12/25][75/96] Loss_D: -1.3432 (-1.3124) Loss_G: 6.5351 (5.1934) D(x): 0.9721 D(G(z)): 0.0018 / 0.0021 Acc: 100.0000 (99.4605)\n",
      "[12/25][76/96] Loss_D: -1.4946 (-1.3125) Loss_G: 4.6319 (5.1930) D(x): 0.9961 D(G(z)): 0.0014 / 0.0113 Acc: 100.0000 (99.4609)\n",
      "[12/25][77/96] Loss_D: -1.4207 (-1.3126) Loss_G: 3.9295 (5.1920) D(x): 0.9881 D(G(z)): 0.0045 / 0.0275 Acc: 98.4375 (99.4601)\n",
      "[12/25][78/96] Loss_D: -1.3105 (-1.3126) Loss_G: 5.6926 (5.1924) D(x): 0.9987 D(G(z)): 0.0860 / 0.0071 Acc: 98.4375 (99.4593)\n",
      "[12/25][79/96] Loss_D: -1.4746 (-1.3128) Loss_G: 6.2628 (5.1932) D(x): 0.9782 D(G(z)): 0.0022 / 0.0033 Acc: 100.0000 (99.4597)\n",
      "[12/25][80/96] Loss_D: -1.6717 (-1.3130) Loss_G: 8.9039 (5.1962) D(x): 0.9863 D(G(z)): 0.0015 / 0.0001 Acc: 100.0000 (99.4602)\n",
      "[12/25][81/96] Loss_D: -1.5055 (-1.3132) Loss_G: 7.8149 (5.1984) D(x): 0.9755 D(G(z)): 0.0006 / 0.0008 Acc: 100.0000 (99.4606)\n",
      "[12/25][82/96] Loss_D: -1.3655 (-1.3132) Loss_G: 7.5214 (5.2002) D(x): 0.9918 D(G(z)): 0.0012 / 0.0009 Acc: 98.4375 (99.4598)\n",
      "[12/25][83/96] Loss_D: -1.4467 (-1.3133) Loss_G: 7.0565 (5.2017) D(x): 0.9914 D(G(z)): 0.0130 / 0.0012 Acc: 96.8750 (99.4577)\n",
      "[12/25][84/96] Loss_D: -1.5289 (-1.3135) Loss_G: 4.9501 (5.2015) D(x): 0.9851 D(G(z)): 0.0027 / 0.0125 Acc: 100.0000 (99.4581)\n",
      "[12/25][85/96] Loss_D: -1.4655 (-1.3136) Loss_G: 5.8863 (5.2021) D(x): 0.9716 D(G(z)): 0.0048 / 0.0060 Acc: 100.0000 (99.4586)\n",
      "[12/25][86/96] Loss_D: -1.5243 (-1.3138) Loss_G: 4.3497 (5.2014) D(x): 0.9992 D(G(z)): 0.0060 / 0.0192 Acc: 100.0000 (99.4590)\n",
      "[12/25][87/96] Loss_D: -1.5204 (-1.3140) Loss_G: 3.4240 (5.2000) D(x): 0.9949 D(G(z)): 0.0351 / 0.0357 Acc: 100.0000 (99.4594)\n",
      "[12/25][88/96] Loss_D: -1.4785 (-1.3141) Loss_G: 4.6818 (5.1996) D(x): 0.9885 D(G(z)): 0.0094 / 0.0143 Acc: 100.0000 (99.4599)\n",
      "[12/25][89/96] Loss_D: -1.5287 (-1.3143) Loss_G: 7.1387 (5.2011) D(x): 0.9987 D(G(z)): 0.0013 / 0.0014 Acc: 100.0000 (99.4603)\n",
      "[12/25][90/96] Loss_D: -1.4474 (-1.3144) Loss_G: 3.0682 (5.1994) D(x): 0.9443 D(G(z)): 0.0059 / 0.0511 Acc: 98.4375 (99.4595)\n",
      "[12/25][91/96] Loss_D: -1.4912 (-1.3145) Loss_G: 2.4369 (5.1972) D(x): 0.9943 D(G(z)): 0.0182 / 0.0896 Acc: 100.0000 (99.4599)\n",
      "[12/25][92/96] Loss_D: -1.4633 (-1.3147) Loss_G: 6.2261 (5.1980) D(x): 0.9969 D(G(z)): 0.0439 / 0.0033 Acc: 100.0000 (99.4603)\n",
      "[12/25][93/96] Loss_D: -1.4349 (-1.3148) Loss_G: 8.0375 (5.2003) D(x): 0.9860 D(G(z)): 0.0024 / 0.0006 Acc: 100.0000 (99.4608)\n",
      "[12/25][94/96] Loss_D: -1.4404 (-1.3149) Loss_G: 4.8178 (5.2000) D(x): 0.9938 D(G(z)): 0.0064 / 0.0107 Acc: 98.4375 (99.4600)\n",
      "[12/25][95/96] Loss_D: -1.4390 (-1.3150) Loss_G: 7.8883 (5.2021) D(x): 0.9865 D(G(z)): 0.0002 / 0.0008 Acc: 98.4375 (99.4591)\n",
      "[13/25][0/96] Loss_D: -1.3827 (-1.3150) Loss_G: 6.8081 (5.2034) D(x): 0.9571 D(G(z)): 0.0053 / 0.0016 Acc: 98.4375 (99.4583)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[13/25][1/96] Loss_D: -1.5105 (-1.3152) Loss_G: 5.1523 (5.2034) D(x): 0.9981 D(G(z)): 0.0032 / 0.0099 Acc: 100.0000 (99.4587)\n",
      "[13/25][2/96] Loss_D: -1.5402 (-1.3153) Loss_G: 3.3677 (5.2019) D(x): 0.9990 D(G(z)): 0.0056 / 0.0438 Acc: 100.0000 (99.4592)\n",
      "[13/25][3/96] Loss_D: -1.5544 (-1.3155) Loss_G: 7.7638 (5.2040) D(x): 0.9994 D(G(z)): 0.0752 / 0.0007 Acc: 98.4375 (99.4584)\n",
      "[13/25][4/96] Loss_D: -1.4037 (-1.3156) Loss_G: 3.7220 (5.2028) D(x): 0.9774 D(G(z)): 0.0100 / 0.0366 Acc: 100.0000 (99.4588)\n",
      "[13/25][5/96] Loss_D: -1.5174 (-1.3158) Loss_G: 3.6279 (5.2015) D(x): 0.9615 D(G(z)): 0.0017 / 0.0249 Acc: 100.0000 (99.4592)\n",
      "[13/25][6/96] Loss_D: -1.4635 (-1.3159) Loss_G: 6.4586 (5.2025) D(x): 0.9974 D(G(z)): 0.0026 / 0.0032 Acc: 100.0000 (99.4597)\n",
      "[13/25][7/96] Loss_D: -1.4102 (-1.3160) Loss_G: 6.7879 (5.2038) D(x): 0.9959 D(G(z)): 0.0074 / 0.0019 Acc: 100.0000 (99.4601)\n",
      "[13/25][8/96] Loss_D: -1.4479 (-1.3161) Loss_G: 5.8595 (5.2043) D(x): 0.9978 D(G(z)): 0.0288 / 0.0036 Acc: 100.0000 (99.4605)\n",
      "[13/25][9/96] Loss_D: -1.5520 (-1.3163) Loss_G: 6.7733 (5.2056) D(x): 0.9896 D(G(z)): 0.0142 / 0.0026 Acc: 100.0000 (99.4609)\n",
      "[13/25][10/96] Loss_D: -1.4344 (-1.3163) Loss_G: 4.2158 (5.2048) D(x): 0.9679 D(G(z)): 0.0009 / 0.0247 Acc: 100.0000 (99.4614)\n",
      "[13/25][11/96] Loss_D: -1.4405 (-1.3164) Loss_G: 5.8358 (5.2053) D(x): 0.9986 D(G(z)): 0.0626 / 0.0044 Acc: 100.0000 (99.4618)\n",
      "[13/25][12/96] Loss_D: -1.4049 (-1.3165) Loss_G: 6.0578 (5.2059) D(x): 0.9350 D(G(z)): 0.0044 / 0.0041 Acc: 100.0000 (99.4622)\n",
      "[13/25][13/96] Loss_D: -1.4558 (-1.3166) Loss_G: 4.7541 (5.2056) D(x): 0.9751 D(G(z)): 0.0020 / 0.0101 Acc: 98.4375 (99.4614)\n",
      "[13/25][14/96] Loss_D: -1.4892 (-1.3168) Loss_G: 7.0975 (5.2071) D(x): 0.9960 D(G(z)): 0.0493 / 0.0010 Acc: 96.8750 (99.4594)\n",
      "[13/25][15/96] Loss_D: -1.3552 (-1.3168) Loss_G: 6.3081 (5.2080) D(x): 0.9834 D(G(z)): 0.0018 / 0.0032 Acc: 98.4375 (99.4586)\n",
      "[13/25][16/96] Loss_D: -1.5718 (-1.3170) Loss_G: 5.8188 (5.2084) D(x): 0.9947 D(G(z)): 0.0010 / 0.0036 Acc: 96.8750 (99.4565)\n",
      "[13/25][17/96] Loss_D: -1.4736 (-1.3171) Loss_G: 3.2604 (5.2069) D(x): 0.9873 D(G(z)): 0.0130 / 0.0495 Acc: 100.0000 (99.4570)\n",
      "[13/25][18/96] Loss_D: -1.5244 (-1.3173) Loss_G: 5.3313 (5.2070) D(x): 0.9935 D(G(z)): 0.0003 / 0.0090 Acc: 98.4375 (99.4561)\n",
      "[13/25][19/96] Loss_D: -1.3433 (-1.3173) Loss_G: 6.0079 (5.2076) D(x): 0.9974 D(G(z)): 0.0414 / 0.0046 Acc: 96.8750 (99.4541)\n",
      "[13/25][20/96] Loss_D: -1.4356 (-1.3174) Loss_G: 6.8193 (5.2089) D(x): 0.9538 D(G(z)): 0.0262 / 0.0032 Acc: 100.0000 (99.4545)\n",
      "[13/25][21/96] Loss_D: -1.4815 (-1.3175) Loss_G: 1.8662 (5.2063) D(x): 0.9099 D(G(z)): 0.0089 / 0.1641 Acc: 100.0000 (99.4550)\n",
      "[13/25][22/96] Loss_D: -1.4652 (-1.3176) Loss_G: 5.4980 (5.2065) D(x): 0.9948 D(G(z)): 0.0554 / 0.0046 Acc: 100.0000 (99.4554)\n",
      "[13/25][23/96] Loss_D: -1.4482 (-1.3177) Loss_G: 7.4359 (5.2082) D(x): 0.9979 D(G(z)): 0.0028 / 0.0012 Acc: 98.4375 (99.4546)\n",
      "[13/25][24/96] Loss_D: -1.5144 (-1.3179) Loss_G: 7.3058 (5.2099) D(x): 0.9980 D(G(z)): 0.0288 / 0.0009 Acc: 100.0000 (99.4550)\n",
      "[13/25][25/96] Loss_D: -1.4675 (-1.3180) Loss_G: 7.3667 (5.2116) D(x): 0.9895 D(G(z)): 0.0205 / 0.0010 Acc: 98.4375 (99.4542)\n",
      "[13/25][26/96] Loss_D: -1.1792 (-1.3179) Loss_G: 1.3920 (5.2086) D(x): 0.8199 D(G(z)): 0.0016 / 0.2557 Acc: 98.4375 (99.4534)\n",
      "[13/25][27/96] Loss_D: -0.5138 (-1.3173) Loss_G: 12.0297 (5.2139) D(x): 1.0000 D(G(z)): 0.4689 / 0.0000 Acc: 100.0000 (99.4539)\n",
      "[13/25][28/96] Loss_D: 0.6056 (-1.3158) Loss_G: -0.4665 (5.2095) D(x): 0.1896 D(G(z)): 0.0000 / 0.9560 Acc: 100.0000 (99.4543)\n",
      "[13/25][29/96] Loss_D: 2.2962 (-1.3129) Loss_G: 11.3316 (5.2143) D(x): 1.0000 D(G(z)): 0.9502 / 0.0000 Acc: 100.0000 (99.4547)\n",
      "[13/25][30/96] Loss_D: -1.2136 (-1.3129) Loss_G: 12.7763 (5.2202) D(x): 0.8466 D(G(z)): 0.0001 / 0.0000 Acc: 98.4375 (99.4539)\n",
      "[13/25][31/96] Loss_D: 0.4925 (-1.3115) Loss_G: -0.5018 (5.2157) D(x): 0.2314 D(G(z)): 0.0001 / 0.9019 Acc: 100.0000 (99.4543)\n",
      "[13/25][32/96] Loss_D: 2.0206 (-1.3089) Loss_G: 0.8266 (5.2123) D(x): 0.9970 D(G(z)): 0.9396 / 0.4298 Acc: 100.0000 (99.4548)\n",
      "[13/25][33/96] Loss_D: -0.8280 (-1.3085) Loss_G: 3.3400 (5.2108) D(x): 0.8997 D(G(z)): 0.3506 / 0.0734 Acc: 100.0000 (99.4552)\n",
      "[13/25][34/96] Loss_D: -0.8266 (-1.3081) Loss_G: 3.6961 (5.2097) D(x): 0.7325 D(G(z)): 0.0984 / 0.0372 Acc: 100.0000 (99.4556)\n",
      "[13/25][35/96] Loss_D: -1.2507 (-1.3081) Loss_G: 2.9545 (5.2079) D(x): 0.8841 D(G(z)): 0.0827 / 0.0895 Acc: 100.0000 (99.4560)\n",
      "[13/25][36/96] Loss_D: -1.2845 (-1.3080) Loss_G: 2.7826 (5.2060) D(x): 0.9536 D(G(z)): 0.1670 / 0.1167 Acc: 100.0000 (99.4565)\n",
      "[13/25][37/96] Loss_D: -1.4753 (-1.3082) Loss_G: 3.5628 (5.2047) D(x): 0.9873 D(G(z)): 0.0739 / 0.0736 Acc: 100.0000 (99.4569)\n",
      "[13/25][38/96] Loss_D: -1.4703 (-1.3083) Loss_G: 4.8096 (5.2044) D(x): 0.9960 D(G(z)): 0.0233 / 0.0392 Acc: 100.0000 (99.4573)\n",
      "[13/25][39/96] Loss_D: -1.3826 (-1.3084) Loss_G: 4.8808 (5.2042) D(x): 0.9377 D(G(z)): 0.0160 / 0.0314 Acc: 100.0000 (99.4577)\n",
      "[13/25][40/96] Loss_D: -1.4109 (-1.3084) Loss_G: 5.2352 (5.2042) D(x): 0.9687 D(G(z)): 0.0310 / 0.0212 Acc: 100.0000 (99.4582)\n",
      "[13/25][41/96] Loss_D: -1.4155 (-1.3085) Loss_G: 4.8527 (5.2039) D(x): 0.9845 D(G(z)): 0.0330 / 0.0298 Acc: 98.4375 (99.4574)\n",
      "[13/25][42/96] Loss_D: -1.4063 (-1.3086) Loss_G: 4.0707 (5.2030) D(x): 0.9977 D(G(z)): 0.0363 / 0.0567 Acc: 100.0000 (99.4578)\n",
      "[13/25][43/96] Loss_D: -1.4202 (-1.3087) Loss_G: 5.9781 (5.2036) D(x): 0.9753 D(G(z)): 0.0176 / 0.0201 Acc: 98.4375 (99.4570)\n",
      "[13/25][44/96] Loss_D: -1.4656 (-1.3088) Loss_G: 3.8985 (5.2026) D(x): 0.9812 D(G(z)): 0.0334 / 0.0441 Acc: 98.4375 (99.4562)\n",
      "[13/25][45/96] Loss_D: -1.3573 (-1.3088) Loss_G: 4.0710 (5.2018) D(x): 0.9424 D(G(z)): 0.0132 / 0.0344 Acc: 100.0000 (99.4566)\n",
      "[13/25][46/96] Loss_D: -1.3883 (-1.3089) Loss_G: 4.4507 (5.2012) D(x): 0.9595 D(G(z)): 0.0316 / 0.0274 Acc: 100.0000 (99.4570)\n",
      "[13/25][47/96] Loss_D: -1.4456 (-1.3090) Loss_G: 3.0122 (5.1995) D(x): 0.9950 D(G(z)): 0.0367 / 0.0798 Acc: 100.0000 (99.4575)\n",
      "[13/25][48/96] Loss_D: -1.3527 (-1.3090) Loss_G: 6.8564 (5.2008) D(x): 0.9933 D(G(z)): 0.1023 / 0.0033 Acc: 100.0000 (99.4579)\n",
      "[13/25][49/96] Loss_D: -1.3840 (-1.3091) Loss_G: 6.4750 (5.2018) D(x): 0.9537 D(G(z)): 0.0364 / 0.0028 Acc: 100.0000 (99.4583)\n",
      "[13/25][50/96] Loss_D: -1.3600 (-1.3091) Loss_G: 5.3158 (5.2018) D(x): 0.8758 D(G(z)): 0.0181 / 0.0104 Acc: 100.0000 (99.4587)\n",
      "[13/25][51/96] Loss_D: -1.4235 (-1.3092) Loss_G: 3.7506 (5.2007) D(x): 0.9563 D(G(z)): 0.0132 / 0.0389 Acc: 100.0000 (99.4591)\n",
      "[13/25][52/96] Loss_D: -1.3978 (-1.3093) Loss_G: 3.5941 (5.1995) D(x): 0.9841 D(G(z)): 0.0334 / 0.0332 Acc: 96.8750 (99.4571)\n",
      "[13/25][53/96] Loss_D: -1.4594 (-1.3094) Loss_G: 3.7899 (5.1984) D(x): 0.9966 D(G(z)): 0.0335 / 0.0402 Acc: 100.0000 (99.4576)\n",
      "[13/25][54/96] Loss_D: -1.4129 (-1.3095) Loss_G: 6.4629 (5.1994) D(x): 0.9931 D(G(z)): 0.0311 / 0.0020 Acc: 100.0000 (99.4580)\n",
      "[13/25][55/96] Loss_D: -1.4431 (-1.3096) Loss_G: 3.0612 (5.1977) D(x): 0.9646 D(G(z)): 0.0161 / 0.0606 Acc: 98.4375 (99.4572)\n",
      "[13/25][56/96] Loss_D: -1.4544 (-1.3097) Loss_G: 4.6123 (5.1973) D(x): 0.9749 D(G(z)): 0.0420 / 0.0130 Acc: 100.0000 (99.4576)\n",
      "[13/25][57/96] Loss_D: -1.3533 (-1.3097) Loss_G: 5.0485 (5.1972) D(x): 0.9756 D(G(z)): 0.0112 / 0.0122 Acc: 100.0000 (99.4580)\n",
      "[13/25][58/96] Loss_D: -1.4352 (-1.3098) Loss_G: 3.7133 (5.1960) D(x): 0.9894 D(G(z)): 0.0628 / 0.0328 Acc: 100.0000 (99.4584)\n",
      "[13/25][59/96] Loss_D: -1.5195 (-1.3100) Loss_G: 4.9763 (5.1959) D(x): 0.9797 D(G(z)): 0.0062 / 0.0096 Acc: 100.0000 (99.4589)\n",
      "[13/25][60/96] Loss_D: -1.3925 (-1.3101) Loss_G: 4.8581 (5.1956) D(x): 0.9045 D(G(z)): 0.0151 / 0.0130 Acc: 100.0000 (99.4593)\n",
      "[13/25][61/96] Loss_D: -1.3435 (-1.3101) Loss_G: 3.6662 (5.1944) D(x): 0.9939 D(G(z)): 0.0514 / 0.0421 Acc: 98.4375 (99.4585)\n",
      "[13/25][62/96] Loss_D: -1.2445 (-1.3100) Loss_G: 5.7866 (5.1949) D(x): 0.9641 D(G(z)): 0.0575 / 0.0054 Acc: 98.4375 (99.4577)\n",
      "[13/25][63/96] Loss_D: -1.3293 (-1.3100) Loss_G: 3.8803 (5.1939) D(x): 0.9468 D(G(z)): 0.0288 / 0.0304 Acc: 98.4375 (99.4569)\n",
      "[13/25][64/96] Loss_D: -1.2701 (-1.3100) Loss_G: 5.2426 (5.1939) D(x): 0.8794 D(G(z)): 0.1021 / 0.0088 Acc: 100.0000 (99.4573)\n",
      "[13/25][65/96] Loss_D: -1.5361 (-1.3102) Loss_G: 5.4157 (5.1941) D(x): 0.9868 D(G(z)): 0.0813 / 0.0059 Acc: 100.0000 (99.4578)\n",
      "[13/25][66/96] Loss_D: -1.3555 (-1.3102) Loss_G: 1.9744 (5.1917) D(x): 0.8284 D(G(z)): 0.0040 / 0.1649 Acc: 100.0000 (99.4582)\n",
      "[13/25][67/96] Loss_D: -1.3069 (-1.3102) Loss_G: 3.2881 (5.1902) D(x): 0.9854 D(G(z)): 0.1160 / 0.0531 Acc: 100.0000 (99.4586)\n",
      "[13/25][68/96] Loss_D: -1.5083 (-1.3104) Loss_G: 4.7390 (5.1899) D(x): 0.9938 D(G(z)): 0.0163 / 0.0128 Acc: 100.0000 (99.4590)\n",
      "[13/25][69/96] Loss_D: -1.4374 (-1.3105) Loss_G: 5.3228 (5.1900) D(x): 0.9944 D(G(z)): 0.0100 / 0.0062 Acc: 100.0000 (99.4594)\n",
      "[13/25][70/96] Loss_D: -1.4908 (-1.3106) Loss_G: 5.3349 (5.1901) D(x): 0.9873 D(G(z)): 0.0403 / 0.0082 Acc: 100.0000 (99.4598)\n",
      "[13/25][71/96] Loss_D: -1.4407 (-1.3107) Loss_G: 6.0623 (5.1907) D(x): 0.9762 D(G(z)): 0.0031 / 0.0046 Acc: 100.0000 (99.4602)\n",
      "[13/25][72/96] Loss_D: -1.4871 (-1.3108) Loss_G: 5.6026 (5.1910) D(x): 0.9748 D(G(z)): 0.0017 / 0.0062 Acc: 100.0000 (99.4606)\n",
      "[13/25][73/96] Loss_D: -1.4375 (-1.3109) Loss_G: 6.3169 (5.1919) D(x): 0.9948 D(G(z)): 0.0381 / 0.0031 Acc: 100.0000 (99.4610)\n",
      "[13/25][74/96] Loss_D: -1.3475 (-1.3110) Loss_G: 2.6341 (5.1900) D(x): 0.8957 D(G(z)): 0.0118 / 0.0982 Acc: 96.8750 (99.4591)\n",
      "[13/25][75/96] Loss_D: -1.4075 (-1.3110) Loss_G: 5.1084 (5.1899) D(x): 0.9866 D(G(z)): 0.0944 / 0.0096 Acc: 100.0000 (99.4595)\n",
      "[13/25][76/96] Loss_D: -1.3538 (-1.3111) Loss_G: 4.9074 (5.1897) D(x): 0.8987 D(G(z)): 0.0283 / 0.0118 Acc: 100.0000 (99.4599)\n",
      "[13/25][77/96] Loss_D: -1.3728 (-1.3111) Loss_G: 5.8588 (5.1902) D(x): 0.9921 D(G(z)): 0.0095 / 0.0043 Acc: 100.0000 (99.4603)\n",
      "[13/25][78/96] Loss_D: -1.4901 (-1.3112) Loss_G: 5.2992 (5.1903) D(x): 0.9975 D(G(z)): 0.0073 / 0.0072 Acc: 100.0000 (99.4607)\n",
      "[13/25][79/96] Loss_D: -1.2353 (-1.3112) Loss_G: 4.6515 (5.1899) D(x): 0.9856 D(G(z)): 0.0836 / 0.0155 Acc: 98.4375 (99.4599)\n",
      "[13/25][80/96] Loss_D: -1.5296 (-1.3113) Loss_G: 6.9571 (5.1912) D(x): 0.9614 D(G(z)): 0.0038 / 0.0011 Acc: 98.4375 (99.4592)\n",
      "[13/25][81/96] Loss_D: -1.4976 (-1.3115) Loss_G: 4.3403 (5.1906) D(x): 0.9462 D(G(z)): 0.0014 / 0.0181 Acc: 98.4375 (99.4584)\n",
      "[13/25][82/96] Loss_D: -1.4144 (-1.3116) Loss_G: 3.5226 (5.1893) D(x): 0.9897 D(G(z)): 0.0122 / 0.0419 Acc: 100.0000 (99.4588)\n",
      "[13/25][83/96] Loss_D: -1.1411 (-1.3114) Loss_G: 8.4921 (5.1918) D(x): 0.9879 D(G(z)): 0.1903 / 0.0004 Acc: 98.4375 (99.4581)\n",
      "[13/25][84/96] Loss_D: -1.1085 (-1.3113) Loss_G: 3.7573 (5.1907) D(x): 0.7330 D(G(z)): 0.0002 / 0.0347 Acc: 100.0000 (99.4585)\n",
      "[13/25][85/96] Loss_D: -1.4831 (-1.3114) Loss_G: 1.4947 (5.1879) D(x): 0.9940 D(G(z)): 0.0105 / 0.2074 Acc: 100.0000 (99.4589)\n",
      "[13/25][86/96] Loss_D: -1.0982 (-1.3113) Loss_G: 9.0562 (5.1908) D(x): 0.9984 D(G(z)): 0.2655 / 0.0001 Acc: 100.0000 (99.4593)\n",
      "[13/25][87/96] Loss_D: -1.5065 (-1.3114) Loss_G: 8.5566 (5.1934) D(x): 0.9389 D(G(z)): 0.0005 / 0.0002 Acc: 100.0000 (99.4597)\n",
      "[13/25][88/96] Loss_D: -0.3302 (-1.3107) Loss_G: -0.3742 (5.1892) D(x): 0.4482 D(G(z)): 0.0001 / 0.9110 Acc: 100.0000 (99.4601)\n",
      "[13/25][89/96] Loss_D: 3.2505 (-1.3073) Loss_G: 8.9675 (5.1920) D(x): 1.0000 D(G(z)): 0.9794 / 0.0002 Acc: 100.0000 (99.4605)\n",
      "[13/25][90/96] Loss_D: -1.3316 (-1.3073) Loss_G: 9.1190 (5.1950) D(x): 0.8723 D(G(z)): 0.0005 / 0.0001 Acc: 100.0000 (99.4609)\n",
      "[13/25][91/96] Loss_D: 0.6756 (-1.3058) Loss_G: 0.0299 (5.1911) D(x): 0.2095 D(G(z)): 0.0003 / 0.6537 Acc: 100.0000 (99.4613)\n",
      "[13/25][92/96] Loss_D: -0.1063 (-1.3049) Loss_G: 1.0422 (5.1880) D(x): 0.9814 D(G(z)): 0.6506 / 0.3075 Acc: 100.0000 (99.4617)\n",
      "[13/25][93/96] Loss_D: -0.9480 (-1.3046) Loss_G: 3.8651 (5.1870) D(x): 0.9206 D(G(z)): 0.3046 / 0.0296 Acc: 100.0000 (99.4621)\n",
      "[13/25][94/96] Loss_D: -0.9805 (-1.3044) Loss_G: 3.4460 (5.1857) D(x): 0.7178 D(G(z)): 0.0778 / 0.0370 Acc: 100.0000 (99.4625)\n",
      "[13/25][95/96] Loss_D: -1.2952 (-1.3044) Loss_G: 2.6591 (5.1838) D(x): 0.9491 D(G(z)): 0.0612 / 0.0898 Acc: 100.0000 (99.4629)\n",
      "[14/25][0/96] Loss_D: -1.3346 (-1.3044) Loss_G: 4.8339 (5.1836) D(x): 0.9883 D(G(z)): 0.0866 / 0.0222 Acc: 96.8750 (99.4610)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[14/25][1/96] Loss_D: -1.4438 (-1.3045) Loss_G: 6.3380 (5.1844) D(x): 0.9624 D(G(z)): 0.0292 / 0.0036 Acc: 96.8750 (99.4590)\n",
      "[14/25][2/96] Loss_D: -1.5002 (-1.3047) Loss_G: 6.9825 (5.1858) D(x): 0.9931 D(G(z)): 0.0083 / 0.0017 Acc: 100.0000 (99.4594)\n",
      "[14/25][3/96] Loss_D: -1.5531 (-1.3048) Loss_G: 5.5447 (5.1860) D(x): 0.9939 D(G(z)): 0.0030 / 0.0047 Acc: 100.0000 (99.4598)\n",
      "[14/25][4/96] Loss_D: -1.5322 (-1.3050) Loss_G: 5.4349 (5.1862) D(x): 0.9839 D(G(z)): 0.0130 / 0.0057 Acc: 100.0000 (99.4602)\n",
      "[14/25][5/96] Loss_D: -1.4522 (-1.3051) Loss_G: 5.2599 (5.1863) D(x): 0.9919 D(G(z)): 0.0081 / 0.0095 Acc: 98.4375 (99.4595)\n",
      "[14/25][6/96] Loss_D: -1.5061 (-1.3053) Loss_G: 5.2100 (5.1863) D(x): 0.9946 D(G(z)): 0.0040 / 0.0103 Acc: 98.4375 (99.4587)\n",
      "[14/25][7/96] Loss_D: -1.4873 (-1.3054) Loss_G: 5.9265 (5.1868) D(x): 0.9817 D(G(z)): 0.0176 / 0.0042 Acc: 100.0000 (99.4591)\n",
      "[14/25][8/96] Loss_D: -1.4236 (-1.3055) Loss_G: 4.8400 (5.1866) D(x): 0.9965 D(G(z)): 0.0103 / 0.0131 Acc: 100.0000 (99.4595)\n",
      "[14/25][9/96] Loss_D: -1.5597 (-1.3057) Loss_G: 4.8337 (5.1863) D(x): 0.9963 D(G(z)): 0.0144 / 0.0142 Acc: 100.0000 (99.4599)\n",
      "[14/25][10/96] Loss_D: -1.4035 (-1.3058) Loss_G: 2.9114 (5.1846) D(x): 0.9894 D(G(z)): 0.0220 / 0.0748 Acc: 100.0000 (99.4603)\n",
      "[14/25][11/96] Loss_D: -1.5308 (-1.3059) Loss_G: 4.9385 (5.1845) D(x): 0.9967 D(G(z)): 0.0111 / 0.0141 Acc: 100.0000 (99.4607)\n",
      "[14/25][12/96] Loss_D: -1.5888 (-1.3061) Loss_G: 4.2721 (5.1838) D(x): 0.9850 D(G(z)): 0.0052 / 0.0254 Acc: 98.4375 (99.4600)\n",
      "[14/25][13/96] Loss_D: -1.4036 (-1.3062) Loss_G: 5.5545 (5.1841) D(x): 0.9887 D(G(z)): 0.0065 / 0.0065 Acc: 100.0000 (99.4604)\n",
      "[14/25][14/96] Loss_D: -1.4871 (-1.3063) Loss_G: 4.7539 (5.1838) D(x): 0.9713 D(G(z)): 0.0134 / 0.0128 Acc: 100.0000 (99.4608)\n",
      "[14/25][15/96] Loss_D: -1.3944 (-1.3064) Loss_G: 3.3612 (5.1824) D(x): 0.9852 D(G(z)): 0.0256 / 0.0701 Acc: 98.4375 (99.4600)\n",
      "[14/25][16/96] Loss_D: -1.3442 (-1.3064) Loss_G: 4.1657 (5.1817) D(x): 0.9844 D(G(z)): 0.0673 / 0.0251 Acc: 98.4375 (99.4593)\n",
      "[14/25][17/96] Loss_D: -1.3705 (-1.3065) Loss_G: 4.8227 (5.1814) D(x): 0.9429 D(G(z)): 0.0410 / 0.0153 Acc: 98.4375 (99.4585)\n",
      "[14/25][18/96] Loss_D: -1.3204 (-1.3065) Loss_G: 3.7982 (5.1804) D(x): 0.9502 D(G(z)): 0.0439 / 0.0387 Acc: 100.0000 (99.4589)\n",
      "[14/25][19/96] Loss_D: -1.5512 (-1.3067) Loss_G: 4.3655 (5.1798) D(x): 0.9493 D(G(z)): 0.0016 / 0.0171 Acc: 100.0000 (99.4593)\n",
      "[14/25][20/96] Loss_D: -1.4546 (-1.3068) Loss_G: 5.1706 (5.1798) D(x): 0.9836 D(G(z)): 0.0264 / 0.0088 Acc: 100.0000 (99.4597)\n",
      "[14/25][21/96] Loss_D: -1.4018 (-1.3068) Loss_G: 5.6456 (5.1801) D(x): 0.9885 D(G(z)): 0.1188 / 0.0060 Acc: 100.0000 (99.4601)\n",
      "[14/25][22/96] Loss_D: -1.4328 (-1.3069) Loss_G: 5.9447 (5.1807) D(x): 0.9758 D(G(z)): 0.0095 / 0.0048 Acc: 98.4375 (99.4594)\n",
      "[14/25][23/96] Loss_D: -1.3240 (-1.3069) Loss_G: 4.3448 (5.1801) D(x): 0.8262 D(G(z)): 0.0024 / 0.0191 Acc: 100.0000 (99.4597)\n",
      "[14/25][24/96] Loss_D: -1.3549 (-1.3070) Loss_G: 2.0286 (5.1778) D(x): 0.9703 D(G(z)): 0.0185 / 0.1924 Acc: 98.4375 (99.4590)\n",
      "[14/25][25/96] Loss_D: -1.2193 (-1.3069) Loss_G: 6.3366 (5.1786) D(x): 0.9592 D(G(z)): 0.1899 / 0.0030 Acc: 100.0000 (99.4594)\n",
      "[14/25][26/96] Loss_D: -1.4296 (-1.3070) Loss_G: 5.1299 (5.1786) D(x): 0.9511 D(G(z)): 0.0013 / 0.0073 Acc: 100.0000 (99.4598)\n",
      "[14/25][27/96] Loss_D: -1.4591 (-1.3071) Loss_G: 5.4820 (5.1788) D(x): 0.9143 D(G(z)): 0.0053 / 0.0051 Acc: 100.0000 (99.4602)\n",
      "[14/25][28/96] Loss_D: -1.5238 (-1.3073) Loss_G: 2.8936 (5.1771) D(x): 0.9874 D(G(z)): 0.0260 / 0.0610 Acc: 98.4375 (99.4594)\n",
      "[14/25][29/96] Loss_D: -1.3837 (-1.3073) Loss_G: 2.8195 (5.1754) D(x): 0.9251 D(G(z)): 0.0287 / 0.0629 Acc: 100.0000 (99.4598)\n",
      "[14/25][30/96] Loss_D: -1.4049 (-1.3074) Loss_G: 3.8604 (5.1745) D(x): 0.9924 D(G(z)): 0.0357 / 0.0400 Acc: 96.8750 (99.4580)\n",
      "[14/25][31/96] Loss_D: -1.3857 (-1.3075) Loss_G: 4.9079 (5.1743) D(x): 0.9889 D(G(z)): 0.0624 / 0.0104 Acc: 98.4375 (99.4572)\n",
      "[14/25][32/96] Loss_D: -1.4353 (-1.3075) Loss_G: 3.7733 (5.1733) D(x): 0.9874 D(G(z)): 0.0346 / 0.0409 Acc: 98.4375 (99.4565)\n",
      "[14/25][33/96] Loss_D: -1.4343 (-1.3076) Loss_G: 4.5019 (5.1728) D(x): 0.9545 D(G(z)): 0.0010 / 0.0218 Acc: 100.0000 (99.4569)\n",
      "[14/25][34/96] Loss_D: -1.5834 (-1.3078) Loss_G: 4.0176 (5.1719) D(x): 0.9576 D(G(z)): 0.0114 / 0.0222 Acc: 98.4375 (99.4561)\n",
      "[14/25][35/96] Loss_D: -1.2163 (-1.3078) Loss_G: 4.3803 (5.1714) D(x): 0.9288 D(G(z)): 0.0985 / 0.0179 Acc: 98.4375 (99.4554)\n",
      "[14/25][36/96] Loss_D: -1.3907 (-1.3078) Loss_G: 2.6968 (5.1696) D(x): 0.8950 D(G(z)): 0.0121 / 0.0896 Acc: 100.0000 (99.4558)\n",
      "[14/25][37/96] Loss_D: -1.4714 (-1.3080) Loss_G: 4.0781 (5.1688) D(x): 0.9845 D(G(z)): 0.0741 / 0.0353 Acc: 100.0000 (99.4562)\n",
      "[14/25][38/96] Loss_D: -1.3717 (-1.3080) Loss_G: 5.1200 (5.1687) D(x): 0.9702 D(G(z)): 0.0938 / 0.0082 Acc: 100.0000 (99.4566)\n",
      "[14/25][39/96] Loss_D: -1.3718 (-1.3080) Loss_G: 2.4076 (5.1667) D(x): 0.8839 D(G(z)): 0.0180 / 0.1255 Acc: 100.0000 (99.4570)\n",
      "[14/25][40/96] Loss_D: -1.5039 (-1.3082) Loss_G: 4.1443 (5.1660) D(x): 0.9740 D(G(z)): 0.0272 / 0.0228 Acc: 98.4375 (99.4562)\n",
      "[14/25][41/96] Loss_D: -1.3804 (-1.3082) Loss_G: 2.1087 (5.1638) D(x): 0.9420 D(G(z)): 0.0167 / 0.1548 Acc: 100.0000 (99.4566)\n",
      "[14/25][42/96] Loss_D: -1.4110 (-1.3083) Loss_G: 5.1473 (5.1638) D(x): 0.9944 D(G(z)): 0.1054 / 0.0076 Acc: 98.4375 (99.4559)\n",
      "[14/25][43/96] Loss_D: -1.0899 (-1.3082) Loss_G: 1.1504 (5.1609) D(x): 0.7467 D(G(z)): 0.0061 / 0.3216 Acc: 100.0000 (99.4563)\n",
      "[14/25][44/96] Loss_D: 0.4534 (-1.3069) Loss_G: 11.8217 (5.1657) D(x): 0.9995 D(G(z)): 0.7599 / 0.0000 Acc: 100.0000 (99.4567)\n",
      "[14/25][45/96] Loss_D: 3.0363 (-1.3038) Loss_G: 0.4407 (5.1623) D(x): 0.0355 D(G(z)): 0.0000 / 0.4681 Acc: 100.0000 (99.4571)\n",
      "[14/25][46/96] Loss_D: -0.4723 (-1.3032) Loss_G: 1.3655 (5.1596) D(x): 0.9958 D(G(z)): 0.5601 / 0.2913 Acc: 100.0000 (99.4574)\n",
      "[14/25][47/96] Loss_D: -1.2431 (-1.3031) Loss_G: 2.1388 (5.1574) D(x): 0.9848 D(G(z)): 0.2050 / 0.1349 Acc: 100.0000 (99.4578)\n",
      "[14/25][48/96] Loss_D: -1.2403 (-1.3031) Loss_G: 3.9925 (5.1566) D(x): 0.8875 D(G(z)): 0.0928 / 0.0320 Acc: 100.0000 (99.4582)\n",
      "[14/25][49/96] Loss_D: -1.2068 (-1.3030) Loss_G: 4.0000 (5.1557) D(x): 0.8717 D(G(z)): 0.0848 / 0.0281 Acc: 100.0000 (99.4586)\n",
      "[14/25][50/96] Loss_D: -1.1941 (-1.3029) Loss_G: 3.6215 (5.1546) D(x): 0.8932 D(G(z)): 0.0962 / 0.0609 Acc: 100.0000 (99.4590)\n",
      "[14/25][51/96] Loss_D: -1.4563 (-1.3030) Loss_G: 3.7812 (5.1536) D(x): 0.9622 D(G(z)): 0.0553 / 0.0456 Acc: 98.4375 (99.4583)\n",
      "[14/25][52/96] Loss_D: -1.2744 (-1.3030) Loss_G: 2.3756 (5.1517) D(x): 0.8837 D(G(z)): 0.0674 / 0.1281 Acc: 100.0000 (99.4587)\n",
      "[14/25][53/96] Loss_D: -1.3251 (-1.3030) Loss_G: 2.8135 (5.1500) D(x): 0.9713 D(G(z)): 0.0364 / 0.0817 Acc: 100.0000 (99.4590)\n",
      "[14/25][54/96] Loss_D: -1.3473 (-1.3031) Loss_G: 4.7904 (5.1497) D(x): 0.9967 D(G(z)): 0.0885 / 0.0125 Acc: 98.4375 (99.4583)\n",
      "[14/25][55/96] Loss_D: -1.4369 (-1.3032) Loss_G: 5.1630 (5.1497) D(x): 0.9595 D(G(z)): 0.0331 / 0.0108 Acc: 98.4375 (99.4576)\n",
      "[14/25][56/96] Loss_D: -1.4447 (-1.3033) Loss_G: 4.7378 (5.1494) D(x): 0.9850 D(G(z)): 0.0083 / 0.0143 Acc: 100.0000 (99.4580)\n",
      "[14/25][57/96] Loss_D: -1.5130 (-1.3034) Loss_G: 5.2326 (5.1495) D(x): 0.9757 D(G(z)): 0.0460 / 0.0069 Acc: 100.0000 (99.4584)\n",
      "[14/25][58/96] Loss_D: -1.2502 (-1.3034) Loss_G: 5.6557 (5.1499) D(x): 0.9227 D(G(z)): 0.0151 / 0.0068 Acc: 100.0000 (99.4587)\n",
      "[14/25][59/96] Loss_D: -1.4448 (-1.3035) Loss_G: 5.4512 (5.1501) D(x): 0.9861 D(G(z)): 0.0094 / 0.0078 Acc: 100.0000 (99.4591)\n",
      "[14/25][60/96] Loss_D: -1.5333 (-1.3036) Loss_G: 4.8020 (5.1498) D(x): 0.9905 D(G(z)): 0.0312 / 0.0092 Acc: 100.0000 (99.4595)\n",
      "[14/25][61/96] Loss_D: -1.4646 (-1.3038) Loss_G: 2.7702 (5.1481) D(x): 0.9816 D(G(z)): 0.0161 / 0.0911 Acc: 100.0000 (99.4599)\n",
      "[14/25][62/96] Loss_D: -1.5084 (-1.3039) Loss_G: 5.0489 (5.1481) D(x): 0.9869 D(G(z)): 0.0083 / 0.0090 Acc: 100.0000 (99.4603)\n",
      "[14/25][63/96] Loss_D: -1.4388 (-1.3040) Loss_G: 4.4388 (5.1476) D(x): 0.9395 D(G(z)): 0.0221 / 0.0154 Acc: 100.0000 (99.4607)\n",
      "[14/25][64/96] Loss_D: -1.3658 (-1.3040) Loss_G: 3.5913 (5.1465) D(x): 0.9906 D(G(z)): 0.0417 / 0.0403 Acc: 100.0000 (99.4611)\n",
      "[14/25][65/96] Loss_D: -1.5107 (-1.3042) Loss_G: 4.1449 (5.1457) D(x): 0.9659 D(G(z)): 0.0262 / 0.0288 Acc: 100.0000 (99.4614)\n",
      "[14/25][66/96] Loss_D: -1.5073 (-1.3043) Loss_G: 4.4674 (5.1453) D(x): 0.9915 D(G(z)): 0.0151 / 0.0136 Acc: 100.0000 (99.4618)\n",
      "[14/25][67/96] Loss_D: -1.3848 (-1.3044) Loss_G: 3.8991 (5.1444) D(x): 0.9367 D(G(z)): 0.0091 / 0.0311 Acc: 100.0000 (99.4622)\n",
      "[14/25][68/96] Loss_D: -1.4619 (-1.3045) Loss_G: 3.2973 (5.1431) D(x): 0.9944 D(G(z)): 0.0446 / 0.0464 Acc: 100.0000 (99.4626)\n",
      "[14/25][69/96] Loss_D: -1.3127 (-1.3045) Loss_G: 6.7020 (5.1442) D(x): 0.9517 D(G(z)): 0.1118 / 0.0017 Acc: 100.0000 (99.4630)\n",
      "[14/25][70/96] Loss_D: -1.3949 (-1.3046) Loss_G: 3.4884 (5.1430) D(x): 0.8239 D(G(z)): 0.0010 / 0.0386 Acc: 100.0000 (99.4633)\n",
      "[14/25][71/96] Loss_D: -1.4607 (-1.3047) Loss_G: 3.7826 (5.1420) D(x): 0.9905 D(G(z)): 0.0824 / 0.0267 Acc: 100.0000 (99.4637)\n",
      "[14/25][72/96] Loss_D: -1.3595 (-1.3047) Loss_G: 4.7494 (5.1418) D(x): 0.9721 D(G(z)): 0.0569 / 0.0124 Acc: 100.0000 (99.4641)\n",
      "[14/25][73/96] Loss_D: -1.3628 (-1.3048) Loss_G: 6.1675 (5.1425) D(x): 0.9485 D(G(z)): 0.0301 / 0.0028 Acc: 100.0000 (99.4645)\n",
      "[14/25][74/96] Loss_D: -1.3641 (-1.3048) Loss_G: 4.4260 (5.1420) D(x): 0.9928 D(G(z)): 0.0333 / 0.0334 Acc: 100.0000 (99.4649)\n",
      "[14/25][75/96] Loss_D: -1.4484 (-1.3049) Loss_G: 5.8018 (5.1425) D(x): 0.9857 D(G(z)): 0.0056 / 0.0056 Acc: 98.4375 (99.4641)\n",
      "[14/25][76/96] Loss_D: -1.3076 (-1.3049) Loss_G: 5.0445 (5.1424) D(x): 0.9676 D(G(z)): 0.0572 / 0.0114 Acc: 100.0000 (99.4645)\n",
      "[14/25][77/96] Loss_D: -1.5149 (-1.3050) Loss_G: 5.2965 (5.1425) D(x): 0.9898 D(G(z)): 0.0058 / 0.0097 Acc: 100.0000 (99.4649)\n",
      "[14/25][78/96] Loss_D: -1.3943 (-1.3051) Loss_G: 4.1171 (5.1418) D(x): 0.8869 D(G(z)): 0.0121 / 0.0267 Acc: 98.4375 (99.4642)\n",
      "[14/25][79/96] Loss_D: -1.2820 (-1.3051) Loss_G: 7.6812 (5.1436) D(x): 0.9974 D(G(z)): 0.1769 / 0.0007 Acc: 100.0000 (99.4645)\n",
      "[14/25][80/96] Loss_D: -1.2563 (-1.3051) Loss_G: 4.4972 (5.1431) D(x): 0.7980 D(G(z)): 0.0010 / 0.0164 Acc: 98.4375 (99.4638)\n",
      "[14/25][81/96] Loss_D: -1.4769 (-1.3052) Loss_G: 5.3849 (5.1433) D(x): 0.9966 D(G(z)): 0.0472 / 0.0055 Acc: 100.0000 (99.4642)\n",
      "[14/25][82/96] Loss_D: -1.4269 (-1.3053) Loss_G: 4.6156 (5.1429) D(x): 0.9745 D(G(z)): 0.0442 / 0.0190 Acc: 100.0000 (99.4646)\n",
      "[14/25][83/96] Loss_D: -1.4921 (-1.3054) Loss_G: 3.1889 (5.1415) D(x): 0.9865 D(G(z)): 0.0088 / 0.0513 Acc: 100.0000 (99.4649)\n",
      "[14/25][84/96] Loss_D: -1.5678 (-1.3056) Loss_G: 4.1220 (5.1408) D(x): 0.9962 D(G(z)): 0.0064 / 0.0192 Acc: 98.4375 (99.4642)\n",
      "[14/25][85/96] Loss_D: -1.3044 (-1.3056) Loss_G: 2.8646 (5.1392) D(x): 0.8876 D(G(z)): 0.0053 / 0.0701 Acc: 98.4375 (99.4635)\n",
      "[14/25][86/96] Loss_D: -1.3872 (-1.3056) Loss_G: 3.4012 (5.1380) D(x): 0.9948 D(G(z)): 0.0958 / 0.0463 Acc: 100.0000 (99.4639)\n",
      "[14/25][87/96] Loss_D: -1.5036 (-1.3058) Loss_G: 4.5965 (5.1376) D(x): 0.9814 D(G(z)): 0.0739 / 0.0147 Acc: 100.0000 (99.4643)\n",
      "[14/25][88/96] Loss_D: -1.4325 (-1.3059) Loss_G: 7.1995 (5.1391) D(x): 0.9775 D(G(z)): 0.0125 / 0.0011 Acc: 100.0000 (99.4646)\n",
      "[14/25][89/96] Loss_D: -1.3513 (-1.3059) Loss_G: 2.8644 (5.1375) D(x): 0.8381 D(G(z)): 0.0066 / 0.0705 Acc: 100.0000 (99.4650)\n",
      "[14/25][90/96] Loss_D: -1.5120 (-1.3060) Loss_G: 1.2987 (5.1348) D(x): 0.9903 D(G(z)): 0.0234 / 0.2552 Acc: 100.0000 (99.4654)\n",
      "[14/25][91/96] Loss_D: -1.3103 (-1.3060) Loss_G: 5.4834 (5.1351) D(x): 0.9991 D(G(z)): 0.1662 / 0.0049 Acc: 100.0000 (99.4657)\n",
      "[14/25][92/96] Loss_D: -1.5106 (-1.3062) Loss_G: 6.8764 (5.1363) D(x): 0.9867 D(G(z)): 0.0069 / 0.0013 Acc: 100.0000 (99.4661)\n",
      "[14/25][93/96] Loss_D: -1.3974 (-1.3062) Loss_G: 6.7277 (5.1374) D(x): 0.9640 D(G(z)): 0.0004 / 0.0020 Acc: 100.0000 (99.4665)\n",
      "[14/25][94/96] Loss_D: -1.5146 (-1.3064) Loss_G: 4.6579 (5.1370) D(x): 0.9862 D(G(z)): 0.0026 / 0.0135 Acc: 98.4375 (99.4658)\n",
      "[14/25][95/96] Loss_D: -1.4069 (-1.3065) Loss_G: 7.2526 (5.1385) D(x): 0.9774 D(G(z)): 0.0069 / 0.0011 Acc: 100.0000 (99.4661)\n",
      "[15/25][0/96] Loss_D: -1.3456 (-1.3065) Loss_G: 4.4370 (5.1380) D(x): 0.8995 D(G(z)): 0.0103 / 0.0142 Acc: 100.0000 (99.4665)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[15/25][1/96] Loss_D: -1.3719 (-1.3065) Loss_G: 3.7567 (5.1371) D(x): 0.9990 D(G(z)): 0.1139 / 0.0311 Acc: 98.4375 (99.4658)\n",
      "[15/25][2/96] Loss_D: -1.4904 (-1.3067) Loss_G: 7.6648 (5.1388) D(x): 0.9930 D(G(z)): 0.0025 / 0.0011 Acc: 98.4375 (99.4651)\n",
      "[15/25][3/96] Loss_D: -1.3198 (-1.3067) Loss_G: 4.0868 (5.1381) D(x): 0.9269 D(G(z)): 0.0050 / 0.0297 Acc: 98.4375 (99.4644)\n",
      "[15/25][4/96] Loss_D: -1.4110 (-1.3067) Loss_G: 5.7911 (5.1385) D(x): 0.9929 D(G(z)): 0.0181 / 0.0042 Acc: 100.0000 (99.4647)\n",
      "[15/25][5/96] Loss_D: -1.5383 (-1.3069) Loss_G: 5.8039 (5.1390) D(x): 0.9968 D(G(z)): 0.0204 / 0.0036 Acc: 100.0000 (99.4651)\n",
      "[15/25][6/96] Loss_D: -1.3914 (-1.3070) Loss_G: 3.6238 (5.1380) D(x): 0.9940 D(G(z)): 0.0236 / 0.0373 Acc: 100.0000 (99.4655)\n",
      "[15/25][7/96] Loss_D: -1.4901 (-1.3071) Loss_G: 3.3037 (5.1367) D(x): 0.9770 D(G(z)): 0.0169 / 0.0511 Acc: 100.0000 (99.4659)\n",
      "[15/25][8/96] Loss_D: -1.5747 (-1.3073) Loss_G: 5.1223 (5.1367) D(x): 0.9806 D(G(z)): 0.0112 / 0.0082 Acc: 100.0000 (99.4662)\n",
      "[15/25][9/96] Loss_D: -1.4696 (-1.3074) Loss_G: 3.7651 (5.1357) D(x): 0.9220 D(G(z)): 0.0064 / 0.0300 Acc: 98.4375 (99.4655)\n",
      "[15/25][10/96] Loss_D: -1.4698 (-1.3075) Loss_G: 3.0744 (5.1343) D(x): 0.9871 D(G(z)): 0.0533 / 0.0584 Acc: 100.0000 (99.4659)\n",
      "[15/25][11/96] Loss_D: -1.0095 (-1.3073) Loss_G: 10.4339 (5.1380) D(x): 0.9929 D(G(z)): 0.2349 / 0.0001 Acc: 100.0000 (99.4663)\n",
      "[15/25][12/96] Loss_D: -0.8933 (-1.3070) Loss_G: 0.8586 (5.1350) D(x): 0.6068 D(G(z)): 0.0004 / 0.3447 Acc: 100.0000 (99.4666)\n",
      "[15/25][13/96] Loss_D: -1.2876 (-1.3070) Loss_G: 0.8675 (5.1321) D(x): 0.9995 D(G(z)): 0.0879 / 0.3732 Acc: 98.4375 (99.4659)\n",
      "[15/25][14/96] Loss_D: -1.0712 (-1.3068) Loss_G: 6.2502 (5.1328) D(x): 0.9995 D(G(z)): 0.2586 / 0.0032 Acc: 100.0000 (99.4663)\n",
      "[15/25][15/96] Loss_D: -1.4559 (-1.3069) Loss_G: 7.7387 (5.1346) D(x): 0.9604 D(G(z)): 0.0010 / 0.0007 Acc: 100.0000 (99.4666)\n",
      "[15/25][16/96] Loss_D: -1.4469 (-1.3070) Loss_G: 7.9646 (5.1366) D(x): 0.9535 D(G(z)): 0.0002 / 0.0004 Acc: 100.0000 (99.4670)\n",
      "[15/25][17/96] Loss_D: -1.2396 (-1.3070) Loss_G: 5.0669 (5.1365) D(x): 0.8577 D(G(z)): 0.0005 / 0.0166 Acc: 100.0000 (99.4674)\n",
      "[15/25][18/96] Loss_D: -1.4022 (-1.3070) Loss_G: 2.6097 (5.1348) D(x): 0.9693 D(G(z)): 0.0169 / 0.0834 Acc: 96.8750 (99.4656)\n",
      "[15/25][19/96] Loss_D: -1.4359 (-1.3071) Loss_G: 3.6236 (5.1338) D(x): 0.9992 D(G(z)): 0.0430 / 0.0428 Acc: 100.0000 (99.4660)\n",
      "[15/25][20/96] Loss_D: -1.4882 (-1.3073) Loss_G: 3.9616 (5.1330) D(x): 0.9985 D(G(z)): 0.0098 / 0.0272 Acc: 100.0000 (99.4663)\n",
      "[15/25][21/96] Loss_D: -1.4378 (-1.3073) Loss_G: 4.4387 (5.1325) D(x): 0.9962 D(G(z)): 0.0112 / 0.0156 Acc: 100.0000 (99.4667)\n",
      "[15/25][22/96] Loss_D: -1.5012 (-1.3075) Loss_G: 5.9034 (5.1330) D(x): 0.9911 D(G(z)): 0.0759 / 0.0043 Acc: 100.0000 (99.4671)\n",
      "[15/25][23/96] Loss_D: -1.4974 (-1.3076) Loss_G: 5.9524 (5.1336) D(x): 0.9847 D(G(z)): 0.0024 / 0.0031 Acc: 100.0000 (99.4674)\n",
      "[15/25][24/96] Loss_D: -1.4730 (-1.3077) Loss_G: 4.6058 (5.1332) D(x): 0.9817 D(G(z)): 0.0080 / 0.0116 Acc: 100.0000 (99.4678)\n",
      "[15/25][25/96] Loss_D: -1.5507 (-1.3079) Loss_G: 4.8134 (5.1330) D(x): 0.9933 D(G(z)): 0.0049 / 0.0111 Acc: 100.0000 (99.4682)\n",
      "[15/25][26/96] Loss_D: -1.3434 (-1.3079) Loss_G: 3.7792 (5.1321) D(x): 0.9040 D(G(z)): 0.0021 / 0.0306 Acc: 100.0000 (99.4685)\n",
      "[15/25][27/96] Loss_D: -1.4771 (-1.3080) Loss_G: 1.4931 (5.1296) D(x): 0.9970 D(G(z)): 0.0193 / 0.2309 Acc: 98.4375 (99.4678)\n",
      "[15/25][28/96] Loss_D: -1.2957 (-1.3080) Loss_G: 5.0567 (5.1295) D(x): 0.9960 D(G(z)): 0.1623 / 0.0071 Acc: 100.0000 (99.4682)\n",
      "[15/25][29/96] Loss_D: -1.4541 (-1.3081) Loss_G: 6.4452 (5.1304) D(x): 0.9400 D(G(z)): 0.0112 / 0.0022 Acc: 100.0000 (99.4685)\n",
      "[15/25][30/96] Loss_D: -1.4889 (-1.3082) Loss_G: 6.2078 (5.1312) D(x): 0.9982 D(G(z)): 0.0091 / 0.0022 Acc: 100.0000 (99.4689)\n",
      "[15/25][31/96] Loss_D: -1.4906 (-1.3084) Loss_G: 6.1061 (5.1318) D(x): 0.9949 D(G(z)): 0.0040 / 0.0026 Acc: 100.0000 (99.4693)\n",
      "[15/25][32/96] Loss_D: -1.2422 (-1.3083) Loss_G: 2.4822 (5.1300) D(x): 0.8034 D(G(z)): 0.0011 / 0.1134 Acc: 100.0000 (99.4696)\n",
      "[15/25][33/96] Loss_D: -1.1128 (-1.3082) Loss_G: 4.8554 (5.1298) D(x): 0.9982 D(G(z)): 0.1702 / 0.0137 Acc: 100.0000 (99.4700)\n",
      "[15/25][34/96] Loss_D: -1.4412 (-1.3083) Loss_G: 8.7321 (5.1323) D(x): 0.9937 D(G(z)): 0.0197 / 0.0002 Acc: 100.0000 (99.4703)\n",
      "[15/25][35/96] Loss_D: -1.4274 (-1.3084) Loss_G: 5.4767 (5.1325) D(x): 0.9794 D(G(z)): 0.0043 / 0.0057 Acc: 100.0000 (99.4707)\n",
      "[15/25][36/96] Loss_D: -1.4421 (-1.3085) Loss_G: 6.9839 (5.1338) D(x): 0.9910 D(G(z)): 0.0019 / 0.0015 Acc: 100.0000 (99.4711)\n",
      "[15/25][37/96] Loss_D: -1.3670 (-1.3085) Loss_G: 3.5625 (5.1327) D(x): 0.9393 D(G(z)): 0.0005 / 0.0395 Acc: 98.4375 (99.4704)\n",
      "[15/25][38/96] Loss_D: -1.5424 (-1.3086) Loss_G: 3.6454 (5.1317) D(x): 0.9979 D(G(z)): 0.0023 / 0.0322 Acc: 98.4375 (99.4697)\n",
      "[15/25][39/96] Loss_D: -1.6082 (-1.3089) Loss_G: 4.4918 (5.1313) D(x): 0.9987 D(G(z)): 0.0133 / 0.0125 Acc: 100.0000 (99.4700)\n",
      "[15/25][40/96] Loss_D: -1.4103 (-1.3089) Loss_G: 3.4695 (5.1302) D(x): 0.9978 D(G(z)): 0.0092 / 0.0383 Acc: 98.4375 (99.4693)\n",
      "[15/25][41/96] Loss_D: -1.3580 (-1.3090) Loss_G: 5.8159 (5.1306) D(x): 0.9946 D(G(z)): 0.0634 / 0.0057 Acc: 100.0000 (99.4697)\n",
      "[15/25][42/96] Loss_D: -1.5192 (-1.3091) Loss_G: 8.1885 (5.1327) D(x): 0.9744 D(G(z)): 0.0008 / 0.0005 Acc: 100.0000 (99.4700)\n",
      "[15/25][43/96] Loss_D: -1.5027 (-1.3092) Loss_G: 5.7706 (5.1331) D(x): 0.9889 D(G(z)): 0.0017 / 0.0038 Acc: 100.0000 (99.4704)\n",
      "[15/25][44/96] Loss_D: -1.5546 (-1.3094) Loss_G: 8.0628 (5.1351) D(x): 0.9980 D(G(z)): 0.0206 / 0.0005 Acc: 98.4375 (99.4697)\n",
      "[15/25][45/96] Loss_D: -1.4449 (-1.3095) Loss_G: 6.8681 (5.1362) D(x): 0.9967 D(G(z)): 0.0048 / 0.0018 Acc: 98.4375 (99.4690)\n",
      "[15/25][46/96] Loss_D: -1.2774 (-1.3095) Loss_G: 5.4924 (5.1365) D(x): 0.9318 D(G(z)): 0.0044 / 0.0063 Acc: 100.0000 (99.4694)\n",
      "[15/25][47/96] Loss_D: -1.3746 (-1.3095) Loss_G: 3.9343 (5.1357) D(x): 0.9733 D(G(z)): 0.0039 / 0.0298 Acc: 96.8750 (99.4676)\n",
      "[15/25][48/96] Loss_D: -1.4292 (-1.3096) Loss_G: 3.5798 (5.1346) D(x): 0.9992 D(G(z)): 0.0217 / 0.0403 Acc: 98.4375 (99.4669)\n",
      "[15/25][49/96] Loss_D: -1.4458 (-1.3097) Loss_G: 3.6073 (5.1336) D(x): 0.9983 D(G(z)): 0.0204 / 0.0378 Acc: 100.0000 (99.4673)\n",
      "[15/25][50/96] Loss_D: -1.4658 (-1.3098) Loss_G: 5.1623 (5.1336) D(x): 0.9974 D(G(z)): 0.0978 / 0.0078 Acc: 100.0000 (99.4676)\n",
      "[15/25][51/96] Loss_D: -1.3945 (-1.3098) Loss_G: 6.7252 (5.1347) D(x): 0.8893 D(G(z)): 0.0013 / 0.0020 Acc: 100.0000 (99.4680)\n",
      "[15/25][52/96] Loss_D: -1.4578 (-1.3099) Loss_G: 5.6230 (5.1350) D(x): 0.9932 D(G(z)): 0.0331 / 0.0056 Acc: 100.0000 (99.4684)\n",
      "[15/25][53/96] Loss_D: -1.4133 (-1.3100) Loss_G: 5.7065 (5.1354) D(x): 0.9982 D(G(z)): 0.0066 / 0.0058 Acc: 100.0000 (99.4687)\n",
      "[15/25][54/96] Loss_D: -1.3848 (-1.3101) Loss_G: 4.8899 (5.1352) D(x): 0.9968 D(G(z)): 0.0725 / 0.0095 Acc: 100.0000 (99.4691)\n",
      "[15/25][55/96] Loss_D: -1.3326 (-1.3101) Loss_G: 3.9700 (5.1345) D(x): 0.9007 D(G(z)): 0.0174 / 0.0275 Acc: 98.4375 (99.4684)\n",
      "[15/25][56/96] Loss_D: -1.4911 (-1.3102) Loss_G: 4.9934 (5.1344) D(x): 0.9974 D(G(z)): 0.0060 / 0.0083 Acc: 98.4375 (99.4677)\n",
      "[15/25][57/96] Loss_D: -1.5090 (-1.3103) Loss_G: 5.5151 (5.1346) D(x): 0.9984 D(G(z)): 0.0185 / 0.0062 Acc: 98.4375 (99.4670)\n",
      "[15/25][58/96] Loss_D: -1.5226 (-1.3105) Loss_G: 6.0848 (5.1353) D(x): 0.9987 D(G(z)): 0.0213 / 0.0081 Acc: 100.0000 (99.4674)\n",
      "[15/25][59/96] Loss_D: -1.4710 (-1.3106) Loss_G: 6.2625 (5.1360) D(x): 0.9634 D(G(z)): 0.0060 / 0.0025 Acc: 100.0000 (99.4677)\n",
      "[15/25][60/96] Loss_D: -1.3941 (-1.3106) Loss_G: 3.9182 (5.1352) D(x): 0.9751 D(G(z)): 0.0310 / 0.0343 Acc: 98.4375 (99.4670)\n",
      "[15/25][61/96] Loss_D: -1.6145 (-1.3108) Loss_G: 4.0461 (5.1345) D(x): 0.9927 D(G(z)): 0.0181 / 0.0216 Acc: 100.0000 (99.4674)\n",
      "[15/25][62/96] Loss_D: -1.4680 (-1.3109) Loss_G: 4.0118 (5.1337) D(x): 0.9945 D(G(z)): 0.0104 / 0.0269 Acc: 98.4375 (99.4667)\n",
      "[15/25][63/96] Loss_D: -1.5151 (-1.3111) Loss_G: 4.3976 (5.1332) D(x): 0.9742 D(G(z)): 0.0051 / 0.0164 Acc: 98.4375 (99.4660)\n",
      "[15/25][64/96] Loss_D: -1.3682 (-1.3111) Loss_G: 4.3000 (5.1327) D(x): 0.9835 D(G(z)): 0.0055 / 0.0226 Acc: 100.0000 (99.4664)\n",
      "[15/25][65/96] Loss_D: -1.5052 (-1.3112) Loss_G: 4.1405 (5.1320) D(x): 0.9928 D(G(z)): 0.0183 / 0.0219 Acc: 100.0000 (99.4667)\n",
      "[15/25][66/96] Loss_D: -1.4682 (-1.3113) Loss_G: 4.0703 (5.1313) D(x): 0.9958 D(G(z)): 0.0117 / 0.0201 Acc: 100.0000 (99.4671)\n",
      "[15/25][67/96] Loss_D: -1.3992 (-1.3114) Loss_G: 7.9943 (5.1332) D(x): 0.9855 D(G(z)): 0.0923 / 0.0005 Acc: 100.0000 (99.4674)\n",
      "[15/25][68/96] Loss_D: -1.4407 (-1.3115) Loss_G: 6.1787 (5.1339) D(x): 0.9319 D(G(z)): 0.0008 / 0.0032 Acc: 100.0000 (99.4678)\n",
      "[15/25][69/96] Loss_D: -1.3731 (-1.3115) Loss_G: 2.7559 (5.1323) D(x): 0.8827 D(G(z)): 0.0010 / 0.0839 Acc: 100.0000 (99.4681)\n",
      "[15/25][70/96] Loss_D: -1.4333 (-1.3116) Loss_G: 2.4951 (5.1306) D(x): 0.9995 D(G(z)): 0.0467 / 0.1014 Acc: 98.4375 (99.4674)\n",
      "[15/25][71/96] Loss_D: -1.2482 (-1.3116) Loss_G: 9.4316 (5.1334) D(x): 0.9989 D(G(z)): 0.2259 / 0.0001 Acc: 100.0000 (99.4678)\n",
      "[15/25][72/96] Loss_D: -1.5504 (-1.3117) Loss_G: 9.1974 (5.1361) D(x): 0.9893 D(G(z)): 0.0004 / 0.0001 Acc: 100.0000 (99.4682)\n",
      "[15/25][73/96] Loss_D: -0.6416 (-1.3113) Loss_G: 0.1635 (5.1328) D(x): 0.5496 D(G(z)): 0.0002 / 0.6187 Acc: 100.0000 (99.4685)\n",
      "[15/25][74/96] Loss_D: 0.1810 (-1.3103) Loss_G: 11.1430 (5.1368) D(x): 1.0000 D(G(z)): 0.7296 / 0.0000 Acc: 100.0000 (99.4689)\n",
      "[15/25][75/96] Loss_D: -0.8345 (-1.3100) Loss_G: 7.4632 (5.1383) D(x): 0.6312 D(G(z)): 0.0000 / 0.0011 Acc: 100.0000 (99.4692)\n",
      "[15/25][76/96] Loss_D: -1.5291 (-1.3101) Loss_G: 5.7880 (5.1388) D(x): 0.9424 D(G(z)): 0.0014 / 0.0041 Acc: 100.0000 (99.4696)\n",
      "[15/25][77/96] Loss_D: -1.6667 (-1.3104) Loss_G: 4.9920 (5.1387) D(x): 0.9958 D(G(z)): 0.0009 / 0.0111 Acc: 100.0000 (99.4699)\n",
      "[15/25][78/96] Loss_D: -1.4114 (-1.3104) Loss_G: 3.6463 (5.1377) D(x): 0.9989 D(G(z)): 0.0518 / 0.0366 Acc: 100.0000 (99.4703)\n",
      "[15/25][79/96] Loss_D: -1.4524 (-1.3105) Loss_G: 5.9979 (5.1383) D(x): 0.9979 D(G(z)): 0.0285 / 0.0048 Acc: 100.0000 (99.4706)\n",
      "[15/25][80/96] Loss_D: -1.4309 (-1.3106) Loss_G: 4.6235 (5.1379) D(x): 0.9867 D(G(z)): 0.0545 / 0.0163 Acc: 100.0000 (99.4709)\n",
      "[15/25][81/96] Loss_D: -1.4725 (-1.3107) Loss_G: 7.2266 (5.1393) D(x): 0.9889 D(G(z)): 0.0005 / 0.0010 Acc: 98.4375 (99.4703)\n",
      "[15/25][82/96] Loss_D: -1.3444 (-1.3107) Loss_G: 5.6211 (5.1396) D(x): 0.9404 D(G(z)): 0.0069 / 0.0058 Acc: 100.0000 (99.4706)\n",
      "[15/25][83/96] Loss_D: -1.4436 (-1.3108) Loss_G: 5.9167 (5.1401) D(x): 0.9771 D(G(z)): 0.0158 / 0.0031 Acc: 100.0000 (99.4710)\n",
      "[15/25][84/96] Loss_D: -1.4772 (-1.3109) Loss_G: 4.3965 (5.1396) D(x): 0.9943 D(G(z)): 0.0162 / 0.0197 Acc: 100.0000 (99.4713)\n",
      "[15/25][85/96] Loss_D: -1.4551 (-1.3110) Loss_G: 4.8413 (5.1394) D(x): 0.9977 D(G(z)): 0.0112 / 0.0119 Acc: 100.0000 (99.4717)\n",
      "[15/25][86/96] Loss_D: -1.4999 (-1.3111) Loss_G: 5.5406 (5.1397) D(x): 0.9898 D(G(z)): 0.0342 / 0.0059 Acc: 100.0000 (99.4720)\n",
      "[15/25][87/96] Loss_D: -1.3351 (-1.3112) Loss_G: 5.6692 (5.1400) D(x): 0.9950 D(G(z)): 0.0036 / 0.0056 Acc: 98.4375 (99.4713)\n",
      "[15/25][88/96] Loss_D: -1.3948 (-1.3112) Loss_G: 5.7664 (5.1404) D(x): 0.9909 D(G(z)): 0.0022 / 0.0047 Acc: 100.0000 (99.4717)\n",
      "[15/25][89/96] Loss_D: -1.4526 (-1.3113) Loss_G: 6.4750 (5.1413) D(x): 0.9876 D(G(z)): 0.0185 / 0.0036 Acc: 100.0000 (99.4720)\n",
      "[15/25][90/96] Loss_D: -1.4973 (-1.3114) Loss_G: 5.2956 (5.1414) D(x): 0.9813 D(G(z)): 0.0142 / 0.0061 Acc: 100.0000 (99.4724)\n",
      "[15/25][91/96] Loss_D: -1.5754 (-1.3116) Loss_G: 4.9934 (5.1413) D(x): 0.9906 D(G(z)): 0.0087 / 0.0071 Acc: 100.0000 (99.4727)\n",
      "[15/25][92/96] Loss_D: -1.4648 (-1.3117) Loss_G: 4.0554 (5.1406) D(x): 0.9977 D(G(z)): 0.0016 / 0.0263 Acc: 98.4375 (99.4720)\n",
      "[15/25][93/96] Loss_D: -1.4620 (-1.3118) Loss_G: 3.6800 (5.1397) D(x): 0.9124 D(G(z)): 0.0035 / 0.0394 Acc: 100.0000 (99.4724)\n",
      "[15/25][94/96] Loss_D: -1.4483 (-1.3119) Loss_G: 2.6755 (5.1381) D(x): 0.9957 D(G(z)): 0.0446 / 0.0758 Acc: 96.8750 (99.4707)\n",
      "[15/25][95/96] Loss_D: -1.3944 (-1.3119) Loss_G: 2.7835 (5.1365) D(x): 0.9980 D(G(z)): 0.0522 / 0.0611 Acc: 96.8750 (99.4690)\n",
      "[16/25][0/96] Loss_D: -1.6256 (-1.3121) Loss_G: 4.8278 (5.1363) D(x): 0.9980 D(G(z)): 0.0128 / 0.0090 Acc: 100.0000 (99.4693)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[16/25][1/96] Loss_D: -1.4829 (-1.3123) Loss_G: 4.8053 (5.1361) D(x): 0.9369 D(G(z)): 0.0358 / 0.0098 Acc: 100.0000 (99.4697)\n",
      "[16/25][2/96] Loss_D: -1.4551 (-1.3123) Loss_G: 3.2700 (5.1349) D(x): 0.9894 D(G(z)): 0.0590 / 0.0519 Acc: 100.0000 (99.4700)\n",
      "[16/25][3/96] Loss_D: -1.3469 (-1.3124) Loss_G: 7.9846 (5.1367) D(x): 0.9585 D(G(z)): 0.0537 / 0.0006 Acc: 100.0000 (99.4704)\n",
      "[16/25][4/96] Loss_D: -1.3889 (-1.3124) Loss_G: 6.3778 (5.1376) D(x): 0.9532 D(G(z)): 0.0106 / 0.0033 Acc: 100.0000 (99.4707)\n",
      "[16/25][5/96] Loss_D: -1.3280 (-1.3124) Loss_G: 4.3340 (5.1370) D(x): 0.9070 D(G(z)): 0.0039 / 0.0233 Acc: 100.0000 (99.4711)\n",
      "[16/25][6/96] Loss_D: -1.4368 (-1.3125) Loss_G: 2.2325 (5.1351) D(x): 0.9771 D(G(z)): 0.0079 / 0.1253 Acc: 100.0000 (99.4714)\n",
      "[16/25][7/96] Loss_D: -1.3261 (-1.3125) Loss_G: 5.2764 (5.1352) D(x): 0.9958 D(G(z)): 0.0669 / 0.0072 Acc: 98.4375 (99.4707)\n",
      "[16/25][8/96] Loss_D: -1.4722 (-1.3126) Loss_G: 5.2558 (5.1353) D(x): 0.9921 D(G(z)): 0.0041 / 0.0074 Acc: 98.4375 (99.4701)\n",
      "[16/25][9/96] Loss_D: -1.4788 (-1.3127) Loss_G: 5.7602 (5.1357) D(x): 0.9839 D(G(z)): 0.0191 / 0.0044 Acc: 100.0000 (99.4704)\n",
      "[16/25][10/96] Loss_D: -1.3715 (-1.3128) Loss_G: 4.6592 (5.1354) D(x): 0.9826 D(G(z)): 0.0312 / 0.0158 Acc: 100.0000 (99.4707)\n",
      "[16/25][11/96] Loss_D: -1.5073 (-1.3129) Loss_G: 5.9525 (5.1359) D(x): 0.9895 D(G(z)): 0.0360 / 0.0029 Acc: 100.0000 (99.4711)\n",
      "[16/25][12/96] Loss_D: -1.4641 (-1.3130) Loss_G: 2.4318 (5.1342) D(x): 0.8930 D(G(z)): 0.0015 / 0.0856 Acc: 96.8750 (99.4694)\n",
      "[16/25][13/96] Loss_D: -1.3945 (-1.3130) Loss_G: 3.8671 (5.1334) D(x): 0.9985 D(G(z)): 0.1105 / 0.0262 Acc: 100.0000 (99.4698)\n",
      "[16/25][14/96] Loss_D: -1.5266 (-1.3132) Loss_G: 6.4237 (5.1342) D(x): 0.9884 D(G(z)): 0.0083 / 0.0022 Acc: 100.0000 (99.4701)\n",
      "[16/25][15/96] Loss_D: -1.2361 (-1.3131) Loss_G: 2.7576 (5.1327) D(x): 0.8433 D(G(z)): 0.0065 / 0.0921 Acc: 100.0000 (99.4704)\n",
      "[16/25][16/96] Loss_D: -1.3883 (-1.3132) Loss_G: 4.1822 (5.1321) D(x): 0.9970 D(G(z)): 0.0292 / 0.0242 Acc: 100.0000 (99.4708)\n",
      "[16/25][17/96] Loss_D: -1.0048 (-1.3130) Loss_G: 9.5698 (5.1349) D(x): 0.9984 D(G(z)): 0.2995 / 0.0001 Acc: 100.0000 (99.4711)\n",
      "[16/25][18/96] Loss_D: -1.2463 (-1.3129) Loss_G: 8.1259 (5.1368) D(x): 0.8257 D(G(z)): 0.0037 / 0.0004 Acc: 100.0000 (99.4715)\n",
      "[16/25][19/96] Loss_D: -1.2811 (-1.3129) Loss_G: 5.8570 (5.1373) D(x): 0.8875 D(G(z)): 0.0005 / 0.0046 Acc: 98.4375 (99.4708)\n",
      "[16/25][20/96] Loss_D: -1.4707 (-1.3130) Loss_G: 3.4754 (5.1362) D(x): 0.9859 D(G(z)): 0.0143 / 0.0362 Acc: 98.4375 (99.4701)\n",
      "[16/25][21/96] Loss_D: -1.3854 (-1.3131) Loss_G: 6.9918 (5.1374) D(x): 0.9980 D(G(z)): 0.0851 / 0.0019 Acc: 100.0000 (99.4705)\n",
      "[16/25][22/96] Loss_D: -1.4245 (-1.3131) Loss_G: 3.5223 (5.1364) D(x): 0.9728 D(G(z)): 0.0151 / 0.0541 Acc: 100.0000 (99.4708)\n",
      "[16/25][23/96] Loss_D: -1.3188 (-1.3131) Loss_G: 6.7256 (5.1374) D(x): 0.9371 D(G(z)): 0.0314 / 0.0022 Acc: 100.0000 (99.4712)\n",
      "[16/25][24/96] Loss_D: -1.4473 (-1.3132) Loss_G: 3.5507 (5.1364) D(x): 0.9513 D(G(z)): 0.0137 / 0.0410 Acc: 100.0000 (99.4715)\n",
      "[16/25][25/96] Loss_D: -1.4629 (-1.3133) Loss_G: 2.8234 (5.1349) D(x): 0.9981 D(G(z)): 0.0039 / 0.0692 Acc: 98.4375 (99.4708)\n",
      "[16/25][26/96] Loss_D: -1.3683 (-1.3134) Loss_G: 5.3322 (5.1350) D(x): 0.9976 D(G(z)): 0.0622 / 0.0066 Acc: 100.0000 (99.4712)\n",
      "[16/25][27/96] Loss_D: -1.4669 (-1.3135) Loss_G: 6.7423 (5.1361) D(x): 0.9854 D(G(z)): 0.0023 / 0.0016 Acc: 100.0000 (99.4715)\n",
      "[16/25][28/96] Loss_D: -1.1765 (-1.3134) Loss_G: 5.1580 (5.1361) D(x): 0.9910 D(G(z)): 0.0585 / 0.0092 Acc: 98.4375 (99.4708)\n",
      "[16/25][29/96] Loss_D: -1.4146 (-1.3134) Loss_G: 6.8887 (5.1372) D(x): 0.9085 D(G(z)): 0.0002 / 0.0014 Acc: 100.0000 (99.4712)\n",
      "[16/25][30/96] Loss_D: -1.4616 (-1.3135) Loss_G: 3.1869 (5.1360) D(x): 0.9884 D(G(z)): 0.0108 / 0.0500 Acc: 96.8750 (99.4695)\n",
      "[16/25][31/96] Loss_D: -1.5101 (-1.3137) Loss_G: 4.8094 (5.1358) D(x): 0.9846 D(G(z)): 0.0741 / 0.0093 Acc: 100.0000 (99.4699)\n",
      "[16/25][32/96] Loss_D: -1.3366 (-1.3137) Loss_G: 5.4384 (5.1359) D(x): 0.9578 D(G(z)): 0.0081 / 0.0071 Acc: 100.0000 (99.4702)\n",
      "[16/25][33/96] Loss_D: -1.5345 (-1.3138) Loss_G: 5.7364 (5.1363) D(x): 0.9761 D(G(z)): 0.0027 / 0.0065 Acc: 100.0000 (99.4705)\n",
      "[16/25][34/96] Loss_D: -1.4617 (-1.3139) Loss_G: 5.7172 (5.1367) D(x): 0.9836 D(G(z)): 0.0476 / 0.0045 Acc: 98.4375 (99.4699)\n",
      "[16/25][35/96] Loss_D: -1.3626 (-1.3139) Loss_G: 2.5819 (5.1351) D(x): 0.8161 D(G(z)): 0.0039 / 0.0801 Acc: 100.0000 (99.4702)\n",
      "[16/25][36/96] Loss_D: -1.3764 (-1.3140) Loss_G: 3.4132 (5.1340) D(x): 0.9995 D(G(z)): 0.1281 / 0.0470 Acc: 98.4375 (99.4696)\n",
      "[16/25][37/96] Loss_D: -1.4394 (-1.3141) Loss_G: 4.1211 (5.1333) D(x): 0.9963 D(G(z)): 0.0745 / 0.0179 Acc: 100.0000 (99.4699)\n",
      "[16/25][38/96] Loss_D: -1.4364 (-1.3141) Loss_G: 4.7800 (5.1331) D(x): 0.9483 D(G(z)): 0.0623 / 0.0147 Acc: 98.4375 (99.4692)\n",
      "[16/25][39/96] Loss_D: -1.4269 (-1.3142) Loss_G: 6.8081 (5.1342) D(x): 0.9164 D(G(z)): 0.0077 / 0.0016 Acc: 98.4375 (99.4686)\n",
      "[16/25][40/96] Loss_D: -1.3682 (-1.3142) Loss_G: 4.2751 (5.1336) D(x): 0.9675 D(G(z)): 0.0023 / 0.0290 Acc: 100.0000 (99.4689)\n",
      "[16/25][41/96] Loss_D: -1.4329 (-1.3143) Loss_G: 3.5869 (5.1326) D(x): 0.9820 D(G(z)): 0.0304 / 0.0388 Acc: 100.0000 (99.4693)\n",
      "[16/25][42/96] Loss_D: -1.4836 (-1.3144) Loss_G: 5.0519 (5.1326) D(x): 0.9973 D(G(z)): 0.0651 / 0.0106 Acc: 100.0000 (99.4696)\n",
      "[16/25][43/96] Loss_D: -1.5083 (-1.3145) Loss_G: 5.7627 (5.1330) D(x): 0.9670 D(G(z)): 0.0039 / 0.0042 Acc: 100.0000 (99.4699)\n",
      "[16/25][44/96] Loss_D: -1.2951 (-1.3145) Loss_G: 2.9989 (5.1316) D(x): 0.8626 D(G(z)): 0.0020 / 0.0642 Acc: 100.0000 (99.4703)\n",
      "[16/25][45/96] Loss_D: -1.3818 (-1.3146) Loss_G: 1.7106 (5.1295) D(x): 0.9978 D(G(z)): 0.0324 / 0.2025 Acc: 100.0000 (99.4706)\n",
      "[16/25][46/96] Loss_D: -1.1580 (-1.3145) Loss_G: 8.9769 (5.1319) D(x): 0.9992 D(G(z)): 0.2380 / 0.0001 Acc: 100.0000 (99.4709)\n",
      "[16/25][47/96] Loss_D: -1.5270 (-1.3146) Loss_G: 9.2625 (5.1345) D(x): 0.9448 D(G(z)): 0.0066 / 0.0002 Acc: 100.0000 (99.4713)\n",
      "[16/25][48/96] Loss_D: -0.7491 (-1.3143) Loss_G: 0.2187 (5.1314) D(x): 0.5303 D(G(z)): 0.0001 / 0.5531 Acc: 100.0000 (99.4716)\n",
      "[16/25][49/96] Loss_D: 1.6802 (-1.3124) Loss_G: 10.2827 (5.1347) D(x): 1.0000 D(G(z)): 0.9242 / 0.0001 Acc: 100.0000 (99.4719)\n",
      "[16/25][50/96] Loss_D: 1.3073 (-1.3107) Loss_G: 1.7200 (5.1325) D(x): 0.1624 D(G(z)): 0.0000 / 0.1595 Acc: 96.8750 (99.4703)\n",
      "[16/25][51/96] Loss_D: -1.0555 (-1.3106) Loss_G: 0.8678 (5.1298) D(x): 0.9791 D(G(z)): 0.3102 / 0.3349 Acc: 100.0000 (99.4706)\n",
      "[16/25][52/96] Loss_D: -1.0453 (-1.3104) Loss_G: 5.0240 (5.1298) D(x): 0.9881 D(G(z)): 0.3496 / 0.0086 Acc: 100.0000 (99.4710)\n",
      "[16/25][53/96] Loss_D: -1.4331 (-1.3105) Loss_G: 6.3313 (5.1305) D(x): 0.9572 D(G(z)): 0.0057 / 0.0025 Acc: 100.0000 (99.4713)\n",
      "[16/25][54/96] Loss_D: -1.3995 (-1.3105) Loss_G: 5.9639 (5.1310) D(x): 0.9038 D(G(z)): 0.0044 / 0.0035 Acc: 100.0000 (99.4716)\n",
      "[16/25][55/96] Loss_D: -1.4026 (-1.3106) Loss_G: 4.9790 (5.1309) D(x): 0.9846 D(G(z)): 0.0019 / 0.0315 Acc: 98.4375 (99.4710)\n",
      "[16/25][56/96] Loss_D: -1.5071 (-1.3107) Loss_G: 3.0263 (5.1296) D(x): 0.9957 D(G(z)): 0.0193 / 0.0894 Acc: 98.4375 (99.4703)\n",
      "[16/25][57/96] Loss_D: -1.4111 (-1.3108) Loss_G: 5.2391 (5.1297) D(x): 0.9823 D(G(z)): 0.0848 / 0.0088 Acc: 100.0000 (99.4707)\n",
      "[16/25][58/96] Loss_D: -1.4062 (-1.3108) Loss_G: 4.6238 (5.1294) D(x): 0.9576 D(G(z)): 0.0160 / 0.0167 Acc: 100.0000 (99.4710)\n",
      "[16/25][59/96] Loss_D: -1.4680 (-1.3109) Loss_G: 5.3595 (5.1295) D(x): 0.9912 D(G(z)): 0.0214 / 0.0079 Acc: 100.0000 (99.4713)\n",
      "[16/25][60/96] Loss_D: -1.3104 (-1.3109) Loss_G: 5.2032 (5.1296) D(x): 0.9930 D(G(z)): 0.0638 / 0.0122 Acc: 98.4375 (99.4707)\n",
      "[16/25][61/96] Loss_D: -1.3702 (-1.3110) Loss_G: 7.9077 (5.1313) D(x): 0.9613 D(G(z)): 0.0084 / 0.0005 Acc: 100.0000 (99.4710)\n",
      "[16/25][62/96] Loss_D: -1.4660 (-1.3111) Loss_G: 5.1234 (5.1313) D(x): 0.9489 D(G(z)): 0.0082 / 0.0111 Acc: 100.0000 (99.4713)\n",
      "[16/25][63/96] Loss_D: -1.5216 (-1.3112) Loss_G: 3.5108 (5.1303) D(x): 0.9967 D(G(z)): 0.0062 / 0.0410 Acc: 100.0000 (99.4717)\n",
      "[16/25][64/96] Loss_D: -1.4726 (-1.3113) Loss_G: 2.8560 (5.1289) D(x): 0.9781 D(G(z)): 0.0047 / 0.0652 Acc: 100.0000 (99.4720)\n",
      "[16/25][65/96] Loss_D: -1.4722 (-1.3114) Loss_G: 4.5766 (5.1285) D(x): 0.9903 D(G(z)): 0.0564 / 0.0156 Acc: 100.0000 (99.4723)\n",
      "[16/25][66/96] Loss_D: -1.4380 (-1.3115) Loss_G: 4.4275 (5.1281) D(x): 0.9634 D(G(z)): 0.0073 / 0.0163 Acc: 100.0000 (99.4727)\n",
      "[16/25][67/96] Loss_D: -1.4044 (-1.3115) Loss_G: 4.4918 (5.1277) D(x): 0.9912 D(G(z)): 0.0495 / 0.0142 Acc: 100.0000 (99.4730)\n",
      "[16/25][68/96] Loss_D: -1.5178 (-1.3117) Loss_G: 5.5549 (5.1280) D(x): 0.9863 D(G(z)): 0.0287 / 0.0043 Acc: 100.0000 (99.4733)\n",
      "[16/25][69/96] Loss_D: -1.5073 (-1.3118) Loss_G: 5.3075 (5.1281) D(x): 0.9394 D(G(z)): 0.0037 / 0.0059 Acc: 98.4375 (99.4727)\n",
      "[16/25][70/96] Loss_D: -1.4902 (-1.3119) Loss_G: 3.2772 (5.1269) D(x): 0.9432 D(G(z)): 0.0247 / 0.0400 Acc: 100.0000 (99.4730)\n",
      "[16/25][71/96] Loss_D: -1.5162 (-1.3120) Loss_G: 4.0829 (5.1263) D(x): 0.9961 D(G(z)): 0.0109 / 0.0278 Acc: 100.0000 (99.4733)\n",
      "[16/25][72/96] Loss_D: -1.0989 (-1.3119) Loss_G: 10.4701 (5.1296) D(x): 0.9713 D(G(z)): 0.2817 / 0.0001 Acc: 100.0000 (99.4737)\n",
      "[16/25][73/96] Loss_D: 0.7610 (-1.3106) Loss_G: -0.3762 (5.1262) D(x): 0.1916 D(G(z)): 0.0001 / 0.8380 Acc: 100.0000 (99.4740)\n",
      "[16/25][74/96] Loss_D: 1.7231 (-1.3087) Loss_G: 2.3269 (5.1244) D(x): 0.9999 D(G(z)): 0.9000 / 0.1344 Acc: 98.4375 (99.4733)\n",
      "[16/25][75/96] Loss_D: -1.3736 (-1.3088) Loss_G: 4.8116 (5.1242) D(x): 0.9807 D(G(z)): 0.1756 / 0.0113 Acc: 100.0000 (99.4737)\n",
      "[16/25][76/96] Loss_D: -0.5931 (-1.3083) Loss_G: 4.4052 (5.1238) D(x): 0.5022 D(G(z)): 0.0342 / 0.0244 Acc: 100.0000 (99.4740)\n",
      "[16/25][77/96] Loss_D: -1.1660 (-1.3082) Loss_G: 0.8619 (5.1211) D(x): 0.8465 D(G(z)): 0.1992 / 0.3119 Acc: 100.0000 (99.4743)\n",
      "[16/25][78/96] Loss_D: -1.2165 (-1.3082) Loss_G: 2.5167 (5.1195) D(x): 0.9325 D(G(z)): 0.1704 / 0.1060 Acc: 100.0000 (99.4747)\n",
      "[16/25][79/96] Loss_D: -1.3167 (-1.3082) Loss_G: 5.3257 (5.1197) D(x): 0.9444 D(G(z)): 0.0894 / 0.0097 Acc: 100.0000 (99.4750)\n",
      "[16/25][80/96] Loss_D: -1.2702 (-1.3081) Loss_G: 3.5525 (5.1187) D(x): 0.8512 D(G(z)): 0.0191 / 0.0422 Acc: 100.0000 (99.4753)\n",
      "[16/25][81/96] Loss_D: -1.3759 (-1.3082) Loss_G: 4.3935 (5.1182) D(x): 0.9763 D(G(z)): 0.0628 / 0.0212 Acc: 98.4375 (99.4747)\n",
      "[16/25][82/96] Loss_D: -1.4969 (-1.3083) Loss_G: 4.9957 (5.1182) D(x): 0.9641 D(G(z)): 0.0528 / 0.0113 Acc: 100.0000 (99.4750)\n",
      "[16/25][83/96] Loss_D: -1.4945 (-1.3084) Loss_G: 4.3902 (5.1177) D(x): 0.9820 D(G(z)): 0.0024 / 0.0167 Acc: 100.0000 (99.4753)\n",
      "[16/25][84/96] Loss_D: -1.3030 (-1.3084) Loss_G: 5.4546 (5.1179) D(x): 0.9901 D(G(z)): 0.0807 / 0.0057 Acc: 100.0000 (99.4756)\n",
      "[16/25][85/96] Loss_D: -1.4454 (-1.3085) Loss_G: 5.7229 (5.1183) D(x): 0.9875 D(G(z)): 0.0103 / 0.0075 Acc: 96.8750 (99.4740)\n",
      "[16/25][86/96] Loss_D: -1.4469 (-1.3086) Loss_G: 6.9672 (5.1194) D(x): 0.9802 D(G(z)): 0.0014 / 0.0015 Acc: 100.0000 (99.4744)\n",
      "[16/25][87/96] Loss_D: -1.3379 (-1.3086) Loss_G: 5.3212 (5.1196) D(x): 0.9902 D(G(z)): 0.0105 / 0.0106 Acc: 96.8750 (99.4728)\n",
      "[16/25][88/96] Loss_D: -1.3839 (-1.3086) Loss_G: 3.5808 (5.1186) D(x): 0.9088 D(G(z)): 0.0091 / 0.0315 Acc: 100.0000 (99.4731)\n",
      "[16/25][89/96] Loss_D: -1.5566 (-1.3088) Loss_G: 1.2568 (5.1162) D(x): 0.9934 D(G(z)): 0.0295 / 0.2350 Acc: 98.4375 (99.4724)\n",
      "[16/25][90/96] Loss_D: -1.2524 (-1.3088) Loss_G: 6.9200 (5.1174) D(x): 0.9931 D(G(z)): 0.1708 / 0.0016 Acc: 98.4375 (99.4718)\n",
      "[16/25][91/96] Loss_D: -1.3714 (-1.3088) Loss_G: 6.0729 (5.1179) D(x): 0.9292 D(G(z)): 0.0018 / 0.0040 Acc: 100.0000 (99.4721)\n",
      "[16/25][92/96] Loss_D: -1.2897 (-1.3088) Loss_G: 3.5203 (5.1170) D(x): 0.8734 D(G(z)): 0.0051 / 0.0423 Acc: 98.4375 (99.4715)\n",
      "[16/25][93/96] Loss_D: -1.3825 (-1.3088) Loss_G: 3.5866 (5.1160) D(x): 0.9941 D(G(z)): 0.0576 / 0.0363 Acc: 100.0000 (99.4718)\n",
      "[16/25][94/96] Loss_D: -1.5380 (-1.3090) Loss_G: 5.0889 (5.1160) D(x): 0.9991 D(G(z)): 0.0632 / 0.0093 Acc: 98.4375 (99.4712)\n",
      "[16/25][95/96] Loss_D: -1.2935 (-1.3090) Loss_G: 5.4336 (5.1162) D(x): 0.9811 D(G(z)): 0.0293 / 0.0078 Acc: 100.0000 (99.4715)\n",
      "[17/25][0/96] Loss_D: -1.3696 (-1.3090) Loss_G: 3.5599 (5.1152) D(x): 0.9130 D(G(z)): 0.0167 / 0.0500 Acc: 100.0000 (99.4718)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[17/25][1/96] Loss_D: -1.2332 (-1.3090) Loss_G: 3.9808 (5.1146) D(x): 0.9531 D(G(z)): 0.1067 / 0.0261 Acc: 100.0000 (99.4722)\n",
      "[17/25][2/96] Loss_D: -1.4726 (-1.3091) Loss_G: 5.8006 (5.1150) D(x): 0.9949 D(G(z)): 0.0215 / 0.0055 Acc: 100.0000 (99.4725)\n",
      "[17/25][3/96] Loss_D: -1.1387 (-1.3090) Loss_G: 2.4909 (5.1134) D(x): 0.7560 D(G(z)): 0.0016 / 0.1004 Acc: 100.0000 (99.4728)\n",
      "[17/25][4/96] Loss_D: -1.1960 (-1.3089) Loss_G: 2.1333 (5.1115) D(x): 0.9995 D(G(z)): 0.2245 / 0.1542 Acc: 100.0000 (99.4731)\n",
      "[17/25][5/96] Loss_D: -1.4517 (-1.3090) Loss_G: 7.2427 (5.1128) D(x): 0.9963 D(G(z)): 0.0937 / 0.0008 Acc: 100.0000 (99.4734)\n",
      "[17/25][6/96] Loss_D: -1.2331 (-1.3089) Loss_G: 3.2925 (5.1117) D(x): 0.8007 D(G(z)): 0.0009 / 0.0544 Acc: 96.8750 (99.4719)\n",
      "[17/25][7/96] Loss_D: -1.4253 (-1.3090) Loss_G: 2.7335 (5.1103) D(x): 0.9884 D(G(z)): 0.0427 / 0.0830 Acc: 100.0000 (99.4722)\n",
      "[17/25][8/96] Loss_D: -1.4434 (-1.3091) Loss_G: 2.8241 (5.1089) D(x): 0.9949 D(G(z)): 0.0481 / 0.0691 Acc: 98.4375 (99.4715)\n",
      "[17/25][9/96] Loss_D: -1.4530 (-1.3092) Loss_G: 3.5965 (5.1080) D(x): 0.9945 D(G(z)): 0.0099 / 0.0405 Acc: 100.0000 (99.4719)\n",
      "[17/25][10/96] Loss_D: -1.5149 (-1.3093) Loss_G: 1.9292 (5.1060) D(x): 0.9671 D(G(z)): 0.0422 / 0.1474 Acc: 98.4375 (99.4712)\n",
      "[17/25][11/96] Loss_D: -1.3781 (-1.3093) Loss_G: 5.5159 (5.1063) D(x): 0.9666 D(G(z)): 0.0507 / 0.0074 Acc: 100.0000 (99.4716)\n",
      "[17/25][12/96] Loss_D: -1.3763 (-1.3094) Loss_G: 4.1558 (5.1057) D(x): 0.9255 D(G(z)): 0.0372 / 0.0204 Acc: 100.0000 (99.4719)\n",
      "[17/25][13/96] Loss_D: -1.4702 (-1.3095) Loss_G: 3.4271 (5.1047) D(x): 0.9917 D(G(z)): 0.0053 / 0.0427 Acc: 100.0000 (99.4722)\n",
      "[17/25][14/96] Loss_D: -1.4003 (-1.3095) Loss_G: 2.6939 (5.1032) D(x): 0.9857 D(G(z)): 0.0210 / 0.0782 Acc: 98.4375 (99.4716)\n",
      "[17/25][15/96] Loss_D: -1.4000 (-1.3096) Loss_G: 2.4520 (5.1016) D(x): 0.9901 D(G(z)): 0.0115 / 0.1001 Acc: 98.4375 (99.4709)\n",
      "[17/25][16/96] Loss_D: -1.3940 (-1.3096) Loss_G: 4.3249 (5.1011) D(x): 0.9938 D(G(z)): 0.0355 / 0.0181 Acc: 98.4375 (99.4703)\n",
      "[17/25][17/96] Loss_D: -1.4740 (-1.3097) Loss_G: 2.5177 (5.0996) D(x): 0.9548 D(G(z)): 0.0081 / 0.1226 Acc: 100.0000 (99.4706)\n",
      "[17/25][18/96] Loss_D: -1.1717 (-1.3097) Loss_G: 5.1998 (5.0996) D(x): 0.9460 D(G(z)): 0.1490 / 0.0080 Acc: 98.4375 (99.4700)\n",
      "[17/25][19/96] Loss_D: -1.1268 (-1.3095) Loss_G: 1.9011 (5.0977) D(x): 0.7650 D(G(z)): 0.0022 / 0.1584 Acc: 98.4375 (99.4694)\n",
      "[17/25][20/96] Loss_D: -0.5567 (-1.3091) Loss_G: 9.1921 (5.1002) D(x): 0.9995 D(G(z)): 0.5352 / 0.0002 Acc: 100.0000 (99.4697)\n",
      "[17/25][21/96] Loss_D: 0.0505 (-1.3083) Loss_G: 3.4977 (5.0992) D(x): 0.3308 D(G(z)): 0.0001 / 0.0388 Acc: 100.0000 (99.4700)\n",
      "[17/25][22/96] Loss_D: -1.3243 (-1.3083) Loss_G: 0.9155 (5.0967) D(x): 0.9981 D(G(z)): 0.1242 / 0.3608 Acc: 98.4375 (99.4694)\n",
      "[17/25][23/96] Loss_D: -0.3784 (-1.3077) Loss_G: 7.2745 (5.0980) D(x): 0.9991 D(G(z)): 0.5640 / 0.0012 Acc: 98.4375 (99.4688)\n",
      "[17/25][24/96] Loss_D: -0.7231 (-1.3074) Loss_G: 3.2875 (5.0969) D(x): 0.6104 D(G(z)): 0.0006 / 0.0549 Acc: 98.4375 (99.4682)\n",
      "[17/25][25/96] Loss_D: -1.3279 (-1.3074) Loss_G: 3.0306 (5.0957) D(x): 0.9402 D(G(z)): 0.0183 / 0.0541 Acc: 98.4375 (99.4675)\n",
      "[17/25][26/96] Loss_D: -1.1886 (-1.3073) Loss_G: 4.1994 (5.0951) D(x): 0.9328 D(G(z)): 0.1256 / 0.0249 Acc: 100.0000 (99.4679)\n",
      "[17/25][27/96] Loss_D: -1.3531 (-1.3073) Loss_G: 3.7772 (5.0943) D(x): 0.9798 D(G(z)): 0.0915 / 0.0366 Acc: 98.4375 (99.4672)\n",
      "[17/25][28/96] Loss_D: -1.3684 (-1.3074) Loss_G: 3.1669 (5.0932) D(x): 0.8720 D(G(z)): 0.0083 / 0.0517 Acc: 100.0000 (99.4676)\n",
      "[17/25][29/96] Loss_D: -1.2354 (-1.3073) Loss_G: 5.8898 (5.0936) D(x): 0.9958 D(G(z)): 0.1831 / 0.0036 Acc: 100.0000 (99.4679)\n",
      "[17/25][30/96] Loss_D: -1.5390 (-1.3075) Loss_G: 5.3813 (5.0938) D(x): 0.9389 D(G(z)): 0.0008 / 0.0120 Acc: 100.0000 (99.4682)\n",
      "[17/25][31/96] Loss_D: -1.4466 (-1.3075) Loss_G: 4.9152 (5.0937) D(x): 0.9838 D(G(z)): 0.0197 / 0.0133 Acc: 98.4375 (99.4676)\n",
      "[17/25][32/96] Loss_D: -1.4951 (-1.3077) Loss_G: 6.6115 (5.0946) D(x): 0.9855 D(G(z)): 0.0037 / 0.0034 Acc: 98.4375 (99.4670)\n",
      "[17/25][33/96] Loss_D: -1.5085 (-1.3078) Loss_G: 5.2309 (5.0947) D(x): 0.9567 D(G(z)): 0.0205 / 0.0084 Acc: 100.0000 (99.4673)\n",
      "[17/25][34/96] Loss_D: -1.4714 (-1.3079) Loss_G: 1.1406 (5.0923) D(x): 0.8925 D(G(z)): 0.0161 / 0.2559 Acc: 100.0000 (99.4676)\n",
      "[17/25][35/96] Loss_D: -1.4711 (-1.3080) Loss_G: 0.2047 (5.0894) D(x): 0.9982 D(G(z)): 0.0112 / 0.5731 Acc: 100.0000 (99.4679)\n",
      "[17/25][36/96] Loss_D: -1.5064 (-1.3081) Loss_G: 3.1651 (5.0882) D(x): 0.9946 D(G(z)): 0.0724 / 0.0443 Acc: 98.4375 (99.4673)\n",
      "[17/25][37/96] Loss_D: -1.5107 (-1.3082) Loss_G: 4.7868 (5.0881) D(x): 0.9952 D(G(z)): 0.0681 / 0.0077 Acc: 100.0000 (99.4676)\n",
      "[17/25][38/96] Loss_D: -1.5157 (-1.3083) Loss_G: 6.8452 (5.0891) D(x): 0.9733 D(G(z)): 0.0030 / 0.0014 Acc: 100.0000 (99.4679)\n",
      "[17/25][39/96] Loss_D: -1.5294 (-1.3085) Loss_G: 5.0063 (5.0891) D(x): 0.9856 D(G(z)): 0.0026 / 0.0105 Acc: 100.0000 (99.4683)\n",
      "[17/25][40/96] Loss_D: -1.4454 (-1.3085) Loss_G: 5.4667 (5.0893) D(x): 0.9687 D(G(z)): 0.0061 / 0.0057 Acc: 100.0000 (99.4686)\n",
      "[17/25][41/96] Loss_D: -1.5025 (-1.3087) Loss_G: 4.2942 (5.0888) D(x): 0.9912 D(G(z)): 0.0193 / 0.0174 Acc: 100.0000 (99.4689)\n",
      "[17/25][42/96] Loss_D: -1.4403 (-1.3087) Loss_G: 4.6162 (5.0885) D(x): 0.9972 D(G(z)): 0.0099 / 0.0148 Acc: 96.8750 (99.4674)\n",
      "[17/25][43/96] Loss_D: -1.4599 (-1.3088) Loss_G: 5.6201 (5.0889) D(x): 0.9925 D(G(z)): 0.0434 / 0.0067 Acc: 100.0000 (99.4677)\n",
      "[17/25][44/96] Loss_D: -1.5019 (-1.3089) Loss_G: 5.5634 (5.0891) D(x): 0.9639 D(G(z)): 0.0306 / 0.0058 Acc: 100.0000 (99.4680)\n",
      "[17/25][45/96] Loss_D: -1.5793 (-1.3091) Loss_G: 6.2352 (5.0898) D(x): 0.9593 D(G(z)): 0.0292 / 0.0027 Acc: 98.4375 (99.4674)\n",
      "[17/25][46/96] Loss_D: -1.4665 (-1.3092) Loss_G: 4.3545 (5.0894) D(x): 0.9715 D(G(z)): 0.0036 / 0.0196 Acc: 100.0000 (99.4677)\n",
      "[17/25][47/96] Loss_D: -1.3922 (-1.3093) Loss_G: 3.5200 (5.0885) D(x): 0.9386 D(G(z)): 0.0155 / 0.0600 Acc: 100.0000 (99.4680)\n",
      "[17/25][48/96] Loss_D: -1.3204 (-1.3093) Loss_G: 5.5077 (5.0887) D(x): 0.9862 D(G(z)): 0.1333 / 0.0063 Acc: 100.0000 (99.4683)\n",
      "[17/25][49/96] Loss_D: -1.3077 (-1.3093) Loss_G: 6.2574 (5.0894) D(x): 0.9242 D(G(z)): 0.0100 / 0.0028 Acc: 100.0000 (99.4686)\n",
      "[17/25][50/96] Loss_D: -1.4173 (-1.3093) Loss_G: 5.1433 (5.0894) D(x): 0.9862 D(G(z)): 0.0324 / 0.0110 Acc: 100.0000 (99.4690)\n",
      "[17/25][51/96] Loss_D: -1.3532 (-1.3093) Loss_G: 2.6258 (5.0880) D(x): 0.9476 D(G(z)): 0.0114 / 0.0919 Acc: 100.0000 (99.4693)\n",
      "[17/25][52/96] Loss_D: -1.4621 (-1.3094) Loss_G: 2.9710 (5.0867) D(x): 0.9859 D(G(z)): 0.0074 / 0.0629 Acc: 100.0000 (99.4696)\n",
      "[17/25][53/96] Loss_D: -1.5271 (-1.3096) Loss_G: 2.8039 (5.0854) D(x): 0.9961 D(G(z)): 0.0155 / 0.0686 Acc: 100.0000 (99.4699)\n",
      "[17/25][54/96] Loss_D: -1.4418 (-1.3096) Loss_G: 3.5957 (5.0845) D(x): 0.9818 D(G(z)): 0.0077 / 0.0423 Acc: 98.4375 (99.4693)\n",
      "[17/25][55/96] Loss_D: -1.3399 (-1.3097) Loss_G: 3.3458 (5.0834) D(x): 0.9897 D(G(z)): 0.1067 / 0.0522 Acc: 100.0000 (99.4696)\n",
      "[17/25][56/96] Loss_D: -1.5014 (-1.3098) Loss_G: 4.0331 (5.0828) D(x): 0.9605 D(G(z)): 0.0174 / 0.0280 Acc: 100.0000 (99.4699)\n",
      "[17/25][57/96] Loss_D: -1.3719 (-1.3098) Loss_G: 3.0834 (5.0816) D(x): 0.9386 D(G(z)): 0.0096 / 0.0507 Acc: 100.0000 (99.4702)\n",
      "[17/25][58/96] Loss_D: -1.3938 (-1.3099) Loss_G: 2.2141 (5.0799) D(x): 0.8980 D(G(z)): 0.0030 / 0.1145 Acc: 100.0000 (99.4705)\n",
      "[17/25][59/96] Loss_D: -1.3996 (-1.3099) Loss_G: 3.0922 (5.0788) D(x): 0.9979 D(G(z)): 0.1245 / 0.0601 Acc: 98.4375 (99.4699)\n",
      "[17/25][60/96] Loss_D: -1.4269 (-1.3100) Loss_G: 5.0641 (5.0788) D(x): 0.9963 D(G(z)): 0.0739 / 0.0074 Acc: 100.0000 (99.4702)\n",
      "[17/25][61/96] Loss_D: -1.2915 (-1.3100) Loss_G: 2.9422 (5.0775) D(x): 0.8506 D(G(z)): 0.0092 / 0.0724 Acc: 100.0000 (99.4706)\n",
      "[17/25][62/96] Loss_D: -1.2958 (-1.3100) Loss_G: 5.3601 (5.0777) D(x): 0.9983 D(G(z)): 0.1286 / 0.0084 Acc: 100.0000 (99.4709)\n",
      "[17/25][63/96] Loss_D: -1.4335 (-1.3100) Loss_G: 6.2118 (5.0783) D(x): 0.9827 D(G(z)): 0.0310 / 0.0026 Acc: 100.0000 (99.4712)\n",
      "[17/25][64/96] Loss_D: -1.2122 (-1.3100) Loss_G: 2.0418 (5.0765) D(x): 0.7919 D(G(z)): 0.0020 / 0.1391 Acc: 100.0000 (99.4715)\n",
      "[17/25][65/96] Loss_D: -0.6769 (-1.3096) Loss_G: 13.6121 (5.0816) D(x): 0.9988 D(G(z)): 0.5346 / 0.0000 Acc: 98.4375 (99.4709)\n",
      "[17/25][66/96] Loss_D: 1.0217 (-1.3082) Loss_G: -0.3649 (5.0784) D(x): 0.1611 D(G(z)): 0.0000 / 0.8446 Acc: 100.0000 (99.4712)\n",
      "[17/25][67/96] Loss_D: 2.7085 (-1.3059) Loss_G: 4.0420 (5.0778) D(x): 0.9999 D(G(z)): 0.9647 / 0.0344 Acc: 98.4375 (99.4706)\n",
      "[17/25][68/96] Loss_D: -0.7527 (-1.3056) Loss_G: 2.7841 (5.0764) D(x): 0.6660 D(G(z)): 0.0732 / 0.0673 Acc: 100.0000 (99.4709)\n",
      "[17/25][69/96] Loss_D: -0.8480 (-1.3053) Loss_G: 1.2514 (5.0742) D(x): 0.6732 D(G(z)): 0.0635 / 0.2868 Acc: 100.0000 (99.4712)\n",
      "[17/25][70/96] Loss_D: -1.1763 (-1.3052) Loss_G: 1.4579 (5.0720) D(x): 0.9256 D(G(z)): 0.1231 / 0.2582 Acc: 100.0000 (99.4715)\n",
      "[17/25][71/96] Loss_D: -1.0270 (-1.3050) Loss_G: 5.7037 (5.0724) D(x): 0.9662 D(G(z)): 0.2839 / 0.0053 Acc: 100.0000 (99.4718)\n",
      "[17/25][72/96] Loss_D: -1.1700 (-1.3050) Loss_G: 4.9113 (5.0723) D(x): 0.7860 D(G(z)): 0.0178 / 0.0109 Acc: 98.4375 (99.4712)\n",
      "[17/25][73/96] Loss_D: -1.3437 (-1.3050) Loss_G: 3.8633 (5.0716) D(x): 0.8883 D(G(z)): 0.0349 / 0.0308 Acc: 98.4375 (99.4706)\n",
      "[17/25][74/96] Loss_D: -1.6030 (-1.3052) Loss_G: 5.1660 (5.0717) D(x): 0.9902 D(G(z)): 0.0524 / 0.0069 Acc: 100.0000 (99.4709)\n",
      "[17/25][75/96] Loss_D: -1.4461 (-1.3052) Loss_G: 4.7429 (5.0715) D(x): 0.9855 D(G(z)): 0.0075 / 0.0118 Acc: 100.0000 (99.4712)\n",
      "[17/25][76/96] Loss_D: -1.4221 (-1.3053) Loss_G: 3.9223 (5.0708) D(x): 0.9795 D(G(z)): 0.0456 / 0.0311 Acc: 100.0000 (99.4715)\n",
      "[17/25][77/96] Loss_D: -1.4278 (-1.3054) Loss_G: 3.4241 (5.0698) D(x): 0.9609 D(G(z)): 0.0534 / 0.0490 Acc: 100.0000 (99.4719)\n",
      "[17/25][78/96] Loss_D: -1.4795 (-1.3055) Loss_G: 4.3425 (5.0694) D(x): 0.9893 D(G(z)): 0.0088 / 0.0151 Acc: 98.4375 (99.4713)\n",
      "[17/25][79/96] Loss_D: -1.4088 (-1.3055) Loss_G: 5.6862 (5.0698) D(x): 0.9703 D(G(z)): 0.0125 / 0.0103 Acc: 100.0000 (99.4716)\n",
      "[17/25][80/96] Loss_D: -1.5116 (-1.3057) Loss_G: 6.1015 (5.0704) D(x): 0.9808 D(G(z)): 0.0525 / 0.0036 Acc: 100.0000 (99.4719)\n",
      "[17/25][81/96] Loss_D: -1.4192 (-1.3057) Loss_G: 6.1691 (5.0710) D(x): 0.9829 D(G(z)): 0.0529 / 0.0026 Acc: 100.0000 (99.4722)\n",
      "[17/25][82/96] Loss_D: -1.3612 (-1.3058) Loss_G: 5.5794 (5.0713) D(x): 0.9362 D(G(z)): 0.0029 / 0.0051 Acc: 100.0000 (99.4725)\n",
      "[17/25][83/96] Loss_D: -1.5394 (-1.3059) Loss_G: 5.8049 (5.0717) D(x): 0.9712 D(G(z)): 0.0058 / 0.0056 Acc: 100.0000 (99.4728)\n",
      "[17/25][84/96] Loss_D: -1.4454 (-1.3060) Loss_G: 4.5808 (5.0714) D(x): 0.9744 D(G(z)): 0.0404 / 0.0131 Acc: 98.4375 (99.4722)\n",
      "[17/25][85/96] Loss_D: -1.5129 (-1.3061) Loss_G: 4.9898 (5.0714) D(x): 0.9755 D(G(z)): 0.0229 / 0.0109 Acc: 100.0000 (99.4725)\n",
      "[17/25][86/96] Loss_D: -1.4457 (-1.3062) Loss_G: 4.5578 (5.0711) D(x): 0.9824 D(G(z)): 0.0199 / 0.0139 Acc: 100.0000 (99.4728)\n",
      "[17/25][87/96] Loss_D: -1.4897 (-1.3063) Loss_G: 3.7866 (5.0704) D(x): 0.9561 D(G(z)): 0.0080 / 0.0348 Acc: 100.0000 (99.4731)\n",
      "[17/25][88/96] Loss_D: -1.5606 (-1.3064) Loss_G: 3.5678 (5.0695) D(x): 0.9742 D(G(z)): 0.0147 / 0.0362 Acc: 100.0000 (99.4734)\n",
      "[17/25][89/96] Loss_D: -1.5126 (-1.3066) Loss_G: 4.1055 (5.0689) D(x): 0.9826 D(G(z)): 0.0427 / 0.0236 Acc: 100.0000 (99.4737)\n",
      "[17/25][90/96] Loss_D: -1.4686 (-1.3067) Loss_G: 2.6536 (5.0675) D(x): 0.9943 D(G(z)): 0.0247 / 0.0923 Acc: 98.4375 (99.4731)\n",
      "[17/25][91/96] Loss_D: -1.5219 (-1.3068) Loss_G: 3.1641 (5.0664) D(x): 0.9768 D(G(z)): 0.0308 / 0.0561 Acc: 100.0000 (99.4734)\n",
      "[17/25][92/96] Loss_D: -1.5382 (-1.3069) Loss_G: 3.5035 (5.0655) D(x): 0.9766 D(G(z)): 0.0136 / 0.0403 Acc: 100.0000 (99.4737)\n",
      "[17/25][93/96] Loss_D: -1.5234 (-1.3070) Loss_G: 3.2113 (5.0644) D(x): 0.9642 D(G(z)): 0.0160 / 0.0585 Acc: 98.4375 (99.4731)\n",
      "[17/25][94/96] Loss_D: -1.3849 (-1.3071) Loss_G: 4.1232 (5.0639) D(x): 0.9749 D(G(z)): 0.0045 / 0.0240 Acc: 100.0000 (99.4734)\n",
      "[17/25][95/96] Loss_D: -1.3823 (-1.3071) Loss_G: 3.0046 (5.0627) D(x): 0.9866 D(G(z)): 0.0784 / 0.0823 Acc: 100.0000 (99.4737)\n",
      "[18/25][0/96] Loss_D: -1.5211 (-1.3072) Loss_G: 2.9044 (5.0614) D(x): 0.9679 D(G(z)): 0.0066 / 0.0793 Acc: 100.0000 (99.4740)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[18/25][1/96] Loss_D: -1.5296 (-1.3074) Loss_G: 5.0193 (5.0614) D(x): 0.9748 D(G(z)): 0.0064 / 0.0071 Acc: 98.4375 (99.4734)\n",
      "[18/25][2/96] Loss_D: -1.4385 (-1.3075) Loss_G: 2.8699 (5.0602) D(x): 0.9444 D(G(z)): 0.0272 / 0.0593 Acc: 100.0000 (99.4738)\n",
      "[18/25][3/96] Loss_D: -1.3587 (-1.3075) Loss_G: 3.9742 (5.0595) D(x): 0.9943 D(G(z)): 0.1046 / 0.0239 Acc: 98.4375 (99.4732)\n",
      "[18/25][4/96] Loss_D: -1.3498 (-1.3075) Loss_G: 3.1260 (5.0584) D(x): 0.9598 D(G(z)): 0.0108 / 0.0619 Acc: 98.4375 (99.4726)\n",
      "[18/25][5/96] Loss_D: -1.3440 (-1.3075) Loss_G: 4.9430 (5.0583) D(x): 0.9193 D(G(z)): 0.0024 / 0.0117 Acc: 100.0000 (99.4729)\n",
      "[18/25][6/96] Loss_D: -1.4900 (-1.3076) Loss_G: 1.8757 (5.0565) D(x): 0.9799 D(G(z)): 0.0194 / 0.1730 Acc: 100.0000 (99.4732)\n",
      "[18/25][7/96] Loss_D: -1.3966 (-1.3077) Loss_G: 4.7580 (5.0563) D(x): 0.9972 D(G(z)): 0.1307 / 0.0114 Acc: 100.0000 (99.4735)\n",
      "[18/25][8/96] Loss_D: -1.4393 (-1.3078) Loss_G: 7.0045 (5.0575) D(x): 0.9650 D(G(z)): 0.0076 / 0.0011 Acc: 100.0000 (99.4738)\n",
      "[18/25][9/96] Loss_D: -1.4429 (-1.3078) Loss_G: 4.6986 (5.0573) D(x): 0.9396 D(G(z)): 0.0072 / 0.0144 Acc: 100.0000 (99.4741)\n",
      "[18/25][10/96] Loss_D: -1.3959 (-1.3079) Loss_G: 5.6502 (5.0576) D(x): 0.9693 D(G(z)): 0.0093 / 0.0063 Acc: 100.0000 (99.4744)\n",
      "[18/25][11/96] Loss_D: -1.6587 (-1.3081) Loss_G: 4.6684 (5.0574) D(x): 0.9879 D(G(z)): 0.0008 / 0.0093 Acc: 100.0000 (99.4747)\n",
      "[18/25][12/96] Loss_D: -1.3309 (-1.3081) Loss_G: 5.5231 (5.0576) D(x): 0.9929 D(G(z)): 0.0997 / 0.0064 Acc: 100.0000 (99.4750)\n",
      "[18/25][13/96] Loss_D: -1.4427 (-1.3082) Loss_G: 6.4027 (5.0584) D(x): 0.9876 D(G(z)): 0.0132 / 0.0029 Acc: 98.4375 (99.4744)\n",
      "[18/25][14/96] Loss_D: -1.3094 (-1.3082) Loss_G: 4.3818 (5.0580) D(x): 0.9718 D(G(z)): 0.0050 / 0.0250 Acc: 100.0000 (99.4747)\n",
      "[18/25][15/96] Loss_D: -1.2956 (-1.3082) Loss_G: 4.4286 (5.0577) D(x): 0.8890 D(G(z)): 0.0005 / 0.0186 Acc: 100.0000 (99.4750)\n",
      "[18/25][16/96] Loss_D: -1.5048 (-1.3083) Loss_G: 4.2444 (5.0572) D(x): 0.9931 D(G(z)): 0.0578 / 0.0206 Acc: 100.0000 (99.4753)\n",
      "[18/25][17/96] Loss_D: -1.3880 (-1.3083) Loss_G: 3.3672 (5.0562) D(x): 0.9976 D(G(z)): 0.0853 / 0.0564 Acc: 98.4375 (99.4747)\n",
      "[18/25][18/96] Loss_D: -1.4008 (-1.3084) Loss_G: 7.5860 (5.0577) D(x): 0.9904 D(G(z)): 0.0376 / 0.0009 Acc: 100.0000 (99.4750)\n",
      "[18/25][19/96] Loss_D: -1.3785 (-1.3084) Loss_G: 2.6064 (5.0563) D(x): 0.8780 D(G(z)): 0.0063 / 0.0895 Acc: 98.4375 (99.4744)\n",
      "[18/25][20/96] Loss_D: -1.3211 (-1.3084) Loss_G: 5.1974 (5.0564) D(x): 0.9503 D(G(z)): 0.0281 / 0.0100 Acc: 100.0000 (99.4747)\n",
      "[18/25][21/96] Loss_D: -1.3579 (-1.3085) Loss_G: 2.8767 (5.0551) D(x): 0.9938 D(G(z)): 0.0402 / 0.0729 Acc: 100.0000 (99.4750)\n",
      "[18/25][22/96] Loss_D: -1.4031 (-1.3085) Loss_G: 5.1571 (5.0552) D(x): 0.9869 D(G(z)): 0.0610 / 0.0105 Acc: 98.4375 (99.4744)\n",
      "[18/25][23/96] Loss_D: -1.4392 (-1.3086) Loss_G: 7.3355 (5.0565) D(x): 0.9901 D(G(z)): 0.0178 / 0.0010 Acc: 100.0000 (99.4747)\n",
      "[18/25][24/96] Loss_D: -1.4482 (-1.3087) Loss_G: 6.0581 (5.0570) D(x): 0.9690 D(G(z)): 0.0032 / 0.0043 Acc: 100.0000 (99.4750)\n",
      "[18/25][25/96] Loss_D: -1.4569 (-1.3088) Loss_G: 3.4890 (5.0561) D(x): 0.9856 D(G(z)): 0.0122 / 0.0469 Acc: 100.0000 (99.4753)\n",
      "[18/25][26/96] Loss_D: -1.4986 (-1.3089) Loss_G: 5.2683 (5.0563) D(x): 0.9595 D(G(z)): 0.0230 / 0.0076 Acc: 100.0000 (99.4756)\n",
      "[18/25][27/96] Loss_D: -1.3762 (-1.3089) Loss_G: 4.4120 (5.0559) D(x): 0.9630 D(G(z)): 0.0059 / 0.0245 Acc: 98.4375 (99.4750)\n",
      "[18/25][28/96] Loss_D: -1.3773 (-1.3089) Loss_G: 3.5278 (5.0550) D(x): 0.9837 D(G(z)): 0.0262 / 0.0501 Acc: 98.4375 (99.4744)\n",
      "[18/25][29/96] Loss_D: -1.3985 (-1.3090) Loss_G: 5.2971 (5.0552) D(x): 0.9971 D(G(z)): 0.0875 / 0.0059 Acc: 100.0000 (99.4747)\n",
      "[18/25][30/96] Loss_D: -1.4953 (-1.3091) Loss_G: 4.6995 (5.0550) D(x): 0.9336 D(G(z)): 0.0038 / 0.0161 Acc: 100.0000 (99.4750)\n",
      "[18/25][31/96] Loss_D: -1.4845 (-1.3092) Loss_G: 4.8523 (5.0549) D(x): 0.9894 D(G(z)): 0.0046 / 0.0109 Acc: 98.4375 (99.4744)\n",
      "[18/25][32/96] Loss_D: -1.4663 (-1.3093) Loss_G: 5.4318 (5.0551) D(x): 0.9955 D(G(z)): 0.0132 / 0.0094 Acc: 100.0000 (99.4747)\n",
      "[18/25][33/96] Loss_D: -1.3223 (-1.3093) Loss_G: 6.7292 (5.0560) D(x): 0.9784 D(G(z)): 0.1639 / 0.0018 Acc: 100.0000 (99.4750)\n",
      "[18/25][34/96] Loss_D: -1.2983 (-1.3093) Loss_G: 2.8134 (5.0547) D(x): 0.7958 D(G(z)): 0.0043 / 0.0836 Acc: 100.0000 (99.4753)\n",
      "[18/25][35/96] Loss_D: -1.6406 (-1.3095) Loss_G: 1.2772 (5.0526) D(x): 0.9843 D(G(z)): 0.0432 / 0.2208 Acc: 100.0000 (99.4756)\n",
      "[18/25][36/96] Loss_D: -1.1082 (-1.3094) Loss_G: 8.0663 (5.0543) D(x): 0.9983 D(G(z)): 0.2172 / 0.0005 Acc: 100.0000 (99.4759)\n",
      "[18/25][37/96] Loss_D: -1.0762 (-1.3092) Loss_G: 3.6273 (5.0535) D(x): 0.6858 D(G(z)): 0.0004 / 0.0382 Acc: 98.4375 (99.4753)\n",
      "[18/25][38/96] Loss_D: -1.5094 (-1.3093) Loss_G: 1.0748 (5.0513) D(x): 0.9996 D(G(z)): 0.0763 / 0.2933 Acc: 98.4375 (99.4747)\n",
      "[18/25][39/96] Loss_D: -0.9016 (-1.3091) Loss_G: 9.7571 (5.0539) D(x): 0.9972 D(G(z)): 0.3289 / 0.0001 Acc: 100.0000 (99.4750)\n",
      "[18/25][40/96] Loss_D: -1.4606 (-1.3092) Loss_G: 10.3240 (5.0569) D(x): 0.8879 D(G(z)): 0.0002 / 0.0000 Acc: 100.0000 (99.4753)\n",
      "[18/25][41/96] Loss_D: -1.0719 (-1.3091) Loss_G: 5.9873 (5.0574) D(x): 0.7452 D(G(z)): 0.0001 / 0.0042 Acc: 100.0000 (99.4756)\n",
      "[18/25][42/96] Loss_D: -1.4004 (-1.3091) Loss_G: 2.4615 (5.0560) D(x): 0.9988 D(G(z)): 0.0198 / 0.1054 Acc: 100.0000 (99.4759)\n",
      "[18/25][43/96] Loss_D: -1.0792 (-1.3090) Loss_G: 7.0801 (5.0571) D(x): 0.9994 D(G(z)): 0.2578 / 0.0018 Acc: 100.0000 (99.4762)\n",
      "[18/25][44/96] Loss_D: -0.9644 (-1.3088) Loss_G: 1.1299 (5.0549) D(x): 0.6598 D(G(z)): 0.0070 / 0.2871 Acc: 100.0000 (99.4765)\n",
      "[18/25][45/96] Loss_D: -1.3399 (-1.3088) Loss_G: 2.9182 (5.0537) D(x): 0.9998 D(G(z)): 0.1741 / 0.0768 Acc: 98.4375 (99.4759)\n",
      "[18/25][46/96] Loss_D: -1.3932 (-1.3089) Loss_G: 6.4822 (5.0545) D(x): 0.9996 D(G(z)): 0.1042 / 0.0021 Acc: 96.8750 (99.4745)\n",
      "[18/25][47/96] Loss_D: -1.4405 (-1.3089) Loss_G: 10.2618 (5.0574) D(x): 0.9728 D(G(z)): 0.0002 / 0.0001 Acc: 100.0000 (99.4748)\n",
      "[18/25][48/96] Loss_D: -1.4269 (-1.3090) Loss_G: 7.6387 (5.0589) D(x): 0.9668 D(G(z)): 0.0051 / 0.0008 Acc: 98.4375 (99.4742)\n",
      "[18/25][49/96] Loss_D: -1.3972 (-1.3090) Loss_G: 6.2179 (5.0595) D(x): 0.9272 D(G(z)): 0.0005 / 0.0031 Acc: 100.0000 (99.4745)\n",
      "[18/25][50/96] Loss_D: -1.4922 (-1.3091) Loss_G: 4.5798 (5.0592) D(x): 0.9756 D(G(z)): 0.0134 / 0.0145 Acc: 98.4375 (99.4739)\n",
      "[18/25][51/96] Loss_D: -1.3931 (-1.3092) Loss_G: 4.9743 (5.0592) D(x): 0.9989 D(G(z)): 0.0118 / 0.0137 Acc: 98.4375 (99.4733)\n",
      "[18/25][52/96] Loss_D: -1.4321 (-1.3093) Loss_G: 4.3981 (5.0588) D(x): 0.9713 D(G(z)): 0.0064 / 0.0243 Acc: 100.0000 (99.4736)\n",
      "[18/25][53/96] Loss_D: -1.3529 (-1.3093) Loss_G: 4.4637 (5.0585) D(x): 0.9956 D(G(z)): 0.0732 / 0.0150 Acc: 100.0000 (99.4739)\n",
      "[18/25][54/96] Loss_D: -1.4969 (-1.3094) Loss_G: 6.5631 (5.0593) D(x): 0.9927 D(G(z)): 0.0111 / 0.0027 Acc: 98.4375 (99.4733)\n",
      "[18/25][55/96] Loss_D: -1.5696 (-1.3095) Loss_G: 6.5847 (5.0602) D(x): 0.9804 D(G(z)): 0.0029 / 0.0017 Acc: 98.4375 (99.4727)\n",
      "[18/25][56/96] Loss_D: -1.4806 (-1.3096) Loss_G: 7.4805 (5.0615) D(x): 0.9935 D(G(z)): 0.0125 / 0.0009 Acc: 100.0000 (99.4730)\n",
      "[18/25][57/96] Loss_D: -1.3397 (-1.3097) Loss_G: 6.9164 (5.0626) D(x): 0.9887 D(G(z)): 0.0080 / 0.0021 Acc: 100.0000 (99.4733)\n",
      "[18/25][58/96] Loss_D: -1.4289 (-1.3097) Loss_G: 6.5930 (5.0634) D(x): 0.9954 D(G(z)): 0.0039 / 0.0018 Acc: 100.0000 (99.4736)\n",
      "[18/25][59/96] Loss_D: -1.3942 (-1.3098) Loss_G: 6.3304 (5.0642) D(x): 0.9623 D(G(z)): 0.0022 / 0.0024 Acc: 96.8750 (99.4722)\n",
      "[18/25][60/96] Loss_D: -1.5190 (-1.3099) Loss_G: 3.3209 (5.0632) D(x): 0.9726 D(G(z)): 0.0270 / 0.0441 Acc: 100.0000 (99.4725)\n",
      "[18/25][61/96] Loss_D: -1.3529 (-1.3099) Loss_G: 5.2263 (5.0633) D(x): 0.9896 D(G(z)): 0.0406 / 0.0084 Acc: 100.0000 (99.4728)\n",
      "[18/25][62/96] Loss_D: -1.4311 (-1.3100) Loss_G: 5.1304 (5.0633) D(x): 0.9856 D(G(z)): 0.0071 / 0.0093 Acc: 100.0000 (99.4731)\n",
      "[18/25][63/96] Loss_D: -1.4096 (-1.3100) Loss_G: 6.4477 (5.0641) D(x): 0.9807 D(G(z)): 0.0497 / 0.0024 Acc: 98.4375 (99.4725)\n",
      "[18/25][64/96] Loss_D: -1.5595 (-1.3102) Loss_G: 5.2947 (5.0642) D(x): 0.9973 D(G(z)): 0.0003 / 0.0078 Acc: 100.0000 (99.4728)\n",
      "[18/25][65/96] Loss_D: -1.4433 (-1.3102) Loss_G: 7.3557 (5.0655) D(x): 0.9776 D(G(z)): 0.0169 / 0.0014 Acc: 100.0000 (99.4731)\n",
      "[18/25][66/96] Loss_D: -1.4443 (-1.3103) Loss_G: 5.3532 (5.0656) D(x): 0.9603 D(G(z)): 0.0082 / 0.0072 Acc: 100.0000 (99.4734)\n",
      "[18/25][67/96] Loss_D: -1.5267 (-1.3104) Loss_G: 3.8277 (5.0650) D(x): 0.9788 D(G(z)): 0.0132 / 0.0320 Acc: 100.0000 (99.4737)\n",
      "[18/25][68/96] Loss_D: -1.3514 (-1.3105) Loss_G: 5.4513 (5.0652) D(x): 0.9896 D(G(z)): 0.0668 / 0.0061 Acc: 100.0000 (99.4739)\n",
      "[18/25][69/96] Loss_D: -1.4236 (-1.3105) Loss_G: 8.3670 (5.0670) D(x): 0.9869 D(G(z)): 0.0006 / 0.0003 Acc: 100.0000 (99.4742)\n",
      "[18/25][70/96] Loss_D: -1.5338 (-1.3106) Loss_G: 7.3038 (5.0683) D(x): 0.9752 D(G(z)): 0.0025 / 0.0010 Acc: 100.0000 (99.4745)\n",
      "[18/25][71/96] Loss_D: -1.5125 (-1.3108) Loss_G: 3.6602 (5.0675) D(x): 0.9686 D(G(z)): 0.0008 / 0.0308 Acc: 100.0000 (99.4748)\n",
      "[18/25][72/96] Loss_D: -1.3873 (-1.3108) Loss_G: 2.4718 (5.0660) D(x): 0.9676 D(G(z)): 0.0142 / 0.0967 Acc: 100.0000 (99.4751)\n",
      "[18/25][73/96] Loss_D: -1.3426 (-1.3108) Loss_G: 6.9018 (5.0670) D(x): 0.9952 D(G(z)): 0.1001 / 0.0012 Acc: 100.0000 (99.4754)\n",
      "[18/25][74/96] Loss_D: -1.4557 (-1.3109) Loss_G: 7.5800 (5.0684) D(x): 0.9759 D(G(z)): 0.0035 / 0.0008 Acc: 100.0000 (99.4757)\n",
      "[18/25][75/96] Loss_D: -1.4956 (-1.3110) Loss_G: 5.9915 (5.0690) D(x): 0.9852 D(G(z)): 0.0043 / 0.0034 Acc: 98.4375 (99.4751)\n",
      "[18/25][76/96] Loss_D: -1.4979 (-1.3111) Loss_G: 3.3328 (5.0680) D(x): 0.9132 D(G(z)): 0.0117 / 0.0403 Acc: 100.0000 (99.4754)\n",
      "[18/25][77/96] Loss_D: -1.4399 (-1.3112) Loss_G: 2.2236 (5.0664) D(x): 0.9923 D(G(z)): 0.0204 / 0.1162 Acc: 100.0000 (99.4757)\n",
      "[18/25][78/96] Loss_D: -1.4071 (-1.3112) Loss_G: 5.2704 (5.0665) D(x): 0.9919 D(G(z)): 0.0213 / 0.0097 Acc: 100.0000 (99.4760)\n",
      "[18/25][79/96] Loss_D: -1.4297 (-1.3113) Loss_G: 4.5092 (5.0662) D(x): 0.9870 D(G(z)): 0.0528 / 0.0131 Acc: 98.4375 (99.4754)\n",
      "[18/25][80/96] Loss_D: -1.4386 (-1.3114) Loss_G: 3.3651 (5.0653) D(x): 0.9917 D(G(z)): 0.0060 / 0.0436 Acc: 100.0000 (99.4757)\n",
      "[18/25][81/96] Loss_D: -1.5228 (-1.3115) Loss_G: 5.5644 (5.0656) D(x): 0.9818 D(G(z)): 0.0050 / 0.0048 Acc: 100.0000 (99.4760)\n",
      "[18/25][82/96] Loss_D: -1.2574 (-1.3115) Loss_G: 2.6897 (5.0642) D(x): 0.9173 D(G(z)): 0.0168 / 0.0891 Acc: 100.0000 (99.4763)\n",
      "[18/25][83/96] Loss_D: -1.5675 (-1.3116) Loss_G: 2.9770 (5.0631) D(x): 0.9933 D(G(z)): 0.0038 / 0.0547 Acc: 100.0000 (99.4766)\n",
      "[18/25][84/96] Loss_D: -1.3280 (-1.3116) Loss_G: 6.5474 (5.0639) D(x): 0.9993 D(G(z)): 0.1892 / 0.0018 Acc: 100.0000 (99.4769)\n",
      "[18/25][85/96] Loss_D: -1.4210 (-1.3117) Loss_G: 6.6934 (5.0648) D(x): 0.9196 D(G(z)): 0.0009 / 0.0020 Acc: 98.4375 (99.4763)\n",
      "[18/25][86/96] Loss_D: -1.2915 (-1.3117) Loss_G: 4.8331 (5.0647) D(x): 0.9384 D(G(z)): 0.0124 / 0.0152 Acc: 100.0000 (99.4766)\n",
      "[18/25][87/96] Loss_D: -1.3639 (-1.3117) Loss_G: 3.9733 (5.0641) D(x): 0.9265 D(G(z)): 0.0078 / 0.0362 Acc: 100.0000 (99.4769)\n",
      "[18/25][88/96] Loss_D: -1.3387 (-1.3117) Loss_G: 3.8616 (5.0634) D(x): 0.9935 D(G(z)): 0.0135 / 0.0369 Acc: 98.4375 (99.4763)\n",
      "[18/25][89/96] Loss_D: -1.4810 (-1.3118) Loss_G: 4.7676 (5.0633) D(x): 0.9986 D(G(z)): 0.0302 / 0.0149 Acc: 98.4375 (99.4757)\n",
      "[18/25][90/96] Loss_D: -1.5479 (-1.3119) Loss_G: 2.9238 (5.0621) D(x): 0.9976 D(G(z)): 0.0248 / 0.0592 Acc: 98.4375 (99.4752)\n",
      "[18/25][91/96] Loss_D: -1.3384 (-1.3119) Loss_G: 5.4140 (5.0623) D(x): 0.9861 D(G(z)): 0.0994 / 0.0083 Acc: 98.4375 (99.4746)\n",
      "[18/25][92/96] Loss_D: -0.2447 (-1.3113) Loss_G: -0.4989 (5.0592) D(x): 0.3943 D(G(z)): 0.0009 / 0.9989 Acc: 100.0000 (99.4749)\n",
      "[18/25][93/96] Loss_D: 6.6872 (-1.3070) Loss_G: 1.5064 (5.0573) D(x): 1.0000 D(G(z)): 0.9992 / 0.2146 Acc: 100.0000 (99.4752)\n",
      "[18/25][94/96] Loss_D: -1.0297 (-1.3068) Loss_G: 3.6609 (5.0565) D(x): 0.8831 D(G(z)): 0.2468 / 0.0439 Acc: 100.0000 (99.4755)\n",
      "[18/25][95/96] Loss_D: -0.2478 (-1.3062) Loss_G: 1.6512 (5.0546) D(x): 0.4449 D(G(z)): 0.0406 / 0.2249 Acc: 100.0000 (99.4757)\n",
      "[19/25][0/96] Loss_D: -1.0868 (-1.3061) Loss_G: 1.5654 (5.0527) D(x): 0.8271 D(G(z)): 0.1831 / 0.2101 Acc: 100.0000 (99.4760)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[19/25][1/96] Loss_D: -0.6905 (-1.3058) Loss_G: 5.2489 (5.0528) D(x): 0.9575 D(G(z)): 0.4473 / 0.0059 Acc: 100.0000 (99.4763)\n",
      "[19/25][2/96] Loss_D: -0.4504 (-1.3053) Loss_G: 2.3851 (5.0514) D(x): 0.3987 D(G(z)): 0.0035 / 0.0949 Acc: 100.0000 (99.4766)\n",
      "[19/25][3/96] Loss_D: -1.5134 (-1.3054) Loss_G: 0.4076 (5.0488) D(x): 0.9926 D(G(z)): 0.0390 / 0.4603 Acc: 100.0000 (99.4769)\n",
      "[19/25][4/96] Loss_D: -0.7065 (-1.3051) Loss_G: 4.3893 (5.0485) D(x): 0.9949 D(G(z)): 0.4285 / 0.0155 Acc: 100.0000 (99.4772)\n",
      "[19/25][5/96] Loss_D: -1.3967 (-1.3051) Loss_G: 5.9732 (5.0490) D(x): 0.9497 D(G(z)): 0.0164 / 0.0046 Acc: 100.0000 (99.4775)\n",
      "[19/25][6/96] Loss_D: -1.3674 (-1.3052) Loss_G: 5.1658 (5.0490) D(x): 0.9508 D(G(z)): 0.0134 / 0.0103 Acc: 100.0000 (99.4777)\n",
      "[19/25][7/96] Loss_D: -1.4854 (-1.3053) Loss_G: 5.6139 (5.0493) D(x): 0.9044 D(G(z)): 0.0090 / 0.0041 Acc: 100.0000 (99.4780)\n",
      "[19/25][8/96] Loss_D: -1.3028 (-1.3053) Loss_G: 3.5934 (5.0486) D(x): 0.9632 D(G(z)): 0.0300 / 0.0403 Acc: 100.0000 (99.4783)\n",
      "[19/25][9/96] Loss_D: -1.4722 (-1.3054) Loss_G: 4.3510 (5.0482) D(x): 0.9904 D(G(z)): 0.0676 / 0.0134 Acc: 98.4375 (99.4777)\n",
      "[19/25][10/96] Loss_D: -1.4640 (-1.3054) Loss_G: 4.5830 (5.0479) D(x): 0.9799 D(G(z)): 0.0140 / 0.0137 Acc: 98.4375 (99.4772)\n",
      "[19/25][11/96] Loss_D: -1.5129 (-1.3056) Loss_G: 5.2253 (5.0480) D(x): 0.9375 D(G(z)): 0.0092 / 0.0070 Acc: 98.4375 (99.4766)\n",
      "[19/25][12/96] Loss_D: -1.5181 (-1.3057) Loss_G: 4.2564 (5.0476) D(x): 0.9897 D(G(z)): 0.0176 / 0.0180 Acc: 98.4375 (99.4760)\n",
      "[19/25][13/96] Loss_D: -1.3584 (-1.3057) Loss_G: 2.0198 (5.0459) D(x): 0.9952 D(G(z)): 0.0258 / 0.1624 Acc: 98.4375 (99.4755)\n",
      "[19/25][14/96] Loss_D: -1.3512 (-1.3057) Loss_G: 3.2831 (5.0450) D(x): 0.9943 D(G(z)): 0.0023 / 0.0546 Acc: 98.4375 (99.4749)\n",
      "[19/25][15/96] Loss_D: -1.4564 (-1.3058) Loss_G: 4.7722 (5.0448) D(x): 0.9908 D(G(z)): 0.0603 / 0.0097 Acc: 100.0000 (99.4752)\n",
      "[19/25][16/96] Loss_D: -1.4168 (-1.3059) Loss_G: 6.6101 (5.0457) D(x): 0.9459 D(G(z)): 0.0076 / 0.0021 Acc: 100.0000 (99.4755)\n",
      "[19/25][17/96] Loss_D: -1.4397 (-1.3059) Loss_G: 4.8242 (5.0456) D(x): 0.9868 D(G(z)): 0.0425 / 0.0136 Acc: 98.4375 (99.4749)\n",
      "[19/25][18/96] Loss_D: -1.3771 (-1.3060) Loss_G: 3.8859 (5.0449) D(x): 0.9323 D(G(z)): 0.0014 / 0.0257 Acc: 100.0000 (99.4752)\n",
      "[19/25][19/96] Loss_D: -1.5337 (-1.3061) Loss_G: 2.5954 (5.0436) D(x): 0.9919 D(G(z)): 0.0321 / 0.0798 Acc: 100.0000 (99.4755)\n",
      "[19/25][20/96] Loss_D: -1.5585 (-1.3062) Loss_G: 2.5226 (5.0422) D(x): 0.9918 D(G(z)): 0.0107 / 0.1004 Acc: 100.0000 (99.4758)\n",
      "[19/25][21/96] Loss_D: -1.5623 (-1.3064) Loss_G: 4.7459 (5.0421) D(x): 0.9846 D(G(z)): 0.0302 / 0.0109 Acc: 100.0000 (99.4761)\n",
      "[19/25][22/96] Loss_D: -1.5418 (-1.3065) Loss_G: 3.6162 (5.0413) D(x): 0.9839 D(G(z)): 0.0184 / 0.0374 Acc: 100.0000 (99.4763)\n",
      "[19/25][23/96] Loss_D: -1.4233 (-1.3066) Loss_G: 3.4153 (5.0404) D(x): 0.8935 D(G(z)): 0.0102 / 0.0365 Acc: 100.0000 (99.4766)\n",
      "[19/25][24/96] Loss_D: -1.3799 (-1.3066) Loss_G: 3.8102 (5.0398) D(x): 0.9988 D(G(z)): 0.0987 / 0.0252 Acc: 100.0000 (99.4769)\n",
      "[19/25][25/96] Loss_D: -1.4853 (-1.3067) Loss_G: 4.9247 (5.0397) D(x): 0.9842 D(G(z)): 0.0274 / 0.0098 Acc: 100.0000 (99.4772)\n",
      "[19/25][26/96] Loss_D: -1.4977 (-1.3068) Loss_G: 5.2625 (5.0398) D(x): 0.9926 D(G(z)): 0.0102 / 0.0065 Acc: 100.0000 (99.4775)\n",
      "[19/25][27/96] Loss_D: -1.4220 (-1.3069) Loss_G: 4.5752 (5.0396) D(x): 0.9323 D(G(z)): 0.0020 / 0.0203 Acc: 100.0000 (99.4778)\n",
      "[19/25][28/96] Loss_D: -1.5651 (-1.3070) Loss_G: 2.6678 (5.0383) D(x): 0.9860 D(G(z)): 0.0139 / 0.0778 Acc: 100.0000 (99.4780)\n",
      "[19/25][29/96] Loss_D: -1.3332 (-1.3070) Loss_G: 6.6741 (5.0392) D(x): 0.9697 D(G(z)): 0.0367 / 0.0036 Acc: 100.0000 (99.4783)\n",
      "[19/25][30/96] Loss_D: -1.3117 (-1.3070) Loss_G: 1.8239 (5.0374) D(x): 0.9441 D(G(z)): 0.0543 / 0.1881 Acc: 100.0000 (99.4786)\n",
      "[19/25][31/96] Loss_D: -1.5813 (-1.3072) Loss_G: 2.9748 (5.0363) D(x): 0.9993 D(G(z)): 0.0252 / 0.0589 Acc: 98.4375 (99.4780)\n",
      "[19/25][32/96] Loss_D: -1.4278 (-1.3072) Loss_G: 3.4991 (5.0355) D(x): 0.9645 D(G(z)): 0.0032 / 0.0431 Acc: 98.4375 (99.4775)\n",
      "[19/25][33/96] Loss_D: -1.2897 (-1.3072) Loss_G: 4.0515 (5.0350) D(x): 0.9931 D(G(z)): 0.0967 / 0.0307 Acc: 100.0000 (99.4778)\n",
      "[19/25][34/96] Loss_D: -1.4763 (-1.3073) Loss_G: 5.8162 (5.0354) D(x): 0.9593 D(G(z)): 0.0120 / 0.0043 Acc: 100.0000 (99.4780)\n",
      "[19/25][35/96] Loss_D: -1.4255 (-1.3074) Loss_G: 5.0913 (5.0354) D(x): 0.9019 D(G(z)): 0.0066 / 0.0102 Acc: 100.0000 (99.4783)\n",
      "[19/25][36/96] Loss_D: -1.0070 (-1.3072) Loss_G: 11.8737 (5.0391) D(x): 0.9933 D(G(z)): 0.3281 / 0.0000 Acc: 100.0000 (99.4786)\n",
      "[19/25][37/96] Loss_D: 0.3224 (-1.3063) Loss_G: -0.4384 (5.0361) D(x): 0.2494 D(G(z)): 0.0000 / 0.9709 Acc: 96.8750 (99.4772)\n",
      "[19/25][38/96] Loss_D: 0.2312 (-1.3055) Loss_G: 6.4485 (5.0369) D(x): 1.0000 D(G(z)): 0.7040 / 0.0020 Acc: 98.4375 (99.4767)\n",
      "[19/25][39/96] Loss_D: -1.5340 (-1.3056) Loss_G: 8.7692 (5.0389) D(x): 0.9916 D(G(z)): 0.0043 / 0.0002 Acc: 98.4375 (99.4761)\n",
      "[19/25][40/96] Loss_D: -1.2444 (-1.3056) Loss_G: 6.2220 (5.0395) D(x): 0.8056 D(G(z)): 0.0006 / 0.0036 Acc: 100.0000 (99.4764)\n",
      "[19/25][41/96] Loss_D: -1.4817 (-1.3057) Loss_G: 4.0547 (5.0390) D(x): 0.9578 D(G(z)): 0.0039 / 0.0271 Acc: 100.0000 (99.4767)\n",
      "[19/25][42/96] Loss_D: -1.4844 (-1.3058) Loss_G: 4.3101 (5.0386) D(x): 0.9744 D(G(z)): 0.0337 / 0.0180 Acc: 100.0000 (99.4769)\n",
      "[19/25][43/96] Loss_D: -1.4609 (-1.3059) Loss_G: 3.5345 (5.0378) D(x): 0.9756 D(G(z)): 0.0276 / 0.0357 Acc: 100.0000 (99.4772)\n",
      "[19/25][44/96] Loss_D: -1.5585 (-1.3060) Loss_G: 4.6712 (5.0376) D(x): 0.9937 D(G(z)): 0.0130 / 0.0110 Acc: 100.0000 (99.4775)\n",
      "[19/25][45/96] Loss_D: -1.4201 (-1.3061) Loss_G: 4.3667 (5.0373) D(x): 0.9905 D(G(z)): 0.0767 / 0.0164 Acc: 100.0000 (99.4778)\n",
      "[19/25][46/96] Loss_D: -1.4832 (-1.3062) Loss_G: 5.1763 (5.0373) D(x): 0.9591 D(G(z)): 0.0038 / 0.0071 Acc: 98.4375 (99.4772)\n",
      "[19/25][47/96] Loss_D: -1.3646 (-1.3062) Loss_G: 4.3470 (5.0370) D(x): 0.9884 D(G(z)): 0.0407 / 0.0219 Acc: 100.0000 (99.4775)\n",
      "[19/25][48/96] Loss_D: -1.4460 (-1.3063) Loss_G: 4.6886 (5.0368) D(x): 0.9690 D(G(z)): 0.0050 / 0.0116 Acc: 100.0000 (99.4778)\n",
      "[19/25][49/96] Loss_D: -1.3964 (-1.3063) Loss_G: 4.6714 (5.0366) D(x): 0.9324 D(G(z)): 0.0093 / 0.0151 Acc: 98.4375 (99.4772)\n",
      "[19/25][50/96] Loss_D: -1.2932 (-1.3063) Loss_G: 6.6258 (5.0374) D(x): 0.9983 D(G(z)): 0.1093 / 0.0020 Acc: 100.0000 (99.4775)\n",
      "[19/25][51/96] Loss_D: -1.3220 (-1.3063) Loss_G: 2.6333 (5.0362) D(x): 0.8449 D(G(z)): 0.0137 / 0.0968 Acc: 98.4375 (99.4769)\n",
      "[19/25][52/96] Loss_D: -1.4247 (-1.3064) Loss_G: 1.8796 (5.0345) D(x): 0.9892 D(G(z)): 0.0305 / 0.1843 Acc: 100.0000 (99.4772)\n",
      "[19/25][53/96] Loss_D: -1.4600 (-1.3065) Loss_G: 3.7530 (5.0338) D(x): 0.9980 D(G(z)): 0.0212 / 0.0290 Acc: 100.0000 (99.4775)\n",
      "[19/25][54/96] Loss_D: -1.3554 (-1.3065) Loss_G: 2.4006 (5.0324) D(x): 0.9993 D(G(z)): 0.0458 / 0.1053 Acc: 95.3125 (99.4753)\n",
      "[19/25][55/96] Loss_D: -1.3686 (-1.3065) Loss_G: 2.6309 (5.0311) D(x): 0.9799 D(G(z)): 0.0012 / 0.0967 Acc: 98.4375 (99.4747)\n",
      "[19/25][56/96] Loss_D: -1.5853 (-1.3067) Loss_G: 5.4431 (5.0313) D(x): 0.9917 D(G(z)): 0.0152 / 0.0048 Acc: 100.0000 (99.4750)\n",
      "[19/25][57/96] Loss_D: -1.3279 (-1.3067) Loss_G: 7.5024 (5.0326) D(x): 0.9806 D(G(z)): 0.0753 / 0.0009 Acc: 100.0000 (99.4753)\n",
      "[19/25][58/96] Loss_D: -1.4466 (-1.3068) Loss_G: 6.4639 (5.0334) D(x): 0.9629 D(G(z)): 0.0132 / 0.0018 Acc: 98.4375 (99.4747)\n",
      "[19/25][59/96] Loss_D: -1.2840 (-1.3068) Loss_G: 4.0736 (5.0329) D(x): 0.8956 D(G(z)): 0.0027 / 0.0209 Acc: 98.4375 (99.4742)\n",
      "[19/25][60/96] Loss_D: -1.6048 (-1.3069) Loss_G: 3.4483 (5.0321) D(x): 0.9978 D(G(z)): 0.0023 / 0.0391 Acc: 98.4375 (99.4736)\n",
      "[19/25][61/96] Loss_D: -1.2028 (-1.3069) Loss_G: 5.1627 (5.0321) D(x): 0.9921 D(G(z)): 0.1920 / 0.0091 Acc: 100.0000 (99.4739)\n",
      "[19/25][62/96] Loss_D: -1.1514 (-1.3068) Loss_G: 4.1377 (5.0317) D(x): 0.8044 D(G(z)): 0.0458 / 0.0227 Acc: 100.0000 (99.4742)\n",
      "[19/25][63/96] Loss_D: -1.5729 (-1.3069) Loss_G: 2.7456 (5.0304) D(x): 0.9934 D(G(z)): 0.0138 / 0.0652 Acc: 100.0000 (99.4745)\n",
      "[19/25][64/96] Loss_D: -1.2942 (-1.3069) Loss_G: 4.1347 (5.0300) D(x): 0.9803 D(G(z)): 0.0995 / 0.0222 Acc: 98.4375 (99.4739)\n",
      "[19/25][65/96] Loss_D: -1.4826 (-1.3070) Loss_G: 7.5175 (5.0313) D(x): 0.9964 D(G(z)): 0.0134 / 0.0009 Acc: 100.0000 (99.4742)\n",
      "[19/25][66/96] Loss_D: -1.4162 (-1.3071) Loss_G: 7.1977 (5.0324) D(x): 0.9774 D(G(z)): 0.0134 / 0.0013 Acc: 98.4375 (99.4737)\n",
      "[19/25][67/96] Loss_D: -0.8138 (-1.3068) Loss_G: -0.3187 (5.0296) D(x): 0.5963 D(G(z)): 0.0004 / 0.8479 Acc: 100.0000 (99.4739)\n",
      "[19/25][68/96] Loss_D: 1.7269 (-1.3052) Loss_G: 8.8858 (5.0316) D(x): 1.0000 D(G(z)): 0.9256 / 0.0002 Acc: 98.4375 (99.4734)\n",
      "[19/25][69/96] Loss_D: -0.6139 (-1.3048) Loss_G: 5.8523 (5.0321) D(x): 0.5275 D(G(z)): 0.0002 / 0.0043 Acc: 100.0000 (99.4737)\n",
      "[19/25][70/96] Loss_D: -0.5483 (-1.3044) Loss_G: -0.3386 (5.0292) D(x): 0.4769 D(G(z)): 0.0193 / 0.8293 Acc: 100.0000 (99.4739)\n",
      "[19/25][71/96] Loss_D: 1.2173 (-1.3031) Loss_G: 3.2877 (5.0283) D(x): 0.9997 D(G(z)): 0.8915 / 0.0412 Acc: 98.4375 (99.4734)\n",
      "[19/25][72/96] Loss_D: -1.3810 (-1.3031) Loss_G: 6.8495 (5.0293) D(x): 0.9591 D(G(z)): 0.0228 / 0.0026 Acc: 100.0000 (99.4737)\n",
      "[19/25][73/96] Loss_D: -0.9957 (-1.3030) Loss_G: 4.5718 (5.0290) D(x): 0.7054 D(G(z)): 0.0068 / 0.0129 Acc: 100.0000 (99.4740)\n",
      "[19/25][74/96] Loss_D: -1.4694 (-1.3031) Loss_G: 2.6428 (5.0278) D(x): 0.9198 D(G(z)): 0.0205 / 0.0857 Acc: 100.0000 (99.4742)\n",
      "[19/25][75/96] Loss_D: -1.3932 (-1.3031) Loss_G: 3.0987 (5.0268) D(x): 0.9920 D(G(z)): 0.1116 / 0.0546 Acc: 100.0000 (99.4745)\n",
      "[19/25][76/96] Loss_D: -1.4216 (-1.3032) Loss_G: 3.9527 (5.0262) D(x): 0.9625 D(G(z)): 0.0473 / 0.0267 Acc: 100.0000 (99.4748)\n",
      "[19/25][77/96] Loss_D: -1.5207 (-1.3033) Loss_G: 5.0185 (5.0262) D(x): 0.9870 D(G(z)): 0.0272 / 0.0132 Acc: 100.0000 (99.4751)\n",
      "[19/25][78/96] Loss_D: -1.4964 (-1.3034) Loss_G: 3.2245 (5.0252) D(x): 0.9882 D(G(z)): 0.0222 / 0.0737 Acc: 100.0000 (99.4753)\n",
      "[19/25][79/96] Loss_D: -1.3679 (-1.3034) Loss_G: 3.6308 (5.0245) D(x): 0.9610 D(G(z)): 0.0257 / 0.0365 Acc: 98.4375 (99.4748)\n",
      "[19/25][80/96] Loss_D: -1.3944 (-1.3035) Loss_G: 5.3399 (5.0247) D(x): 0.9908 D(G(z)): 0.0455 / 0.0080 Acc: 100.0000 (99.4751)\n",
      "[19/25][81/96] Loss_D: -1.3129 (-1.3035) Loss_G: 4.5275 (5.0244) D(x): 0.9055 D(G(z)): 0.0264 / 0.0212 Acc: 100.0000 (99.4753)\n",
      "[19/25][82/96] Loss_D: -1.4747 (-1.3036) Loss_G: 3.3954 (5.0236) D(x): 0.9721 D(G(z)): 0.0114 / 0.0433 Acc: 100.0000 (99.4756)\n",
      "[19/25][83/96] Loss_D: -1.1558 (-1.3035) Loss_G: 5.5945 (5.0239) D(x): 0.9882 D(G(z)): 0.2339 / 0.0056 Acc: 98.4375 (99.4751)\n",
      "[19/25][84/96] Loss_D: -0.5358 (-1.3031) Loss_G: 0.7661 (5.0216) D(x): 0.4962 D(G(z)): 0.0020 / 0.3997 Acc: 100.0000 (99.4753)\n",
      "[19/25][85/96] Loss_D: -0.7304 (-1.3028) Loss_G: 5.3503 (5.0218) D(x): 0.9965 D(G(z)): 0.4736 / 0.0080 Acc: 100.0000 (99.4756)\n",
      "[19/25][86/96] Loss_D: -1.4447 (-1.3029) Loss_G: 7.5790 (5.0231) D(x): 0.9776 D(G(z)): 0.0149 / 0.0007 Acc: 100.0000 (99.4759)\n",
      "[19/25][87/96] Loss_D: -1.3211 (-1.3029) Loss_G: 8.5573 (5.0250) D(x): 0.9238 D(G(z)): 0.0004 / 0.0003 Acc: 100.0000 (99.4762)\n",
      "[19/25][88/96] Loss_D: -1.1733 (-1.3028) Loss_G: 2.2878 (5.0236) D(x): 0.7661 D(G(z)): 0.0019 / 0.1010 Acc: 100.0000 (99.4764)\n",
      "[19/25][89/96] Loss_D: -1.4362 (-1.3029) Loss_G: 0.3162 (5.0211) D(x): 0.9928 D(G(z)): 0.0389 / 0.5289 Acc: 100.0000 (99.4767)\n",
      "[19/25][90/96] Loss_D: -0.7011 (-1.3026) Loss_G: 6.7734 (5.0220) D(x): 0.9965 D(G(z)): 0.4573 / 0.0022 Acc: 100.0000 (99.4770)\n",
      "[19/25][91/96] Loss_D: -0.6941 (-1.3023) Loss_G: 2.8667 (5.0209) D(x): 0.5193 D(G(z)): 0.0066 / 0.0831 Acc: 100.0000 (99.4773)\n",
      "[19/25][92/96] Loss_D: -1.4295 (-1.3023) Loss_G: 1.1355 (5.0189) D(x): 0.9897 D(G(z)): 0.0415 / 0.3013 Acc: 100.0000 (99.4775)\n",
      "[19/25][93/96] Loss_D: -1.1297 (-1.3022) Loss_G: 4.2360 (5.0185) D(x): 0.9996 D(G(z)): 0.2920 / 0.0270 Acc: 100.0000 (99.4778)\n",
      "[19/25][94/96] Loss_D: -1.4836 (-1.3023) Loss_G: 6.0994 (5.0190) D(x): 0.9804 D(G(z)): 0.0110 / 0.0033 Acc: 100.0000 (99.4781)\n",
      "[19/25][95/96] Loss_D: -1.4409 (-1.3024) Loss_G: 6.1561 (5.0196) D(x): 0.9471 D(G(z)): 0.0004 / 0.0043 Acc: 98.4375 (99.4775)\n",
      "[20/25][0/96] Loss_D: -1.2485 (-1.3024) Loss_G: 4.6650 (5.0194) D(x): 0.8496 D(G(z)): 0.0250 / 0.0114 Acc: 96.8750 (99.4762)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[20/25][1/96] Loss_D: -1.3937 (-1.3024) Loss_G: 2.4825 (5.0181) D(x): 0.9529 D(G(z)): 0.0370 / 0.0988 Acc: 100.0000 (99.4765)\n",
      "[20/25][2/96] Loss_D: -1.3333 (-1.3024) Loss_G: 2.7557 (5.0169) D(x): 0.9981 D(G(z)): 0.0509 / 0.1017 Acc: 100.0000 (99.4767)\n",
      "[20/25][3/96] Loss_D: -1.4533 (-1.3025) Loss_G: 4.7753 (5.0168) D(x): 0.9973 D(G(z)): 0.0664 / 0.0112 Acc: 100.0000 (99.4770)\n",
      "[20/25][4/96] Loss_D: -1.5053 (-1.3026) Loss_G: 3.9899 (5.0163) D(x): 0.9510 D(G(z)): 0.0325 / 0.0233 Acc: 100.0000 (99.4773)\n",
      "[20/25][5/96] Loss_D: -1.4203 (-1.3027) Loss_G: 5.3524 (5.0164) D(x): 0.9764 D(G(z)): 0.0637 / 0.0062 Acc: 100.0000 (99.4775)\n",
      "[20/25][6/96] Loss_D: -1.1410 (-1.3026) Loss_G: 3.6575 (5.0157) D(x): 0.7731 D(G(z)): 0.0115 / 0.0292 Acc: 100.0000 (99.4778)\n",
      "[20/25][7/96] Loss_D: -1.2772 (-1.3026) Loss_G: 2.7165 (5.0146) D(x): 0.9972 D(G(z)): 0.1053 / 0.0805 Acc: 100.0000 (99.4781)\n",
      "[20/25][8/96] Loss_D: -1.3203 (-1.3026) Loss_G: 3.9832 (5.0140) D(x): 0.9985 D(G(z)): 0.1218 / 0.0249 Acc: 100.0000 (99.4784)\n",
      "[20/25][9/96] Loss_D: -1.3949 (-1.3026) Loss_G: 5.8889 (5.0145) D(x): 0.9791 D(G(z)): 0.0053 / 0.0050 Acc: 100.0000 (99.4786)\n",
      "[20/25][10/96] Loss_D: -1.5593 (-1.3028) Loss_G: 5.7615 (5.0149) D(x): 0.9842 D(G(z)): 0.0177 / 0.0046 Acc: 100.0000 (99.4789)\n",
      "[20/25][11/96] Loss_D: -1.3308 (-1.3028) Loss_G: 4.1328 (5.0144) D(x): 0.8989 D(G(z)): 0.0027 / 0.0213 Acc: 98.4375 (99.4784)\n",
      "[20/25][12/96] Loss_D: -1.4763 (-1.3029) Loss_G: 2.9524 (5.0133) D(x): 0.9886 D(G(z)): 0.0652 / 0.0536 Acc: 100.0000 (99.4786)\n",
      "[20/25][13/96] Loss_D: -1.0952 (-1.3028) Loss_G: 7.5998 (5.0147) D(x): 0.9704 D(G(z)): 0.2770 / 0.0007 Acc: 100.0000 (99.4789)\n",
      "[20/25][14/96] Loss_D: -0.8930 (-1.3026) Loss_G: 5.5517 (5.0149) D(x): 0.6827 D(G(z)): 0.0006 / 0.0059 Acc: 100.0000 (99.4792)\n",
      "[20/25][15/96] Loss_D: -1.4201 (-1.3026) Loss_G: 1.2482 (5.0130) D(x): 0.8743 D(G(z)): 0.0049 / 0.2603 Acc: 98.4375 (99.4786)\n",
      "[20/25][16/96] Loss_D: -1.1794 (-1.3025) Loss_G: 2.9939 (5.0120) D(x): 0.9948 D(G(z)): 0.2045 / 0.0659 Acc: 100.0000 (99.4789)\n",
      "[20/25][17/96] Loss_D: -1.3462 (-1.3026) Loss_G: 4.3641 (5.0116) D(x): 0.9625 D(G(z)): 0.0998 / 0.0222 Acc: 98.4375 (99.4784)\n",
      "[20/25][18/96] Loss_D: -1.3972 (-1.3026) Loss_G: 4.9922 (5.0116) D(x): 0.9392 D(G(z)): 0.0130 / 0.0104 Acc: 98.4375 (99.4778)\n",
      "[20/25][19/96] Loss_D: -1.1817 (-1.3026) Loss_G: 3.0070 (5.0106) D(x): 0.7644 D(G(z)): 0.0006 / 0.0637 Acc: 100.0000 (99.4781)\n",
      "[20/25][20/96] Loss_D: -1.1121 (-1.3025) Loss_G: 4.8450 (5.0105) D(x): 0.9932 D(G(z)): 0.3373 / 0.0103 Acc: 100.0000 (99.4784)\n",
      "[20/25][21/96] Loss_D: -1.3214 (-1.3025) Loss_G: 5.5423 (5.0108) D(x): 0.9481 D(G(z)): 0.0402 / 0.0072 Acc: 100.0000 (99.4786)\n",
      "[20/25][22/96] Loss_D: -0.8722 (-1.3022) Loss_G: 1.0401 (5.0087) D(x): 0.6507 D(G(z)): 0.0033 / 0.3385 Acc: 100.0000 (99.4789)\n",
      "[20/25][23/96] Loss_D: -1.3508 (-1.3023) Loss_G: 0.3563 (5.0063) D(x): 0.9973 D(G(z)): 0.1262 / 0.4956 Acc: 96.8750 (99.4776)\n",
      "[20/25][24/96] Loss_D: -0.9238 (-1.3021) Loss_G: 6.7745 (5.0072) D(x): 0.9993 D(G(z)): 0.3959 / 0.0013 Acc: 100.0000 (99.4778)\n",
      "[20/25][25/96] Loss_D: -1.1876 (-1.3020) Loss_G: 8.1319 (5.0088) D(x): 0.7746 D(G(z)): 0.0024 / 0.0003 Acc: 96.8750 (99.4765)\n",
      "[20/25][26/96] Loss_D: -1.0449 (-1.3019) Loss_G: 1.5020 (5.0070) D(x): 0.7330 D(G(z)): 0.0044 / 0.2445 Acc: 100.0000 (99.4768)\n",
      "[20/25][27/96] Loss_D: -0.4082 (-1.3014) Loss_G: 6.6431 (5.0079) D(x): 0.9958 D(G(z)): 0.5310 / 0.0028 Acc: 100.0000 (99.4770)\n",
      "[20/25][28/96] Loss_D: -1.0824 (-1.3013) Loss_G: 6.1565 (5.0085) D(x): 0.7668 D(G(z)): 0.0004 / 0.0032 Acc: 100.0000 (99.4773)\n",
      "[20/25][29/96] Loss_D: -1.4710 (-1.3014) Loss_G: 3.9221 (5.0079) D(x): 0.9755 D(G(z)): 0.0036 / 0.0227 Acc: 98.4375 (99.4768)\n",
      "[20/25][30/96] Loss_D: -1.3960 (-1.3015) Loss_G: 3.4733 (5.0071) D(x): 0.9431 D(G(z)): 0.0083 / 0.0563 Acc: 100.0000 (99.4770)\n",
      "[20/25][31/96] Loss_D: -1.4615 (-1.3015) Loss_G: 3.7732 (5.0065) D(x): 0.9787 D(G(z)): 0.0416 / 0.0296 Acc: 98.4375 (99.4765)\n",
      "[20/25][32/96] Loss_D: -1.2965 (-1.3015) Loss_G: 5.7006 (5.0069) D(x): 0.9881 D(G(z)): 0.0843 / 0.0050 Acc: 98.4375 (99.4760)\n",
      "[20/25][33/96] Loss_D: -1.4476 (-1.3016) Loss_G: 7.2076 (5.0080) D(x): 0.9851 D(G(z)): 0.0353 / 0.0015 Acc: 100.0000 (99.4762)\n",
      "[20/25][34/96] Loss_D: -1.5571 (-1.3017) Loss_G: 6.7205 (5.0089) D(x): 0.9591 D(G(z)): 0.0077 / 0.0022 Acc: 98.4375 (99.4757)\n",
      "[20/25][35/96] Loss_D: -1.1451 (-1.3017) Loss_G: 0.7888 (5.0067) D(x): 0.7211 D(G(z)): 0.0042 / 0.3523 Acc: 100.0000 (99.4760)\n",
      "[20/25][36/96] Loss_D: -1.1399 (-1.3016) Loss_G: 4.2594 (5.0063) D(x): 0.9997 D(G(z)): 0.2752 / 0.0259 Acc: 100.0000 (99.4762)\n",
      "[20/25][37/96] Loss_D: -1.4491 (-1.3016) Loss_G: 2.4321 (5.0050) D(x): 0.9617 D(G(z)): 0.0524 / 0.0827 Acc: 100.0000 (99.4765)\n",
      "[20/25][38/96] Loss_D: -1.3194 (-1.3017) Loss_G: 4.9705 (5.0050) D(x): 0.9203 D(G(z)): 0.0115 / 0.0093 Acc: 100.0000 (99.4768)\n",
      "[20/25][39/96] Loss_D: -1.3972 (-1.3017) Loss_G: 4.6918 (5.0048) D(x): 0.9570 D(G(z)): 0.0380 / 0.0200 Acc: 98.4375 (99.4762)\n",
      "[20/25][40/96] Loss_D: -1.2515 (-1.3017) Loss_G: 5.4054 (5.0050) D(x): 0.9521 D(G(z)): 0.1119 / 0.0051 Acc: 100.0000 (99.4765)\n",
      "[20/25][41/96] Loss_D: -1.4157 (-1.3017) Loss_G: 6.6372 (5.0059) D(x): 0.9683 D(G(z)): 0.0812 / 0.0017 Acc: 100.0000 (99.4768)\n",
      "[20/25][42/96] Loss_D: -0.9969 (-1.3016) Loss_G: 0.3056 (5.0035) D(x): 0.6608 D(G(z)): 0.0019 / 0.5074 Acc: 100.0000 (99.4770)\n",
      "[20/25][43/96] Loss_D: -0.5760 (-1.3012) Loss_G: 9.4382 (5.0057) D(x): 0.9989 D(G(z)): 0.4960 / 0.0002 Acc: 100.0000 (99.4773)\n",
      "[20/25][44/96] Loss_D: -1.1434 (-1.3011) Loss_G: 6.5287 (5.0065) D(x): 0.6759 D(G(z)): 0.0001 / 0.0022 Acc: 100.0000 (99.4776)\n",
      "[20/25][45/96] Loss_D: -1.3991 (-1.3012) Loss_G: 3.5327 (5.0058) D(x): 0.9145 D(G(z)): 0.0078 / 0.0431 Acc: 100.0000 (99.4778)\n",
      "[20/25][46/96] Loss_D: -1.5237 (-1.3013) Loss_G: 2.6190 (5.0045) D(x): 0.9892 D(G(z)): 0.0928 / 0.0764 Acc: 100.0000 (99.4781)\n",
      "[20/25][47/96] Loss_D: -1.5607 (-1.3014) Loss_G: 2.9779 (5.0035) D(x): 0.9937 D(G(z)): 0.0378 / 0.0606 Acc: 100.0000 (99.4784)\n",
      "[20/25][48/96] Loss_D: -1.4616 (-1.3015) Loss_G: 6.7469 (5.0044) D(x): 0.9909 D(G(z)): 0.0559 / 0.0015 Acc: 100.0000 (99.4786)\n",
      "[20/25][49/96] Loss_D: -1.4638 (-1.3016) Loss_G: 4.2918 (5.0040) D(x): 0.9747 D(G(z)): 0.0078 / 0.0164 Acc: 98.4375 (99.4781)\n",
      "[20/25][50/96] Loss_D: -1.5167 (-1.3017) Loss_G: 5.3490 (5.0042) D(x): 0.9462 D(G(z)): 0.0531 / 0.0067 Acc: 100.0000 (99.4784)\n",
      "[20/25][51/96] Loss_D: -1.3326 (-1.3017) Loss_G: 6.9313 (5.0052) D(x): 0.9933 D(G(z)): 0.0043 / 0.0018 Acc: 98.4375 (99.4778)\n",
      "[20/25][52/96] Loss_D: -1.4434 (-1.3018) Loss_G: 4.5911 (5.0050) D(x): 0.9693 D(G(z)): 0.0087 / 0.0192 Acc: 100.0000 (99.4781)\n",
      "[20/25][53/96] Loss_D: -1.6064 (-1.3019) Loss_G: 4.8462 (5.0049) D(x): 0.9849 D(G(z)): 0.0184 / 0.0109 Acc: 100.0000 (99.4784)\n",
      "[20/25][54/96] Loss_D: -1.4430 (-1.3020) Loss_G: 4.2242 (5.0045) D(x): 0.9379 D(G(z)): 0.0125 / 0.0235 Acc: 100.0000 (99.4786)\n",
      "[20/25][55/96] Loss_D: -1.4934 (-1.3021) Loss_G: 3.4783 (5.0037) D(x): 0.9939 D(G(z)): 0.0707 / 0.0426 Acc: 100.0000 (99.4789)\n",
      "[20/25][56/96] Loss_D: -1.3526 (-1.3021) Loss_G: 5.2408 (5.0038) D(x): 0.9979 D(G(z)): 0.0677 / 0.0086 Acc: 98.4375 (99.4784)\n",
      "[20/25][57/96] Loss_D: -1.3883 (-1.3022) Loss_G: 5.5703 (5.0041) D(x): 0.9452 D(G(z)): 0.0039 / 0.0076 Acc: 100.0000 (99.4786)\n",
      "[20/25][58/96] Loss_D: -1.3414 (-1.3022) Loss_G: 3.6015 (5.0034) D(x): 0.9727 D(G(z)): 0.0053 / 0.0393 Acc: 98.4375 (99.4781)\n",
      "[20/25][59/96] Loss_D: -1.4397 (-1.3023) Loss_G: 1.3255 (5.0016) D(x): 0.8909 D(G(z)): 0.0153 / 0.2235 Acc: 100.0000 (99.4784)\n",
      "[20/25][60/96] Loss_D: -0.2932 (-1.3018) Loss_G: 12.2875 (5.0052) D(x): 0.9986 D(G(z)): 0.5816 / 0.0000 Acc: 98.4375 (99.4779)\n",
      "[20/25][61/96] Loss_D: 3.3216 (-1.2994) Loss_G: -0.0444 (5.0027) D(x): 0.0238 D(G(z)): 0.0000 / 0.7002 Acc: 100.0000 (99.4781)\n",
      "[20/25][62/96] Loss_D: 0.1607 (-1.2987) Loss_G: 1.6547 (5.0010) D(x): 0.9900 D(G(z)): 0.7264 / 0.2259 Acc: 98.4375 (99.4776)\n",
      "[20/25][63/96] Loss_D: -0.7062 (-1.2984) Loss_G: 1.4857 (4.9992) D(x): 0.8019 D(G(z)): 0.2885 / 0.2329 Acc: 100.0000 (99.4779)\n",
      "[20/25][64/96] Loss_D: -0.7785 (-1.2981) Loss_G: 1.9999 (4.9977) D(x): 0.7111 D(G(z)): 0.1798 / 0.1384 Acc: 100.0000 (99.4781)\n",
      "[20/25][65/96] Loss_D: -0.9331 (-1.2979) Loss_G: 0.9465 (4.9957) D(x): 0.7879 D(G(z)): 0.1576 / 0.3655 Acc: 100.0000 (99.4784)\n",
      "[20/25][66/96] Loss_D: -1.2768 (-1.2979) Loss_G: 2.5048 (4.9944) D(x): 0.9862 D(G(z)): 0.2015 / 0.0914 Acc: 100.0000 (99.4786)\n",
      "[20/25][67/96] Loss_D: -1.3675 (-1.2980) Loss_G: 1.8970 (4.9929) D(x): 0.8715 D(G(z)): 0.0637 / 0.1369 Acc: 100.0000 (99.4789)\n",
      "[20/25][68/96] Loss_D: -1.3711 (-1.2980) Loss_G: 3.2489 (4.9920) D(x): 0.9313 D(G(z)): 0.0163 / 0.0529 Acc: 100.0000 (99.4792)\n",
      "[20/25][69/96] Loss_D: -1.2665 (-1.2980) Loss_G: 3.7159 (4.9914) D(x): 0.9880 D(G(z)): 0.1469 / 0.0465 Acc: 100.0000 (99.4794)\n",
      "[20/25][70/96] Loss_D: -1.3353 (-1.2980) Loss_G: 5.9268 (4.9918) D(x): 0.8968 D(G(z)): 0.1290 / 0.0049 Acc: 100.0000 (99.4797)\n",
      "[20/25][71/96] Loss_D: -0.9663 (-1.2978) Loss_G: 1.5610 (4.9901) D(x): 0.6834 D(G(z)): 0.0022 / 0.1959 Acc: 100.0000 (99.4800)\n",
      "[20/25][72/96] Loss_D: -1.0820 (-1.2977) Loss_G: 3.8463 (4.9895) D(x): 0.9970 D(G(z)): 0.2753 / 0.0210 Acc: 100.0000 (99.4802)\n",
      "[20/25][73/96] Loss_D: -1.4664 (-1.2978) Loss_G: 5.7677 (4.9899) D(x): 0.9862 D(G(z)): 0.0186 / 0.0062 Acc: 100.0000 (99.4805)\n",
      "[20/25][74/96] Loss_D: -1.3210 (-1.2978) Loss_G: 6.4247 (4.9906) D(x): 0.8782 D(G(z)): 0.0305 / 0.0025 Acc: 98.4375 (99.4799)\n",
      "[20/25][75/96] Loss_D: -1.4550 (-1.2979) Loss_G: 4.2233 (4.9903) D(x): 0.9904 D(G(z)): 0.0186 / 0.0184 Acc: 100.0000 (99.4802)\n",
      "[20/25][76/96] Loss_D: -1.2904 (-1.2979) Loss_G: 8.4988 (4.9920) D(x): 0.9921 D(G(z)): 0.1720 / 0.0003 Acc: 100.0000 (99.4805)\n",
      "[20/25][77/96] Loss_D: -1.4751 (-1.2980) Loss_G: 7.1942 (4.9931) D(x): 0.9442 D(G(z)): 0.0103 / 0.0010 Acc: 100.0000 (99.4807)\n",
      "[20/25][78/96] Loss_D: -1.1205 (-1.2979) Loss_G: 3.3050 (4.9923) D(x): 0.7439 D(G(z)): 0.0021 / 0.0448 Acc: 100.0000 (99.4810)\n",
      "[20/25][79/96] Loss_D: -1.0416 (-1.2978) Loss_G: 3.7393 (4.9916) D(x): 0.9973 D(G(z)): 0.2074 / 0.0365 Acc: 98.4375 (99.4805)\n",
      "[20/25][80/96] Loss_D: -1.5171 (-1.2979) Loss_G: 5.2062 (4.9918) D(x): 0.9924 D(G(z)): 0.0063 / 0.0085 Acc: 100.0000 (99.4807)\n",
      "[20/25][81/96] Loss_D: -1.4501 (-1.2980) Loss_G: 3.7883 (4.9912) D(x): 0.9688 D(G(z)): 0.0014 / 0.0294 Acc: 100.0000 (99.4810)\n",
      "[20/25][82/96] Loss_D: -1.4911 (-1.2981) Loss_G: 3.7036 (4.9905) D(x): 0.9850 D(G(z)): 0.0083 / 0.0325 Acc: 100.0000 (99.4812)\n",
      "[20/25][83/96] Loss_D: -1.4398 (-1.2981) Loss_G: 5.4478 (4.9907) D(x): 0.9719 D(G(z)): 0.0145 / 0.0057 Acc: 100.0000 (99.4815)\n",
      "[20/25][84/96] Loss_D: -1.3666 (-1.2982) Loss_G: 2.1576 (4.9893) D(x): 0.9214 D(G(z)): 0.0060 / 0.1171 Acc: 98.4375 (99.4810)\n",
      "[20/25][85/96] Loss_D: -1.1598 (-1.2981) Loss_G: 7.7813 (4.9907) D(x): 0.9748 D(G(z)): 0.2627 / 0.0006 Acc: 100.0000 (99.4812)\n",
      "[20/25][86/96] Loss_D: -1.3927 (-1.2981) Loss_G: 7.0997 (4.9918) D(x): 0.9073 D(G(z)): 0.0008 / 0.0011 Acc: 98.4375 (99.4807)\n",
      "[20/25][87/96] Loss_D: -1.4052 (-1.2982) Loss_G: 8.7182 (4.9936) D(x): 0.9256 D(G(z)): 0.0071 / 0.0002 Acc: 100.0000 (99.4810)\n",
      "[20/25][88/96] Loss_D: -1.4625 (-1.2983) Loss_G: 3.6915 (4.9930) D(x): 0.9163 D(G(z)): 0.0036 / 0.0332 Acc: 98.4375 (99.4805)\n",
      "[20/25][89/96] Loss_D: -1.4240 (-1.2983) Loss_G: 1.7555 (4.9914) D(x): 0.9542 D(G(z)): 0.0264 / 0.1812 Acc: 100.0000 (99.4807)\n",
      "[20/25][90/96] Loss_D: -1.3220 (-1.2984) Loss_G: 2.2610 (4.9900) D(x): 0.9890 D(G(z)): 0.0513 / 0.1284 Acc: 96.8750 (99.4794)\n",
      "[20/25][91/96] Loss_D: -1.3766 (-1.2984) Loss_G: 6.0154 (4.9905) D(x): 0.9790 D(G(z)): 0.0737 / 0.0034 Acc: 98.4375 (99.4789)\n",
      "[20/25][92/96] Loss_D: -1.5283 (-1.2985) Loss_G: 7.5891 (4.9918) D(x): 0.9752 D(G(z)): 0.0747 / 0.0005 Acc: 100.0000 (99.4792)\n",
      "[20/25][93/96] Loss_D: -1.4782 (-1.2986) Loss_G: 6.0274 (4.9923) D(x): 0.9416 D(G(z)): 0.0040 / 0.0042 Acc: 100.0000 (99.4794)\n",
      "[20/25][94/96] Loss_D: -1.3211 (-1.2986) Loss_G: 3.0742 (4.9914) D(x): 0.8392 D(G(z)): 0.0051 / 0.0620 Acc: 100.0000 (99.4797)\n",
      "[20/25][95/96] Loss_D: -0.7102 (-1.2983) Loss_G: 11.5137 (4.9946) D(x): 0.9960 D(G(z)): 0.4664 / 0.0000 Acc: 98.4375 (99.4792)\n",
      "[21/25][0/96] Loss_D: -0.1098 (-1.2977) Loss_G: 1.2479 (4.9927) D(x): 0.3222 D(G(z)): 0.0000 / 0.2399 Acc: 100.0000 (99.4794)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[21/25][1/96] Loss_D: -0.5397 (-1.2974) Loss_G: 4.6171 (4.9926) D(x): 0.9744 D(G(z)): 0.5197 / 0.0135 Acc: 100.0000 (99.4797)\n",
      "[21/25][2/96] Loss_D: -1.6221 (-1.2975) Loss_G: 4.7631 (4.9924) D(x): 0.9476 D(G(z)): 0.0052 / 0.0095 Acc: 100.0000 (99.4799)\n",
      "[21/25][3/96] Loss_D: -1.0893 (-1.2974) Loss_G: 5.3236 (4.9926) D(x): 0.7548 D(G(z)): 0.0144 / 0.0116 Acc: 100.0000 (99.4802)\n",
      "[21/25][4/96] Loss_D: -1.4647 (-1.2975) Loss_G: 1.9252 (4.9911) D(x): 0.9724 D(G(z)): 0.0351 / 0.1400 Acc: 100.0000 (99.4805)\n",
      "[21/25][5/96] Loss_D: -1.4579 (-1.2976) Loss_G: 3.2003 (4.9902) D(x): 0.9945 D(G(z)): 0.1421 / 0.0411 Acc: 100.0000 (99.4807)\n",
      "[21/25][6/96] Loss_D: -1.4646 (-1.2977) Loss_G: 3.2192 (4.9893) D(x): 0.9632 D(G(z)): 0.0111 / 0.0399 Acc: 100.0000 (99.4810)\n",
      "[21/25][7/96] Loss_D: -1.4014 (-1.2977) Loss_G: 1.8367 (4.9878) D(x): 0.9094 D(G(z)): 0.0108 / 0.1592 Acc: 100.0000 (99.4812)\n",
      "[21/25][8/96] Loss_D: -1.3834 (-1.2977) Loss_G: 6.4308 (4.9885) D(x): 0.9973 D(G(z)): 0.1227 / 0.0020 Acc: 98.4375 (99.4807)\n",
      "[21/25][9/96] Loss_D: -1.4820 (-1.2978) Loss_G: 5.4601 (4.9887) D(x): 0.9453 D(G(z)): 0.0056 / 0.0051 Acc: 100.0000 (99.4810)\n",
      "[21/25][10/96] Loss_D: -1.3471 (-1.2979) Loss_G: 4.6489 (4.9886) D(x): 0.9104 D(G(z)): 0.0424 / 0.0128 Acc: 100.0000 (99.4812)\n",
      "[21/25][11/96] Loss_D: -1.5326 (-1.2980) Loss_G: 3.5663 (4.9878) D(x): 0.9278 D(G(z)): 0.0078 / 0.0393 Acc: 100.0000 (99.4815)\n",
      "[21/25][12/96] Loss_D: -1.1055 (-1.2979) Loss_G: 6.5079 (4.9886) D(x): 0.9944 D(G(z)): 0.2543 / 0.0025 Acc: 100.0000 (99.4817)\n",
      "[21/25][13/96] Loss_D: -1.3574 (-1.2979) Loss_G: 5.5813 (4.9889) D(x): 0.8805 D(G(z)): 0.0026 / 0.0075 Acc: 100.0000 (99.4820)\n",
      "[21/25][14/96] Loss_D: -1.4832 (-1.2980) Loss_G: 6.1570 (4.9895) D(x): 0.9719 D(G(z)): 0.0030 / 0.0030 Acc: 100.0000 (99.4822)\n",
      "[21/25][15/96] Loss_D: -1.3600 (-1.2980) Loss_G: 5.0066 (4.9895) D(x): 0.9566 D(G(z)): 0.0278 / 0.0091 Acc: 98.4375 (99.4817)\n",
      "[21/25][16/96] Loss_D: -1.4443 (-1.2981) Loss_G: 4.3196 (4.9891) D(x): 0.9444 D(G(z)): 0.0110 / 0.0165 Acc: 100.0000 (99.4820)\n",
      "[21/25][17/96] Loss_D: -1.5450 (-1.2982) Loss_G: 1.2976 (4.9873) D(x): 0.9372 D(G(z)): 0.0081 / 0.2292 Acc: 100.0000 (99.4822)\n",
      "[21/25][18/96] Loss_D: -1.2092 (-1.2982) Loss_G: 2.0645 (4.9859) D(x): 0.9961 D(G(z)): 0.1377 / 0.1526 Acc: 100.0000 (99.4825)\n",
      "[21/25][19/96] Loss_D: -1.3674 (-1.2982) Loss_G: 7.3656 (4.9871) D(x): 0.9866 D(G(z)): 0.0932 / 0.0007 Acc: 100.0000 (99.4827)\n",
      "[21/25][20/96] Loss_D: -1.4533 (-1.2983) Loss_G: 4.4443 (4.9868) D(x): 0.8826 D(G(z)): 0.0013 / 0.0129 Acc: 98.4375 (99.4822)\n",
      "[21/25][21/96] Loss_D: -1.1202 (-1.2982) Loss_G: 2.2973 (4.9855) D(x): 0.8380 D(G(z)): 0.0611 / 0.1096 Acc: 95.3125 (99.4802)\n",
      "[21/25][22/96] Loss_D: -1.2685 (-1.2982) Loss_G: 3.6091 (4.9848) D(x): 0.9968 D(G(z)): 0.1438 / 0.0377 Acc: 100.0000 (99.4804)\n",
      "[21/25][23/96] Loss_D: -1.3354 (-1.2982) Loss_G: 6.3601 (4.9855) D(x): 0.9908 D(G(z)): 0.0653 / 0.0027 Acc: 98.4375 (99.4799)\n",
      "[21/25][24/96] Loss_D: -1.4790 (-1.2983) Loss_G: 4.6238 (4.9853) D(x): 0.9818 D(G(z)): 0.0018 / 0.0164 Acc: 98.4375 (99.4794)\n",
      "[21/25][25/96] Loss_D: -1.4125 (-1.2984) Loss_G: 6.5192 (4.9860) D(x): 0.9812 D(G(z)): 0.0057 / 0.0028 Acc: 100.0000 (99.4797)\n",
      "[21/25][26/96] Loss_D: -1.3821 (-1.2984) Loss_G: 4.7259 (4.9859) D(x): 0.9061 D(G(z)): 0.0071 / 0.0135 Acc: 100.0000 (99.4799)\n",
      "[21/25][27/96] Loss_D: -1.2414 (-1.2984) Loss_G: 3.3286 (4.9851) D(x): 0.8783 D(G(z)): 0.0235 / 0.0489 Acc: 98.4375 (99.4794)\n",
      "[21/25][28/96] Loss_D: -0.8662 (-1.2982) Loss_G: 10.6529 (4.9879) D(x): 0.9962 D(G(z)): 0.4161 / 0.0000 Acc: 100.0000 (99.4797)\n",
      "[21/25][29/96] Loss_D: -0.4161 (-1.2977) Loss_G: 0.6327 (4.9858) D(x): 0.4465 D(G(z)): 0.0002 / 0.3986 Acc: 100.0000 (99.4799)\n",
      "[21/25][30/96] Loss_D: -1.2411 (-1.2977) Loss_G: 1.3747 (4.9840) D(x): 0.9948 D(G(z)): 0.1585 / 0.2297 Acc: 100.0000 (99.4802)\n",
      "[21/25][31/96] Loss_D: 0.0655 (-1.2970) Loss_G: 9.8637 (4.9864) D(x): 0.9984 D(G(z)): 0.6851 / 0.0001 Acc: 100.0000 (99.4804)\n",
      "[21/25][32/96] Loss_D: 1.2296 (-1.2958) Loss_G: 4.2020 (4.9860) D(x): 0.1403 D(G(z)): 0.0000 / 0.0225 Acc: 98.4375 (99.4799)\n",
      "[21/25][33/96] Loss_D: -1.1694 (-1.2957) Loss_G: 0.8212 (4.9840) D(x): 0.9664 D(G(z)): 0.1888 / 0.3738 Acc: 98.4375 (99.4794)\n",
      "[21/25][34/96] Loss_D: -1.0201 (-1.2956) Loss_G: 4.4269 (4.9837) D(x): 0.9961 D(G(z)): 0.3529 / 0.0137 Acc: 100.0000 (99.4797)\n",
      "[21/25][35/96] Loss_D: -1.3709 (-1.2956) Loss_G: 5.5278 (4.9840) D(x): 0.8945 D(G(z)): 0.0019 / 0.0047 Acc: 100.0000 (99.4799)\n",
      "[21/25][36/96] Loss_D: -1.3782 (-1.2957) Loss_G: 3.8273 (4.9834) D(x): 0.9001 D(G(z)): 0.0092 / 0.0257 Acc: 100.0000 (99.4802)\n",
      "[21/25][37/96] Loss_D: -1.4993 (-1.2958) Loss_G: 4.7198 (4.9833) D(x): 0.9535 D(G(z)): 0.0388 / 0.0128 Acc: 100.0000 (99.4804)\n",
      "[21/25][38/96] Loss_D: -1.4964 (-1.2959) Loss_G: 6.2182 (4.9839) D(x): 0.9840 D(G(z)): 0.0658 / 0.0028 Acc: 100.0000 (99.4807)\n",
      "[21/25][39/96] Loss_D: -1.4111 (-1.2959) Loss_G: 5.3842 (4.9841) D(x): 0.9440 D(G(z)): 0.0113 / 0.0069 Acc: 98.4375 (99.4802)\n",
      "[21/25][40/96] Loss_D: -1.4309 (-1.2960) Loss_G: 5.8985 (4.9845) D(x): 0.9919 D(G(z)): 0.0135 / 0.0054 Acc: 100.0000 (99.4804)\n",
      "[21/25][41/96] Loss_D: -1.4963 (-1.2961) Loss_G: 3.2385 (4.9837) D(x): 0.9882 D(G(z)): 0.0212 / 0.0545 Acc: 98.4375 (99.4799)\n",
      "[21/25][42/96] Loss_D: -1.3905 (-1.2961) Loss_G: 5.1717 (4.9837) D(x): 0.9709 D(G(z)): 0.0157 / 0.0106 Acc: 100.0000 (99.4802)\n",
      "[21/25][43/96] Loss_D: -1.3765 (-1.2962) Loss_G: 4.7825 (4.9836) D(x): 0.9812 D(G(z)): 0.0445 / 0.0119 Acc: 98.4375 (99.4797)\n",
      "[21/25][44/96] Loss_D: -1.5011 (-1.2963) Loss_G: 3.5575 (4.9830) D(x): 0.9460 D(G(z)): 0.0122 / 0.0359 Acc: 98.4375 (99.4792)\n",
      "[21/25][45/96] Loss_D: -1.3342 (-1.2963) Loss_G: 2.1390 (4.9816) D(x): 0.9016 D(G(z)): 0.0284 / 0.1355 Acc: 100.0000 (99.4794)\n",
      "[21/25][46/96] Loss_D: -1.3934 (-1.2963) Loss_G: 5.2425 (4.9817) D(x): 0.9877 D(G(z)): 0.0919 / 0.0083 Acc: 100.0000 (99.4797)\n",
      "[21/25][47/96] Loss_D: -1.3544 (-1.2964) Loss_G: 4.8898 (4.9817) D(x): 0.9742 D(G(z)): 0.0602 / 0.0114 Acc: 100.0000 (99.4799)\n",
      "[21/25][48/96] Loss_D: -1.4122 (-1.2964) Loss_G: 4.7569 (4.9815) D(x): 0.9662 D(G(z)): 0.0204 / 0.0138 Acc: 100.0000 (99.4802)\n",
      "[21/25][49/96] Loss_D: -1.4958 (-1.2965) Loss_G: 3.7494 (4.9810) D(x): 0.9695 D(G(z)): 0.0034 / 0.0249 Acc: 98.4375 (99.4797)\n",
      "[21/25][50/96] Loss_D: -1.4477 (-1.2966) Loss_G: 4.5161 (4.9807) D(x): 0.9132 D(G(z)): 0.0637 / 0.0114 Acc: 100.0000 (99.4799)\n",
      "[21/25][51/96] Loss_D: -1.4239 (-1.2967) Loss_G: 3.2729 (4.9799) D(x): 0.9471 D(G(z)): 0.0031 / 0.0514 Acc: 100.0000 (99.4802)\n",
      "[21/25][52/96] Loss_D: -1.4334 (-1.2967) Loss_G: 2.6630 (4.9788) D(x): 0.9698 D(G(z)): 0.0790 / 0.0867 Acc: 100.0000 (99.4804)\n",
      "[21/25][53/96] Loss_D: -1.4394 (-1.2968) Loss_G: 3.2339 (4.9779) D(x): 0.9969 D(G(z)): 0.0670 / 0.0428 Acc: 96.8750 (99.4792)\n",
      "[21/25][54/96] Loss_D: -1.3791 (-1.2968) Loss_G: 6.4186 (4.9786) D(x): 0.9643 D(G(z)): 0.0597 / 0.0024 Acc: 100.0000 (99.4794)\n",
      "[21/25][55/96] Loss_D: -1.2398 (-1.2968) Loss_G: 3.1874 (4.9778) D(x): 0.7978 D(G(z)): 0.0098 / 0.0496 Acc: 100.0000 (99.4797)\n",
      "[21/25][56/96] Loss_D: -1.2564 (-1.2968) Loss_G: 6.9176 (4.9787) D(x): 0.9928 D(G(z)): 0.2371 / 0.0011 Acc: 100.0000 (99.4799)\n",
      "[21/25][57/96] Loss_D: -1.2497 (-1.2968) Loss_G: 4.8859 (4.9787) D(x): 0.7632 D(G(z)): 0.0009 / 0.0089 Acc: 100.0000 (99.4802)\n",
      "[21/25][58/96] Loss_D: -1.3593 (-1.2968) Loss_G: 3.5434 (4.9780) D(x): 0.9675 D(G(z)): 0.0536 / 0.0343 Acc: 100.0000 (99.4804)\n",
      "[21/25][59/96] Loss_D: -1.2966 (-1.2968) Loss_G: 4.7257 (4.9778) D(x): 0.9890 D(G(z)): 0.1552 / 0.0147 Acc: 98.4375 (99.4799)\n",
      "[21/25][60/96] Loss_D: -1.3505 (-1.2968) Loss_G: 1.7470 (4.9763) D(x): 0.8876 D(G(z)): 0.0051 / 0.1817 Acc: 100.0000 (99.4802)\n",
      "[21/25][61/96] Loss_D: -1.3868 (-1.2969) Loss_G: 2.0457 (4.9749) D(x): 0.9919 D(G(z)): 0.0399 / 0.1510 Acc: 100.0000 (99.4804)\n",
      "[21/25][62/96] Loss_D: -1.3832 (-1.2969) Loss_G: 6.4116 (4.9756) D(x): 0.9932 D(G(z)): 0.1063 / 0.0028 Acc: 100.0000 (99.4807)\n",
      "[21/25][63/96] Loss_D: -1.0033 (-1.2968) Loss_G: 0.0617 (4.9732) D(x): 0.6964 D(G(z)): 0.0300 / 0.6298 Acc: 98.4375 (99.4802)\n",
      "[21/25][64/96] Loss_D: -1.4267 (-1.2968) Loss_G: 0.0389 (4.9708) D(x): 0.9997 D(G(z)): 0.1151 / 0.5975 Acc: 98.4375 (99.4797)\n",
      "[21/25][65/96] Loss_D: -0.7950 (-1.2966) Loss_G: 8.9255 (4.9727) D(x): 0.9996 D(G(z)): 0.4218 / 0.0002 Acc: 98.4375 (99.4792)\n",
      "[21/25][66/96] Loss_D: -0.3162 (-1.2961) Loss_G: 4.4544 (4.9725) D(x): 0.4668 D(G(z)): 0.0001 / 0.0187 Acc: 100.0000 (99.4794)\n",
      "[21/25][67/96] Loss_D: -1.5080 (-1.2962) Loss_G: 0.0418 (4.9701) D(x): 0.9948 D(G(z)): 0.0448 / 0.6157 Acc: 98.4375 (99.4789)\n",
      "[21/25][68/96] Loss_D: -0.6274 (-1.2959) Loss_G: 5.2859 (4.9703) D(x): 0.9998 D(G(z)): 0.4708 / 0.0066 Acc: 95.3125 (99.4769)\n",
      "[21/25][69/96] Loss_D: -1.3591 (-1.2959) Loss_G: 7.7594 (4.9716) D(x): 0.8653 D(G(z)): 0.0009 / 0.0006 Acc: 100.0000 (99.4772)\n",
      "[21/25][70/96] Loss_D: -1.2225 (-1.2959) Loss_G: 4.5042 (4.9714) D(x): 0.7334 D(G(z)): 0.0002 / 0.0147 Acc: 100.0000 (99.4774)\n",
      "[21/25][71/96] Loss_D: -1.5285 (-1.2960) Loss_G: 2.1899 (4.9701) D(x): 0.9967 D(G(z)): 0.0428 / 0.0915 Acc: 100.0000 (99.4777)\n",
      "[21/25][72/96] Loss_D: -1.4141 (-1.2961) Loss_G: 1.4332 (4.9684) D(x): 0.9928 D(G(z)): 0.0450 / 0.2252 Acc: 100.0000 (99.4779)\n",
      "[21/25][73/96] Loss_D: -1.3577 (-1.2961) Loss_G: 6.7902 (4.9692) D(x): 0.9971 D(G(z)): 0.1293 / 0.0018 Acc: 100.0000 (99.4782)\n",
      "[21/25][74/96] Loss_D: -1.4141 (-1.2961) Loss_G: 4.6769 (4.9691) D(x): 0.9941 D(G(z)): 0.0018 / 0.0143 Acc: 98.4375 (99.4777)\n",
      "[21/25][75/96] Loss_D: -1.2802 (-1.2961) Loss_G: 2.5214 (4.9679) D(x): 0.8229 D(G(z)): 0.0038 / 0.0904 Acc: 100.0000 (99.4779)\n",
      "[21/25][76/96] Loss_D: -1.3449 (-1.2962) Loss_G: 3.3744 (4.9672) D(x): 0.9963 D(G(z)): 0.1034 / 0.0551 Acc: 100.0000 (99.4782)\n",
      "[21/25][77/96] Loss_D: -1.3140 (-1.2962) Loss_G: 2.7146 (4.9661) D(x): 0.9980 D(G(z)): 0.0407 / 0.0882 Acc: 98.4375 (99.4777)\n",
      "[21/25][78/96] Loss_D: -1.4031 (-1.2962) Loss_G: 3.1186 (4.9652) D(x): 0.9177 D(G(z)): 0.0153 / 0.0559 Acc: 100.0000 (99.4779)\n",
      "[21/25][79/96] Loss_D: -1.2949 (-1.2962) Loss_G: 6.1675 (4.9658) D(x): 0.9874 D(G(z)): 0.1863 / 0.0025 Acc: 98.4375 (99.4774)\n",
      "[21/25][80/96] Loss_D: -1.4831 (-1.2963) Loss_G: 6.9964 (4.9667) D(x): 0.9706 D(G(z)): 0.0014 / 0.0015 Acc: 100.0000 (99.4777)\n",
      "[21/25][81/96] Loss_D: -1.3888 (-1.2964) Loss_G: 4.8779 (4.9667) D(x): 0.8780 D(G(z)): 0.0027 / 0.0199 Acc: 100.0000 (99.4779)\n",
      "[21/25][82/96] Loss_D: -1.3585 (-1.2964) Loss_G: 2.7993 (4.9657) D(x): 0.9059 D(G(z)): 0.0144 / 0.0678 Acc: 100.0000 (99.4782)\n",
      "[21/25][83/96] Loss_D: -1.3905 (-1.2964) Loss_G: 4.4576 (4.9654) D(x): 0.9961 D(G(z)): 0.0544 / 0.0197 Acc: 100.0000 (99.4784)\n",
      "[21/25][84/96] Loss_D: -1.4500 (-1.2965) Loss_G: 4.4905 (4.9652) D(x): 0.9974 D(G(z)): 0.0436 / 0.0165 Acc: 100.0000 (99.4787)\n",
      "[21/25][85/96] Loss_D: -1.4077 (-1.2966) Loss_G: 3.3482 (4.9644) D(x): 0.9992 D(G(z)): 0.0538 / 0.0393 Acc: 100.0000 (99.4789)\n",
      "[21/25][86/96] Loss_D: -1.4691 (-1.2966) Loss_G: 6.2296 (4.9650) D(x): 0.9922 D(G(z)): 0.0073 / 0.0037 Acc: 100.0000 (99.4792)\n",
      "[21/25][87/96] Loss_D: -1.3833 (-1.2967) Loss_G: 6.3091 (4.9657) D(x): 0.9341 D(G(z)): 0.1079 / 0.0023 Acc: 100.0000 (99.4794)\n",
      "[21/25][88/96] Loss_D: -1.0095 (-1.2965) Loss_G: 2.8154 (4.9647) D(x): 0.7312 D(G(z)): 0.0061 / 0.0764 Acc: 100.0000 (99.4797)\n",
      "[21/25][89/96] Loss_D: -0.9900 (-1.2964) Loss_G: 6.6623 (4.9655) D(x): 0.9978 D(G(z)): 0.3064 / 0.0017 Acc: 100.0000 (99.4799)\n",
      "[21/25][90/96] Loss_D: -1.4694 (-1.2965) Loss_G: 4.9988 (4.9655) D(x): 0.9068 D(G(z)): 0.0320 / 0.0092 Acc: 100.0000 (99.4802)\n",
      "[21/25][91/96] Loss_D: -1.3462 (-1.2965) Loss_G: 7.3779 (4.9666) D(x): 0.8858 D(G(z)): 0.0004 / 0.0008 Acc: 100.0000 (99.4804)\n",
      "[21/25][92/96] Loss_D: -1.3735 (-1.2965) Loss_G: 1.8739 (4.9652) D(x): 0.9399 D(G(z)): 0.0124 / 0.1703 Acc: 100.0000 (99.4806)\n",
      "[21/25][93/96] Loss_D: -1.4109 (-1.2966) Loss_G: 2.0884 (4.9638) D(x): 0.9902 D(G(z)): 0.0307 / 0.1312 Acc: 100.0000 (99.4809)\n",
      "[21/25][94/96] Loss_D: -1.3320 (-1.2966) Loss_G: 4.1579 (4.9634) D(x): 0.9940 D(G(z)): 0.1208 / 0.0210 Acc: 98.4375 (99.4804)\n",
      "[21/25][95/96] Loss_D: -1.5214 (-1.2967) Loss_G: 5.4326 (4.9636) D(x): 0.9512 D(G(z)): 0.0341 / 0.0054 Acc: 100.0000 (99.4806)\n",
      "[22/25][0/96] Loss_D: -1.3537 (-1.2967) Loss_G: 2.6756 (4.9625) D(x): 0.8737 D(G(z)): 0.0270 / 0.0782 Acc: 100.0000 (99.4809)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[22/25][1/96] Loss_D: -1.1087 (-1.2967) Loss_G: 6.7247 (4.9634) D(x): 0.9967 D(G(z)): 0.2157 / 0.0018 Acc: 96.8750 (99.4797)\n",
      "[22/25][2/96] Loss_D: -1.4996 (-1.2967) Loss_G: 7.9859 (4.9648) D(x): 0.9426 D(G(z)): 0.0020 / 0.0004 Acc: 98.4375 (99.4792)\n",
      "[22/25][3/96] Loss_D: -0.5092 (-1.2964) Loss_G: 0.3594 (4.9626) D(x): 0.5000 D(G(z)): 0.0004 / 0.5598 Acc: 98.4375 (99.4787)\n",
      "[22/25][4/96] Loss_D: 1.7665 (-1.2949) Loss_G: 8.4133 (4.9643) D(x): 0.9999 D(G(z)): 0.9168 / 0.0004 Acc: 100.0000 (99.4789)\n",
      "[22/25][5/96] Loss_D: -0.5170 (-1.2946) Loss_G: 6.7156 (4.9651) D(x): 0.4685 D(G(z)): 0.0001 / 0.0014 Acc: 98.4375 (99.4784)\n",
      "[22/25][6/96] Loss_D: -1.2494 (-1.2945) Loss_G: 1.7934 (4.9636) D(x): 0.8418 D(G(z)): 0.0188 / 0.1909 Acc: 100.0000 (99.4787)\n",
      "[22/25][7/96] Loss_D: -1.3067 (-1.2945) Loss_G: 2.3347 (4.9624) D(x): 0.9976 D(G(z)): 0.1350 / 0.1177 Acc: 100.0000 (99.4789)\n",
      "[22/25][8/96] Loss_D: -1.3519 (-1.2946) Loss_G: 4.0606 (4.9619) D(x): 0.9997 D(G(z)): 0.1580 / 0.0236 Acc: 100.0000 (99.4792)\n",
      "[22/25][9/96] Loss_D: -1.4945 (-1.2947) Loss_G: 8.1696 (4.9634) D(x): 0.9764 D(G(z)): 0.0116 / 0.0006 Acc: 100.0000 (99.4794)\n",
      "[22/25][10/96] Loss_D: -1.2902 (-1.2947) Loss_G: 4.2396 (4.9631) D(x): 0.7964 D(G(z)): 0.0025 / 0.0139 Acc: 100.0000 (99.4797)\n",
      "[22/25][11/96] Loss_D: -1.1184 (-1.2946) Loss_G: 3.6505 (4.9625) D(x): 0.9886 D(G(z)): 0.1742 / 0.0520 Acc: 100.0000 (99.4799)\n",
      "[22/25][12/96] Loss_D: -1.3388 (-1.2946) Loss_G: 5.4981 (4.9627) D(x): 0.9722 D(G(z)): 0.0211 / 0.0103 Acc: 96.8750 (99.4787)\n",
      "[22/25][13/96] Loss_D: -1.1886 (-1.2946) Loss_G: 3.8685 (4.9622) D(x): 0.8772 D(G(z)): 0.0644 / 0.0299 Acc: 98.4375 (99.4782)\n",
      "[22/25][14/96] Loss_D: -1.4321 (-1.2946) Loss_G: 5.0716 (4.9623) D(x): 0.9897 D(G(z)): 0.1008 / 0.0093 Acc: 100.0000 (99.4784)\n",
      "[22/25][15/96] Loss_D: -0.8220 (-1.2944) Loss_G: -0.1081 (4.9599) D(x): 0.6055 D(G(z)): 0.0066 / 0.7608 Acc: 100.0000 (99.4787)\n",
      "[22/25][16/96] Loss_D: -0.4120 (-1.2940) Loss_G: 6.3602 (4.9605) D(x): 0.9993 D(G(z)): 0.5438 / 0.0052 Acc: 100.0000 (99.4789)\n",
      "[22/25][17/96] Loss_D: -1.2599 (-1.2940) Loss_G: 10.6987 (4.9632) D(x): 0.8668 D(G(z)): 0.0005 / 0.0000 Acc: 100.0000 (99.4792)\n",
      "[22/25][18/96] Loss_D: -1.5160 (-1.2941) Loss_G: 8.0477 (4.9647) D(x): 0.9713 D(G(z)): 0.0005 / 0.0004 Acc: 100.0000 (99.4794)\n",
      "[22/25][19/96] Loss_D: -1.3961 (-1.2941) Loss_G: 6.7104 (4.9655) D(x): 0.8716 D(G(z)): 0.0000 / 0.0020 Acc: 98.4375 (99.4789)\n",
      "[22/25][20/96] Loss_D: -1.3688 (-1.2942) Loss_G: 1.1388 (4.9637) D(x): 0.9014 D(G(z)): 0.0079 / 0.2647 Acc: 100.0000 (99.4792)\n",
      "[22/25][21/96] Loss_D: -0.8774 (-1.2940) Loss_G: 10.2387 (4.9662) D(x): 0.9976 D(G(z)): 0.3817 / 0.0001 Acc: 100.0000 (99.4794)\n",
      "[22/25][22/96] Loss_D: -1.3757 (-1.2940) Loss_G: 9.1234 (4.9681) D(x): 0.9243 D(G(z)): 0.0001 / 0.0002 Acc: 100.0000 (99.4797)\n",
      "[22/25][23/96] Loss_D: 0.8256 (-1.2930) Loss_G: -0.5314 (4.9656) D(x): 0.1594 D(G(z)): 0.0001 / 0.9709 Acc: 100.0000 (99.4799)\n",
      "[22/25][24/96] Loss_D: 3.8614 (-1.2906) Loss_G: 2.0878 (4.9642) D(x): 1.0000 D(G(z)): 0.9899 / 0.1434 Acc: 100.0000 (99.4801)\n",
      "[22/25][25/96] Loss_D: -0.7130 (-1.2903) Loss_G: 4.8948 (4.9642) D(x): 0.8714 D(G(z)): 0.3560 / 0.0104 Acc: 100.0000 (99.4804)\n",
      "[22/25][26/96] Loss_D: 0.7540 (-1.2894) Loss_G: 0.9932 (4.9623) D(x): 0.1953 D(G(z)): 0.0026 / 0.2991 Acc: 100.0000 (99.4806)\n",
      "[22/25][27/96] Loss_D: -0.9638 (-1.2892) Loss_G: 1.1299 (4.9605) D(x): 0.9363 D(G(z)): 0.3221 / 0.2891 Acc: 100.0000 (99.4809)\n",
      "[22/25][28/96] Loss_D: -0.7262 (-1.2890) Loss_G: 3.9290 (4.9600) D(x): 0.9404 D(G(z)): 0.3569 / 0.0372 Acc: 100.0000 (99.4811)\n",
      "[22/25][29/96] Loss_D: -1.2608 (-1.2889) Loss_G: 5.0986 (4.9601) D(x): 0.8345 D(G(z)): 0.0103 / 0.0101 Acc: 98.4375 (99.4806)\n",
      "[22/25][30/96] Loss_D: -1.1702 (-1.2889) Loss_G: 3.0435 (4.9592) D(x): 0.7844 D(G(z)): 0.0112 / 0.0720 Acc: 100.0000 (99.4809)\n",
      "[22/25][31/96] Loss_D: -1.3715 (-1.2889) Loss_G: 3.9611 (4.9588) D(x): 0.9495 D(G(z)): 0.0474 / 0.0295 Acc: 100.0000 (99.4811)\n",
      "[22/25][32/96] Loss_D: -1.2812 (-1.2889) Loss_G: 4.8045 (4.9587) D(x): 0.9880 D(G(z)): 0.1166 / 0.0132 Acc: 100.0000 (99.4814)\n",
      "[22/25][33/96] Loss_D: -1.3632 (-1.2890) Loss_G: 4.6945 (4.9586) D(x): 0.9852 D(G(z)): 0.0686 / 0.0172 Acc: 98.4375 (99.4809)\n",
      "[22/25][34/96] Loss_D: -1.4416 (-1.2890) Loss_G: 3.1983 (4.9577) D(x): 0.9782 D(G(z)): 0.0033 / 0.0567 Acc: 100.0000 (99.4811)\n",
      "[22/25][35/96] Loss_D: -1.2483 (-1.2890) Loss_G: 3.7073 (4.9572) D(x): 0.8577 D(G(z)): 0.0010 / 0.0458 Acc: 98.4375 (99.4806)\n",
      "[22/25][36/96] Loss_D: -1.3106 (-1.2890) Loss_G: 2.8876 (4.9562) D(x): 0.9991 D(G(z)): 0.0584 / 0.0682 Acc: 98.4375 (99.4801)\n",
      "[22/25][37/96] Loss_D: -1.4848 (-1.2891) Loss_G: 5.0013 (4.9562) D(x): 0.9968 D(G(z)): 0.0815 / 0.0100 Acc: 100.0000 (99.4804)\n",
      "[22/25][38/96] Loss_D: -1.4676 (-1.2892) Loss_G: 5.4150 (4.9564) D(x): 0.9472 D(G(z)): 0.0063 / 0.0051 Acc: 98.4375 (99.4799)\n",
      "[22/25][39/96] Loss_D: -1.3306 (-1.2892) Loss_G: 4.5029 (4.9562) D(x): 0.9321 D(G(z)): 0.0155 / 0.0147 Acc: 100.0000 (99.4801)\n",
      "[22/25][40/96] Loss_D: -1.3614 (-1.2892) Loss_G: 2.4358 (4.9550) D(x): 0.9936 D(G(z)): 0.0222 / 0.1226 Acc: 100.0000 (99.4804)\n",
      "[22/25][41/96] Loss_D: -1.3119 (-1.2893) Loss_G: 4.4002 (4.9548) D(x): 0.9705 D(G(z)): 0.0763 / 0.0228 Acc: 100.0000 (99.4806)\n",
      "[22/25][42/96] Loss_D: -1.3745 (-1.2893) Loss_G: 1.7921 (4.9533) D(x): 0.9473 D(G(z)): 0.0064 / 0.1644 Acc: 100.0000 (99.4809)\n",
      "[22/25][43/96] Loss_D: -1.2696 (-1.2893) Loss_G: 4.3374 (4.9530) D(x): 0.9870 D(G(z)): 0.1276 / 0.0159 Acc: 98.4375 (99.4804)\n",
      "[22/25][44/96] Loss_D: -1.2607 (-1.2893) Loss_G: 6.1709 (4.9536) D(x): 0.9199 D(G(z)): 0.1040 / 0.0028 Acc: 100.0000 (99.4806)\n",
      "[22/25][45/96] Loss_D: -1.2321 (-1.2892) Loss_G: 3.1630 (4.9528) D(x): 0.8341 D(G(z)): 0.0015 / 0.0627 Acc: 100.0000 (99.4809)\n",
      "[22/25][46/96] Loss_D: -1.3218 (-1.2893) Loss_G: 6.5950 (4.9535) D(x): 0.9549 D(G(z)): 0.1481 / 0.0017 Acc: 100.0000 (99.4811)\n",
      "[22/25][47/96] Loss_D: -1.2848 (-1.2893) Loss_G: 6.8308 (4.9544) D(x): 0.9306 D(G(z)): 0.0170 / 0.0018 Acc: 100.0000 (99.4813)\n",
      "[22/25][48/96] Loss_D: -0.9774 (-1.2891) Loss_G: -0.1499 (4.9520) D(x): 0.7215 D(G(z)): 0.0053 / 0.8140 Acc: 96.8750 (99.4801)\n",
      "[22/25][49/96] Loss_D: -0.8679 (-1.2889) Loss_G: 3.6415 (4.9514) D(x): 0.9997 D(G(z)): 0.3807 / 0.0330 Acc: 100.0000 (99.4804)\n",
      "[22/25][50/96] Loss_D: -1.2296 (-1.2889) Loss_G: 3.7550 (4.9509) D(x): 0.8987 D(G(z)): 0.0410 / 0.0299 Acc: 100.0000 (99.4806)\n",
      "[22/25][51/96] Loss_D: -1.4475 (-1.2890) Loss_G: 5.0565 (4.9509) D(x): 0.9204 D(G(z)): 0.0409 / 0.0067 Acc: 100.0000 (99.4809)\n",
      "[22/25][52/96] Loss_D: -1.2265 (-1.2889) Loss_G: 3.9295 (4.9505) D(x): 0.8533 D(G(z)): 0.0229 / 0.0225 Acc: 100.0000 (99.4811)\n",
      "[22/25][53/96] Loss_D: -1.3560 (-1.2890) Loss_G: 2.7822 (4.9495) D(x): 0.9849 D(G(z)): 0.0330 / 0.0884 Acc: 100.0000 (99.4813)\n",
      "[22/25][54/96] Loss_D: -1.1700 (-1.2889) Loss_G: 6.9067 (4.9504) D(x): 0.9980 D(G(z)): 0.2569 / 0.0013 Acc: 100.0000 (99.4816)\n",
      "[22/25][55/96] Loss_D: -1.3194 (-1.2889) Loss_G: 4.5989 (4.9502) D(x): 0.7966 D(G(z)): 0.0062 / 0.0153 Acc: 100.0000 (99.4818)\n",
      "[22/25][56/96] Loss_D: -1.2804 (-1.2889) Loss_G: 4.3244 (4.9499) D(x): 0.9698 D(G(z)): 0.1024 / 0.0264 Acc: 100.0000 (99.4820)\n",
      "[22/25][57/96] Loss_D: -1.4199 (-1.2890) Loss_G: 5.7404 (4.9503) D(x): 0.9914 D(G(z)): 0.0087 / 0.0048 Acc: 100.0000 (99.4823)\n",
      "[22/25][58/96] Loss_D: -1.2110 (-1.2889) Loss_G: 3.4103 (4.9496) D(x): 0.8339 D(G(z)): 0.0378 / 0.0556 Acc: 98.4375 (99.4818)\n",
      "[22/25][59/96] Loss_D: -1.4929 (-1.2890) Loss_G: 4.1315 (4.9492) D(x): 0.9994 D(G(z)): 0.0064 / 0.0169 Acc: 100.0000 (99.4820)\n",
      "[22/25][60/96] Loss_D: -1.3711 (-1.2891) Loss_G: 1.0243 (4.9474) D(x): 0.9958 D(G(z)): 0.0569 / 0.3206 Acc: 100.0000 (99.4823)\n",
      "[22/25][61/96] Loss_D: -1.3581 (-1.2891) Loss_G: 3.4759 (4.9467) D(x): 0.9567 D(G(z)): 0.0792 / 0.0332 Acc: 100.0000 (99.4825)\n",
      "[22/25][62/96] Loss_D: -1.3928 (-1.2892) Loss_G: 3.2353 (4.9459) D(x): 0.9906 D(G(z)): 0.0189 / 0.0541 Acc: 96.8750 (99.4813)\n",
      "[22/25][63/96] Loss_D: -1.5675 (-1.2893) Loss_G: 2.7300 (4.9449) D(x): 0.9731 D(G(z)): 0.0273 / 0.0848 Acc: 96.8750 (99.4801)\n",
      "[22/25][64/96] Loss_D: -1.5710 (-1.2894) Loss_G: 6.1037 (4.9454) D(x): 0.9745 D(G(z)): 0.0112 / 0.0024 Acc: 100.0000 (99.4804)\n",
      "[22/25][65/96] Loss_D: -1.2613 (-1.2894) Loss_G: 4.6851 (4.9453) D(x): 0.9520 D(G(z)): 0.0297 / 0.0132 Acc: 100.0000 (99.4806)\n",
      "[22/25][66/96] Loss_D: -1.2645 (-1.2894) Loss_G: 1.3733 (4.9437) D(x): 0.7693 D(G(z)): 0.0143 / 0.2336 Acc: 98.4375 (99.4801)\n",
      "[22/25][67/96] Loss_D: -0.7240 (-1.2891) Loss_G: 7.4781 (4.9448) D(x): 0.9998 D(G(z)): 0.4656 / 0.0009 Acc: 100.0000 (99.4804)\n",
      "[22/25][68/96] Loss_D: -1.0308 (-1.2890) Loss_G: 3.9789 (4.9444) D(x): 0.7116 D(G(z)): 0.0070 / 0.0232 Acc: 100.0000 (99.4806)\n",
      "[22/25][69/96] Loss_D: -1.4723 (-1.2891) Loss_G: 2.7252 (4.9434) D(x): 0.9912 D(G(z)): 0.0181 / 0.0685 Acc: 100.0000 (99.4808)\n",
      "[22/25][70/96] Loss_D: -1.5003 (-1.2892) Loss_G: 5.3779 (4.9436) D(x): 0.9892 D(G(z)): 0.0186 / 0.0066 Acc: 100.0000 (99.4811)\n",
      "[22/25][71/96] Loss_D: -1.2138 (-1.2892) Loss_G: 2.8028 (4.9426) D(x): 0.8804 D(G(z)): 0.0547 / 0.0861 Acc: 100.0000 (99.4813)\n",
      "[22/25][72/96] Loss_D: -0.5534 (-1.2888) Loss_G: 10.4660 (4.9451) D(x): 0.9437 D(G(z)): 0.4910 / 0.0000 Acc: 98.4375 (99.4808)\n",
      "[22/25][73/96] Loss_D: 1.1498 (-1.2877) Loss_G: -0.3311 (4.9427) D(x): 0.1479 D(G(z)): 0.0008 / 0.9182 Acc: 100.0000 (99.4811)\n",
      "[22/25][74/96] Loss_D: 0.9831 (-1.2867) Loss_G: 4.7795 (4.9426) D(x): 0.9997 D(G(z)): 0.8689 / 0.0090 Acc: 100.0000 (99.4813)\n",
      "[22/25][75/96] Loss_D: -1.2822 (-1.2867) Loss_G: 4.2835 (4.9423) D(x): 0.8459 D(G(z)): 0.0094 / 0.0200 Acc: 100.0000 (99.4815)\n",
      "[22/25][76/96] Loss_D: -1.2134 (-1.2866) Loss_G: 3.7871 (4.9418) D(x): 0.7453 D(G(z)): 0.0039 / 0.0275 Acc: 100.0000 (99.4818)\n",
      "[22/25][77/96] Loss_D: -1.5452 (-1.2867) Loss_G: 2.6994 (4.9408) D(x): 0.9873 D(G(z)): 0.0044 / 0.0677 Acc: 100.0000 (99.4820)\n",
      "[22/25][78/96] Loss_D: -1.5094 (-1.2869) Loss_G: 4.2172 (4.9404) D(x): 0.9973 D(G(z)): 0.0476 / 0.0215 Acc: 100.0000 (99.4823)\n",
      "[22/25][79/96] Loss_D: -1.2346 (-1.2868) Loss_G: 4.5179 (4.9403) D(x): 0.9953 D(G(z)): 0.2647 / 0.0171 Acc: 100.0000 (99.4825)\n",
      "[22/25][80/96] Loss_D: -1.3057 (-1.2868) Loss_G: 5.7309 (4.9406) D(x): 0.8714 D(G(z)): 0.0025 / 0.0048 Acc: 98.4375 (99.4820)\n",
      "[22/25][81/96] Loss_D: -1.2870 (-1.2868) Loss_G: 4.6091 (4.9405) D(x): 0.8717 D(G(z)): 0.0157 / 0.0147 Acc: 100.0000 (99.4823)\n",
      "[22/25][82/96] Loss_D: -1.4177 (-1.2869) Loss_G: 4.3429 (4.9402) D(x): 0.9830 D(G(z)): 0.0593 / 0.0168 Acc: 100.0000 (99.4825)\n",
      "[22/25][83/96] Loss_D: -1.4246 (-1.2870) Loss_G: 2.7269 (4.9392) D(x): 0.9936 D(G(z)): 0.0065 / 0.0939 Acc: 100.0000 (99.4827)\n",
      "[22/25][84/96] Loss_D: -1.4685 (-1.2870) Loss_G: 4.2270 (4.9389) D(x): 0.9769 D(G(z)): 0.0077 / 0.0177 Acc: 100.0000 (99.4830)\n",
      "[22/25][85/96] Loss_D: -1.3942 (-1.2871) Loss_G: 6.5571 (4.9396) D(x): 0.9939 D(G(z)): 0.0344 / 0.0023 Acc: 100.0000 (99.4832)\n",
      "[22/25][86/96] Loss_D: -1.3106 (-1.2871) Loss_G: 4.5445 (4.9394) D(x): 0.9897 D(G(z)): 0.0879 / 0.0156 Acc: 98.4375 (99.4827)\n",
      "[22/25][87/96] Loss_D: -1.3606 (-1.2871) Loss_G: 6.3882 (4.9401) D(x): 0.9466 D(G(z)): 0.0182 / 0.0022 Acc: 96.8750 (99.4815)\n",
      "[22/25][88/96] Loss_D: -1.5016 (-1.2872) Loss_G: 6.9222 (4.9410) D(x): 0.9813 D(G(z)): 0.0095 / 0.0016 Acc: 100.0000 (99.4818)\n",
      "[22/25][89/96] Loss_D: -0.6504 (-1.2869) Loss_G: -0.2969 (4.9386) D(x): 0.5549 D(G(z)): 0.0075 / 0.8736 Acc: 100.0000 (99.4820)\n",
      "[22/25][90/96] Loss_D: 1.7571 (-1.2856) Loss_G: 5.7480 (4.9390) D(x): 0.9999 D(G(z)): 0.9355 / 0.0044 Acc: 100.0000 (99.4822)\n",
      "[22/25][91/96] Loss_D: -0.9873 (-1.2854) Loss_G: 9.6172 (4.9411) D(x): 0.6809 D(G(z)): 0.0012 / 0.0001 Acc: 100.0000 (99.4825)\n",
      "[22/25][92/96] Loss_D: -0.9125 (-1.2853) Loss_G: 3.1917 (4.9403) D(x): 0.6380 D(G(z)): 0.0002 / 0.0416 Acc: 98.4375 (99.4820)\n",
      "[22/25][93/96] Loss_D: -0.5172 (-1.2849) Loss_G: 7.2194 (4.9413) D(x): 0.9879 D(G(z)): 0.5776 / 0.0012 Acc: 98.4375 (99.4815)\n",
      "[22/25][94/96] Loss_D: -0.3946 (-1.2845) Loss_G: 1.4095 (4.9397) D(x): 0.4046 D(G(z)): 0.0017 / 0.2044 Acc: 100.0000 (99.4818)\n",
      "[22/25][95/96] Loss_D: -1.0517 (-1.2844) Loss_G: 2.8585 (4.9388) D(x): 0.9950 D(G(z)): 0.2899 / 0.0877 Acc: 100.0000 (99.4820)\n",
      "[23/25][0/96] Loss_D: -1.2147 (-1.2844) Loss_G: 3.9238 (4.9383) D(x): 0.9917 D(G(z)): 0.1393 / 0.0398 Acc: 100.0000 (99.4822)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[23/25][1/96] Loss_D: -1.2867 (-1.2844) Loss_G: 5.1104 (4.9384) D(x): 0.9539 D(G(z)): 0.0056 / 0.0097 Acc: 100.0000 (99.4825)\n",
      "[23/25][2/96] Loss_D: -1.4716 (-1.2845) Loss_G: 3.7490 (4.9379) D(x): 0.9786 D(G(z)): 0.0058 / 0.0260 Acc: 100.0000 (99.4827)\n",
      "[23/25][3/96] Loss_D: -1.3634 (-1.2845) Loss_G: 3.7101 (4.9373) D(x): 0.9920 D(G(z)): 0.0185 / 0.0408 Acc: 100.0000 (99.4829)\n",
      "[23/25][4/96] Loss_D: -1.5036 (-1.2846) Loss_G: 2.5930 (4.9362) D(x): 0.9627 D(G(z)): 0.0038 / 0.0878 Acc: 100.0000 (99.4832)\n",
      "[23/25][5/96] Loss_D: -1.4176 (-1.2846) Loss_G: 2.2963 (4.9351) D(x): 0.9595 D(G(z)): 0.0079 / 0.1090 Acc: 100.0000 (99.4834)\n",
      "[23/25][6/96] Loss_D: -1.4160 (-1.2847) Loss_G: 6.0856 (4.9356) D(x): 0.9870 D(G(z)): 0.0778 / 0.0032 Acc: 100.0000 (99.4836)\n",
      "[23/25][7/96] Loss_D: -1.5019 (-1.2848) Loss_G: 3.4712 (4.9349) D(x): 0.9818 D(G(z)): 0.0253 / 0.0389 Acc: 100.0000 (99.4839)\n",
      "[23/25][8/96] Loss_D: -1.3753 (-1.2848) Loss_G: 2.4552 (4.9338) D(x): 0.8782 D(G(z)): 0.0015 / 0.0895 Acc: 98.4375 (99.4834)\n",
      "[23/25][9/96] Loss_D: -1.3358 (-1.2849) Loss_G: 3.6424 (4.9332) D(x): 0.9818 D(G(z)): 0.0484 / 0.0392 Acc: 98.4375 (99.4829)\n",
      "[23/25][10/96] Loss_D: -1.3859 (-1.2849) Loss_G: 3.7584 (4.9327) D(x): 0.9912 D(G(z)): 0.0768 / 0.0409 Acc: 100.0000 (99.4832)\n",
      "[23/25][11/96] Loss_D: -1.2129 (-1.2849) Loss_G: 6.9155 (4.9336) D(x): 0.9118 D(G(z)): 0.1215 / 0.0013 Acc: 100.0000 (99.4834)\n",
      "[23/25][12/96] Loss_D: -1.3281 (-1.2849) Loss_G: 5.0179 (4.9336) D(x): 0.8756 D(G(z)): 0.0084 / 0.0082 Acc: 100.0000 (99.4836)\n",
      "[23/25][13/96] Loss_D: -1.3517 (-1.2849) Loss_G: 2.3933 (4.9325) D(x): 0.9587 D(G(z)): 0.0379 / 0.1292 Acc: 100.0000 (99.4839)\n",
      "[23/25][14/96] Loss_D: -1.3000 (-1.2849) Loss_G: 4.3402 (4.9322) D(x): 0.9865 D(G(z)): 0.0427 / 0.0209 Acc: 100.0000 (99.4841)\n",
      "[23/25][15/96] Loss_D: -1.3088 (-1.2849) Loss_G: 5.4623 (4.9324) D(x): 0.9712 D(G(z)): 0.1213 / 0.0058 Acc: 100.0000 (99.4843)\n",
      "[23/25][16/96] Loss_D: -1.4206 (-1.2850) Loss_G: 6.1186 (4.9330) D(x): 0.9775 D(G(z)): 0.0091 / 0.0053 Acc: 100.0000 (99.4846)\n",
      "[23/25][17/96] Loss_D: -1.3816 (-1.2851) Loss_G: 3.6739 (4.9324) D(x): 0.8562 D(G(z)): 0.0034 / 0.0311 Acc: 100.0000 (99.4848)\n",
      "[23/25][18/96] Loss_D: -1.4681 (-1.2851) Loss_G: 5.7726 (4.9328) D(x): 0.9958 D(G(z)): 0.0254 / 0.0043 Acc: 100.0000 (99.4850)\n",
      "[23/25][19/96] Loss_D: -1.2517 (-1.2851) Loss_G: 2.7513 (4.9318) D(x): 0.9228 D(G(z)): 0.0343 / 0.0827 Acc: 100.0000 (99.4852)\n",
      "[23/25][20/96] Loss_D: -1.1913 (-1.2851) Loss_G: 6.6965 (4.9326) D(x): 0.9980 D(G(z)): 0.2368 / 0.0019 Acc: 98.4375 (99.4848)\n",
      "[23/25][21/96] Loss_D: -1.3610 (-1.2851) Loss_G: 5.2816 (4.9328) D(x): 0.8752 D(G(z)): 0.0084 / 0.0064 Acc: 100.0000 (99.4850)\n",
      "[23/25][22/96] Loss_D: -1.4681 (-1.2852) Loss_G: 8.3839 (4.9343) D(x): 0.9432 D(G(z)): 0.0005 / 0.0004 Acc: 100.0000 (99.4852)\n",
      "[23/25][23/96] Loss_D: -1.3331 (-1.2852) Loss_G: 4.3071 (4.9340) D(x): 0.9013 D(G(z)): 0.0084 / 0.0242 Acc: 100.0000 (99.4855)\n",
      "[23/25][24/96] Loss_D: -1.5415 (-1.2853) Loss_G: 2.6255 (4.9330) D(x): 0.9934 D(G(z)): 0.0287 / 0.0699 Acc: 100.0000 (99.4857)\n",
      "[23/25][25/96] Loss_D: -1.1437 (-1.2853) Loss_G: 4.2374 (4.9327) D(x): 0.9966 D(G(z)): 0.2272 / 0.0163 Acc: 100.0000 (99.4859)\n",
      "[23/25][26/96] Loss_D: -1.3235 (-1.2853) Loss_G: 5.7483 (4.9330) D(x): 0.8870 D(G(z)): 0.0007 / 0.0071 Acc: 100.0000 (99.4862)\n",
      "[23/25][27/96] Loss_D: -0.6143 (-1.2850) Loss_G: -0.4538 (4.9306) D(x): 0.4897 D(G(z)): 0.0010 / 0.9296 Acc: 100.0000 (99.4864)\n",
      "[23/25][28/96] Loss_D: 1.1716 (-1.2839) Loss_G: 6.4650 (4.9313) D(x): 0.9999 D(G(z)): 0.8623 / 0.0015 Acc: 100.0000 (99.4866)\n",
      "[23/25][29/96] Loss_D: -1.5953 (-1.2840) Loss_G: 6.2103 (4.9319) D(x): 0.9797 D(G(z)): 0.0077 / 0.0033 Acc: 100.0000 (99.4868)\n",
      "[23/25][30/96] Loss_D: 0.2851 (-1.2833) Loss_G: 0.3002 (4.9298) D(x): 0.2862 D(G(z)): 0.0009 / 0.5204 Acc: 96.8750 (99.4857)\n",
      "[23/25][31/96] Loss_D: 0.7150 (-1.2824) Loss_G: 2.5963 (4.9288) D(x): 0.9947 D(G(z)): 0.8212 / 0.0888 Acc: 98.4375 (99.4852)\n",
      "[23/25][32/96] Loss_D: -1.0265 (-1.2823) Loss_G: 3.3489 (4.9281) D(x): 0.7894 D(G(z)): 0.0492 / 0.0403 Acc: 100.0000 (99.4854)\n",
      "[23/25][33/96] Loss_D: -0.9533 (-1.2822) Loss_G: 2.8633 (4.9272) D(x): 0.6810 D(G(z)): 0.0621 / 0.0621 Acc: 100.0000 (99.4857)\n",
      "[23/25][34/96] Loss_D: -1.1597 (-1.2821) Loss_G: 2.6914 (4.9262) D(x): 0.9434 D(G(z)): 0.1784 / 0.0782 Acc: 100.0000 (99.4859)\n",
      "[23/25][35/96] Loss_D: -1.4121 (-1.2822) Loss_G: 4.2255 (4.9258) D(x): 0.9298 D(G(z)): 0.0522 / 0.0278 Acc: 100.0000 (99.4861)\n",
      "[23/25][36/96] Loss_D: -1.2632 (-1.2822) Loss_G: 3.2225 (4.9251) D(x): 0.9371 D(G(z)): 0.0853 / 0.0454 Acc: 98.4375 (99.4857)\n",
      "[23/25][37/96] Loss_D: -1.4203 (-1.2822) Loss_G: 3.1447 (4.9243) D(x): 0.8839 D(G(z)): 0.0172 / 0.0443 Acc: 100.0000 (99.4859)\n",
      "[23/25][38/96] Loss_D: -1.4448 (-1.2823) Loss_G: 3.5142 (4.9237) D(x): 0.9719 D(G(z)): 0.0525 / 0.0350 Acc: 100.0000 (99.4861)\n",
      "[23/25][39/96] Loss_D: -1.4296 (-1.2824) Loss_G: 4.6951 (4.9236) D(x): 0.9511 D(G(z)): 0.0137 / 0.0124 Acc: 100.0000 (99.4863)\n",
      "[23/25][40/96] Loss_D: -1.0577 (-1.2823) Loss_G: 5.4759 (4.9238) D(x): 0.9568 D(G(z)): 0.2981 / 0.0069 Acc: 100.0000 (99.4866)\n",
      "[23/25][41/96] Loss_D: -0.6999 (-1.2820) Loss_G: 1.1703 (4.9221) D(x): 0.5312 D(G(z)): 0.0027 / 0.2655 Acc: 100.0000 (99.4868)\n",
      "[23/25][42/96] Loss_D: -0.7588 (-1.2818) Loss_G: 4.8856 (4.9221) D(x): 0.9984 D(G(z)): 0.4762 / 0.0114 Acc: 98.4375 (99.4863)\n",
      "[23/25][43/96] Loss_D: -1.2371 (-1.2818) Loss_G: 4.3503 (4.9219) D(x): 0.8023 D(G(z)): 0.0043 / 0.0161 Acc: 100.0000 (99.4866)\n",
      "[23/25][44/96] Loss_D: -0.8526 (-1.2816) Loss_G: 6.4655 (4.9226) D(x): 0.8053 D(G(z)): 0.2295 / 0.0018 Acc: 100.0000 (99.4868)\n",
      "[23/25][45/96] Loss_D: -1.2752 (-1.2816) Loss_G: 2.6729 (4.9216) D(x): 0.8305 D(G(z)): 0.0031 / 0.0719 Acc: 98.4375 (99.4863)\n",
      "[23/25][46/96] Loss_D: -1.3645 (-1.2816) Loss_G: 1.3215 (4.9200) D(x): 0.9081 D(G(z)): 0.0456 / 0.2267 Acc: 98.4375 (99.4859)\n",
      "[23/25][47/96] Loss_D: -1.2397 (-1.2816) Loss_G: 6.3000 (4.9206) D(x): 0.9923 D(G(z)): 0.1733 / 0.0019 Acc: 100.0000 (99.4861)\n",
      "[23/25][48/96] Loss_D: -1.3912 (-1.2816) Loss_G: 5.7923 (4.9210) D(x): 0.9741 D(G(z)): 0.0188 / 0.0044 Acc: 93.7500 (99.4836)\n",
      "[23/25][49/96] Loss_D: -0.8994 (-1.2815) Loss_G: 0.3560 (4.9189) D(x): 0.6280 D(G(z)): 0.0030 / 0.5107 Acc: 100.0000 (99.4838)\n",
      "[23/25][50/96] Loss_D: -0.6310 (-1.2812) Loss_G: 6.1713 (4.9195) D(x): 0.9996 D(G(z)): 0.5106 / 0.0035 Acc: 100.0000 (99.4840)\n",
      "[23/25][51/96] Loss_D: -1.4741 (-1.2813) Loss_G: 9.7639 (4.9216) D(x): 0.9938 D(G(z)): 0.0040 / 0.0001 Acc: 98.4375 (99.4835)\n",
      "[23/25][52/96] Loss_D: -1.0083 (-1.2811) Loss_G: 1.5912 (4.9202) D(x): 0.6357 D(G(z)): 0.0011 / 0.1851 Acc: 98.4375 (99.4831)\n",
      "[23/25][53/96] Loss_D: -1.2723 (-1.2811) Loss_G: 1.6086 (4.9187) D(x): 0.9892 D(G(z)): 0.2263 / 0.1864 Acc: 100.0000 (99.4833)\n",
      "[23/25][54/96] Loss_D: -1.3965 (-1.2812) Loss_G: 2.1462 (4.9175) D(x): 0.9555 D(G(z)): 0.0079 / 0.1167 Acc: 98.4375 (99.4828)\n",
      "[23/25][55/96] Loss_D: -1.4107 (-1.2812) Loss_G: 2.9968 (4.9166) D(x): 0.9108 D(G(z)): 0.0041 / 0.0480 Acc: 100.0000 (99.4831)\n",
      "[23/25][56/96] Loss_D: -1.5179 (-1.2813) Loss_G: 2.0887 (4.9154) D(x): 0.9924 D(G(z)): 0.0055 / 0.1187 Acc: 100.0000 (99.4833)\n",
      "[23/25][57/96] Loss_D: -1.4822 (-1.2814) Loss_G: 2.3308 (4.9142) D(x): 0.9885 D(G(z)): 0.0622 / 0.0862 Acc: 100.0000 (99.4835)\n",
      "[23/25][58/96] Loss_D: -1.4087 (-1.2815) Loss_G: 6.0329 (4.9147) D(x): 0.9923 D(G(z)): 0.0736 / 0.0031 Acc: 100.0000 (99.4838)\n",
      "[23/25][59/96] Loss_D: -1.2928 (-1.2815) Loss_G: 2.7265 (4.9138) D(x): 0.8502 D(G(z)): 0.0191 / 0.0795 Acc: 100.0000 (99.4840)\n",
      "[23/25][60/96] Loss_D: -1.4529 (-1.2816) Loss_G: 2.9524 (4.9129) D(x): 0.9600 D(G(z)): 0.0719 / 0.0569 Acc: 100.0000 (99.4842)\n",
      "[23/25][61/96] Loss_D: -1.5451 (-1.2817) Loss_G: 5.1361 (4.9130) D(x): 0.9948 D(G(z)): 0.0118 / 0.0061 Acc: 98.4375 (99.4838)\n",
      "[23/25][62/96] Loss_D: -1.2143 (-1.2817) Loss_G: 6.3690 (4.9136) D(x): 0.9283 D(G(z)): 0.1050 / 0.0030 Acc: 100.0000 (99.4840)\n",
      "[23/25][63/96] Loss_D: -1.4245 (-1.2817) Loss_G: 6.1375 (4.9142) D(x): 0.9482 D(G(z)): 0.0178 / 0.0034 Acc: 100.0000 (99.4842)\n",
      "[23/25][64/96] Loss_D: -1.4324 (-1.2818) Loss_G: 1.9058 (4.9129) D(x): 0.9461 D(G(z)): 0.0237 / 0.1665 Acc: 100.0000 (99.4844)\n",
      "[23/25][65/96] Loss_D: -1.3486 (-1.2818) Loss_G: 5.9469 (4.9133) D(x): 0.9969 D(G(z)): 0.0968 / 0.0037 Acc: 98.4375 (99.4840)\n",
      "[23/25][66/96] Loss_D: -1.3261 (-1.2818) Loss_G: 1.9739 (4.9120) D(x): 0.8638 D(G(z)): 0.0050 / 0.1328 Acc: 96.8750 (99.4828)\n",
      "[23/25][67/96] Loss_D: -1.4595 (-1.2819) Loss_G: 3.0502 (4.9112) D(x): 0.9856 D(G(z)): 0.0643 / 0.0528 Acc: 100.0000 (99.4831)\n",
      "[23/25][68/96] Loss_D: -1.3596 (-1.2819) Loss_G: 5.7062 (4.9116) D(x): 0.9941 D(G(z)): 0.0091 / 0.0056 Acc: 98.4375 (99.4826)\n",
      "[23/25][69/96] Loss_D: -1.4212 (-1.2820) Loss_G: 4.8203 (4.9115) D(x): 0.9872 D(G(z)): 0.0030 / 0.0090 Acc: 98.4375 (99.4821)\n",
      "[23/25][70/96] Loss_D: -1.3527 (-1.2820) Loss_G: 3.3348 (4.9108) D(x): 0.9559 D(G(z)): 0.0464 / 0.0429 Acc: 98.4375 (99.4817)\n",
      "[23/25][71/96] Loss_D: -1.5135 (-1.2821) Loss_G: 4.3700 (4.9106) D(x): 0.9616 D(G(z)): 0.0199 / 0.0141 Acc: 100.0000 (99.4819)\n",
      "[23/25][72/96] Loss_D: -1.3973 (-1.2822) Loss_G: 5.6415 (4.9109) D(x): 0.9697 D(G(z)): 0.0698 / 0.0044 Acc: 100.0000 (99.4821)\n",
      "[23/25][73/96] Loss_D: -1.4363 (-1.2823) Loss_G: 4.6172 (4.9108) D(x): 0.9542 D(G(z)): 0.0123 / 0.0133 Acc: 100.0000 (99.4824)\n",
      "[23/25][74/96] Loss_D: -1.2027 (-1.2822) Loss_G: 6.0929 (4.9113) D(x): 0.9767 D(G(z)): 0.1564 / 0.0028 Acc: 100.0000 (99.4826)\n",
      "[23/25][75/96] Loss_D: -1.2567 (-1.2822) Loss_G: 4.4945 (4.9111) D(x): 0.7886 D(G(z)): 0.0019 / 0.0183 Acc: 98.4375 (99.4821)\n",
      "[23/25][76/96] Loss_D: -1.4567 (-1.2823) Loss_G: 2.3739 (4.9100) D(x): 0.9961 D(G(z)): 0.0230 / 0.1168 Acc: 100.0000 (99.4824)\n",
      "[23/25][77/96] Loss_D: -1.0846 (-1.2822) Loss_G: 8.8317 (4.9117) D(x): 0.9976 D(G(z)): 0.2437 / 0.0002 Acc: 100.0000 (99.4826)\n",
      "[23/25][78/96] Loss_D: -1.3654 (-1.2822) Loss_G: 6.7727 (4.9125) D(x): 0.9169 D(G(z)): 0.0085 / 0.0019 Acc: 100.0000 (99.4828)\n",
      "[23/25][79/96] Loss_D: -1.0843 (-1.2822) Loss_G: 3.6039 (4.9120) D(x): 0.6738 D(G(z)): 0.0006 / 0.0356 Acc: 100.0000 (99.4830)\n",
      "[23/25][80/96] Loss_D: -1.4359 (-1.2822) Loss_G: 1.3627 (4.9104) D(x): 0.9981 D(G(z)): 0.0936 / 0.2471 Acc: 100.0000 (99.4833)\n",
      "[23/25][81/96] Loss_D: -1.2574 (-1.2822) Loss_G: 3.5596 (4.9098) D(x): 0.9980 D(G(z)): 0.2647 / 0.0299 Acc: 100.0000 (99.4835)\n",
      "[23/25][82/96] Loss_D: -1.5020 (-1.2823) Loss_G: 6.8888 (4.9107) D(x): 0.9269 D(G(z)): 0.0267 / 0.0013 Acc: 100.0000 (99.4837)\n",
      "[23/25][83/96] Loss_D: -1.4870 (-1.2824) Loss_G: 2.1510 (4.9095) D(x): 0.8595 D(G(z)): 0.0017 / 0.1355 Acc: 98.4375 (99.4833)\n",
      "[23/25][84/96] Loss_D: -1.1105 (-1.2823) Loss_G: 8.0187 (4.9108) D(x): 0.9366 D(G(z)): 0.2527 / 0.0003 Acc: 100.0000 (99.4835)\n",
      "[23/25][85/96] Loss_D: -1.1870 (-1.2823) Loss_G: 3.1502 (4.9101) D(x): 0.8061 D(G(z)): 0.0004 / 0.0498 Acc: 100.0000 (99.4837)\n",
      "[23/25][86/96] Loss_D: -1.3073 (-1.2823) Loss_G: 3.2322 (4.9093) D(x): 0.8892 D(G(z)): 0.0032 / 0.0544 Acc: 98.4375 (99.4833)\n",
      "[23/25][87/96] Loss_D: -1.3293 (-1.2823) Loss_G: 2.9093 (4.9085) D(x): 0.9941 D(G(z)): 0.0874 / 0.0633 Acc: 100.0000 (99.4835)\n",
      "[23/25][88/96] Loss_D: -1.2974 (-1.2823) Loss_G: 6.9219 (4.9093) D(x): 0.9977 D(G(z)): 0.2157 / 0.0012 Acc: 100.0000 (99.4837)\n",
      "[23/25][89/96] Loss_D: -1.3703 (-1.2824) Loss_G: 7.8687 (4.9106) D(x): 0.8878 D(G(z)): 0.0012 / 0.0005 Acc: 98.4375 (99.4832)\n",
      "[23/25][90/96] Loss_D: -1.4389 (-1.2824) Loss_G: 4.6318 (4.9105) D(x): 0.8840 D(G(z)): 0.0005 / 0.0134 Acc: 100.0000 (99.4835)\n",
      "[23/25][91/96] Loss_D: -1.3015 (-1.2824) Loss_G: 3.3352 (4.9098) D(x): 0.9808 D(G(z)): 0.1126 / 0.0374 Acc: 100.0000 (99.4837)\n",
      "[23/25][92/96] Loss_D: -1.3932 (-1.2825) Loss_G: 1.7641 (4.9085) D(x): 0.9012 D(G(z)): 0.0072 / 0.1667 Acc: 100.0000 (99.4839)\n",
      "[23/25][93/96] Loss_D: -1.3174 (-1.2825) Loss_G: 4.6178 (4.9083) D(x): 0.9794 D(G(z)): 0.0593 / 0.0159 Acc: 98.4375 (99.4835)\n",
      "[23/25][94/96] Loss_D: -1.3289 (-1.2825) Loss_G: 4.1066 (4.9080) D(x): 0.9536 D(G(z)): 0.0805 / 0.0264 Acc: 100.0000 (99.4837)\n",
      "[23/25][95/96] Loss_D: -1.4266 (-1.2826) Loss_G: 6.0047 (4.9085) D(x): 0.9658 D(G(z)): 0.0202 / 0.0041 Acc: 98.4375 (99.4832)\n",
      "[24/25][0/96] Loss_D: -1.3405 (-1.2826) Loss_G: 6.7176 (4.9092) D(x): 0.9882 D(G(z)): 0.0216 / 0.0016 Acc: 96.8750 (99.4821)\n",
      "Label for eval = [0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 0 0 0]\n",
      "[24/25][1/96] Loss_D: -1.2031 (-1.2826) Loss_G: 4.5149 (4.9091) D(x): 0.8994 D(G(z)): 0.0115 / 0.0143 Acc: 98.4375 (99.4817)\n",
      "[24/25][2/96] Loss_D: -1.4027 (-1.2826) Loss_G: 1.1455 (4.9074) D(x): 0.9680 D(G(z)): 0.0163 / 0.2880 Acc: 98.4375 (99.4812)\n",
      "[24/25][3/96] Loss_D: -1.3951 (-1.2827) Loss_G: 2.4084 (4.9064) D(x): 0.9984 D(G(z)): 0.0520 / 0.0971 Acc: 100.0000 (99.4814)\n",
      "[24/25][4/96] Loss_D: -1.4491 (-1.2827) Loss_G: 3.8812 (4.9059) D(x): 0.9686 D(G(z)): 0.0017 / 0.0260 Acc: 100.0000 (99.4816)\n",
      "[24/25][5/96] Loss_D: -1.4542 (-1.2828) Loss_G: 1.6925 (4.9045) D(x): 0.9958 D(G(z)): 0.0251 / 0.2183 Acc: 100.0000 (99.4819)\n",
      "[24/25][6/96] Loss_D: -1.3219 (-1.2828) Loss_G: 4.2693 (4.9042) D(x): 0.9932 D(G(z)): 0.0429 / 0.0201 Acc: 98.4375 (99.4814)\n",
      "[24/25][7/96] Loss_D: -1.5937 (-1.2830) Loss_G: 4.8070 (4.9042) D(x): 0.9976 D(G(z)): 0.0128 / 0.0103 Acc: 100.0000 (99.4816)\n",
      "[24/25][8/96] Loss_D: -1.0114 (-1.2828) Loss_G: 1.8703 (4.9029) D(x): 0.7503 D(G(z)): 0.0305 / 0.1651 Acc: 98.4375 (99.4812)\n",
      "[24/25][9/96] Loss_D: 1.3049 (-1.2817) Loss_G: 15.4788 (4.9075) D(x): 0.9960 D(G(z)): 0.8770 / 0.0000 Acc: 100.0000 (99.4814)\n",
      "[24/25][10/96] Loss_D: 4.9771 (-1.2790) Loss_G: 0.5260 (4.9056) D(x): 0.0030 D(G(z)): 0.0000 / 0.4440 Acc: 100.0000 (99.4816)\n",
      "[24/25][11/96] Loss_D: -0.9431 (-1.2789) Loss_G: 0.2220 (4.9035) D(x): 0.9281 D(G(z)): 0.3453 / 0.5560 Acc: 98.4375 (99.4812)\n",
      "[24/25][12/96] Loss_D: -0.8451 (-1.2787) Loss_G: 3.3042 (4.9029) D(x): 0.9717 D(G(z)): 0.3900 / 0.0572 Acc: 98.4375 (99.4807)\n",
      "[24/25][13/96] Loss_D: -1.0468 (-1.2786) Loss_G: 2.5445 (4.9018) D(x): 0.7423 D(G(z)): 0.0768 / 0.0898 Acc: 100.0000 (99.4810)\n",
      "[24/25][14/96] Loss_D: -1.3446 (-1.2786) Loss_G: 2.1793 (4.9007) D(x): 0.8866 D(G(z)): 0.0547 / 0.1152 Acc: 100.0000 (99.4812)\n",
      "[24/25][15/96] Loss_D: -1.0704 (-1.2785) Loss_G: 5.6217 (4.9010) D(x): 0.9615 D(G(z)): 0.3247 / 0.0040 Acc: 100.0000 (99.4814)\n",
      "[24/25][16/96] Loss_D: -1.2971 (-1.2785) Loss_G: 5.0245 (4.9010) D(x): 0.8785 D(G(z)): 0.0321 / 0.0136 Acc: 96.8750 (99.4803)\n",
      "[24/25][17/96] Loss_D: -0.6270 (-1.2783) Loss_G: 0.3144 (4.8991) D(x): 0.5441 D(G(z)): 0.0444 / 0.5254 Acc: 100.0000 (99.4805)\n",
      "[24/25][18/96] Loss_D: -0.7542 (-1.2780) Loss_G: 2.9519 (4.8982) D(x): 0.9974 D(G(z)): 0.4706 / 0.0589 Acc: 100.0000 (99.4807)\n",
      "[24/25][19/96] Loss_D: -1.4919 (-1.2781) Loss_G: 4.2311 (4.8979) D(x): 0.9879 D(G(z)): 0.0113 / 0.0201 Acc: 100.0000 (99.4810)\n",
      "[24/25][20/96] Loss_D: -1.3553 (-1.2782) Loss_G: 4.9941 (4.8980) D(x): 0.9229 D(G(z)): 0.0134 / 0.0154 Acc: 98.4375 (99.4805)\n",
      "[24/25][21/96] Loss_D: -1.3773 (-1.2782) Loss_G: 3.9185 (4.8975) D(x): 0.9433 D(G(z)): 0.0441 / 0.0256 Acc: 100.0000 (99.4807)\n",
      "[24/25][22/96] Loss_D: -0.9577 (-1.2781) Loss_G: 0.9339 (4.8958) D(x): 0.6616 D(G(z)): 0.0305 / 0.3238 Acc: 100.0000 (99.4810)\n",
      "[24/25][23/96] Loss_D: -1.2373 (-1.2780) Loss_G: 2.1517 (4.8947) D(x): 0.9987 D(G(z)): 0.1585 / 0.1345 Acc: 100.0000 (99.4812)\n",
      "[24/25][24/96] Loss_D: -1.2040 (-1.2780) Loss_G: 7.0161 (4.8956) D(x): 0.9745 D(G(z)): 0.2944 / 0.0009 Acc: 100.0000 (99.4814)\n",
      "[24/25][25/96] Loss_D: -0.9866 (-1.2779) Loss_G: 3.0421 (4.8948) D(x): 0.6542 D(G(z)): 0.0008 / 0.0584 Acc: 100.0000 (99.4816)\n",
      "[24/25][26/96] Loss_D: -1.4816 (-1.2780) Loss_G: 1.9816 (4.8935) D(x): 0.9451 D(G(z)): 0.0449 / 0.1265 Acc: 100.0000 (99.4818)\n",
      "[24/25][27/96] Loss_D: -1.3900 (-1.2780) Loss_G: 1.9567 (4.8923) D(x): 0.9968 D(G(z)): 0.1478 / 0.1430 Acc: 100.0000 (99.4821)\n",
      "[24/25][28/96] Loss_D: -1.4180 (-1.2781) Loss_G: 6.4025 (4.8929) D(x): 0.9944 D(G(z)): 0.0702 / 0.0021 Acc: 100.0000 (99.4823)\n",
      "[24/25][29/96] Loss_D: -1.2986 (-1.2781) Loss_G: 3.3691 (4.8923) D(x): 0.8743 D(G(z)): 0.0128 / 0.0450 Acc: 100.0000 (99.4825)\n",
      "[24/25][30/96] Loss_D: -1.3718 (-1.2781) Loss_G: 2.3071 (4.8912) D(x): 0.8828 D(G(z)): 0.0061 / 0.1067 Acc: 100.0000 (99.4827)\n",
      "[24/25][31/96] Loss_D: -1.1790 (-1.2781) Loss_G: 3.7942 (4.8907) D(x): 0.9988 D(G(z)): 0.1822 / 0.0335 Acc: 100.0000 (99.4830)\n",
      "[24/25][32/96] Loss_D: -1.3558 (-1.2781) Loss_G: 6.5007 (4.8914) D(x): 0.9860 D(G(z)): 0.1372 / 0.0027 Acc: 96.8750 (99.4818)\n",
      "[24/25][33/96] Loss_D: -0.7347 (-1.2779) Loss_G: 2.2587 (4.8903) D(x): 0.5445 D(G(z)): 0.0058 / 0.1148 Acc: 100.0000 (99.4821)\n",
      "[24/25][34/96] Loss_D: -1.1915 (-1.2779) Loss_G: 0.6666 (4.8884) D(x): 0.9978 D(G(z)): 0.1673 / 0.4150 Acc: 100.0000 (99.4823)\n",
      "[24/25][35/96] Loss_D: -1.1154 (-1.2778) Loss_G: 6.6831 (4.8892) D(x): 0.9981 D(G(z)): 0.2984 / 0.0018 Acc: 100.0000 (99.4825)\n",
      "[24/25][36/96] Loss_D: -1.5031 (-1.2779) Loss_G: 8.7128 (4.8908) D(x): 0.9413 D(G(z)): 0.0115 / 0.0003 Acc: 100.0000 (99.4827)\n",
      "[24/25][37/96] Loss_D: -1.1922 (-1.2778) Loss_G: 6.6883 (4.8916) D(x): 0.8321 D(G(z)): 0.0005 / 0.0023 Acc: 98.4375 (99.4823)\n",
      "[24/25][38/96] Loss_D: -1.4733 (-1.2779) Loss_G: 4.5748 (4.8915) D(x): 0.9659 D(G(z)): 0.0066 / 0.0152 Acc: 100.0000 (99.4825)\n",
      "[24/25][39/96] Loss_D: -1.4178 (-1.2780) Loss_G: 5.8080 (4.8919) D(x): 0.9944 D(G(z)): 0.1144 / 0.0041 Acc: 100.0000 (99.4827)\n",
      "[24/25][40/96] Loss_D: -1.0382 (-1.2779) Loss_G: 5.7435 (4.8922) D(x): 0.8518 D(G(z)): 0.2074 / 0.0050 Acc: 98.4375 (99.4823)\n",
      "[24/25][41/96] Loss_D: -0.9087 (-1.2777) Loss_G: 0.7187 (4.8905) D(x): 0.6403 D(G(z)): 0.0101 / 0.3870 Acc: 98.4375 (99.4818)\n",
      "[24/25][42/96] Loss_D: -0.8477 (-1.2775) Loss_G: 6.2139 (4.8910) D(x): 0.9987 D(G(z)): 0.4213 / 0.0026 Acc: 100.0000 (99.4821)\n",
      "[24/25][43/96] Loss_D: -1.4891 (-1.2776) Loss_G: 5.4952 (4.8913) D(x): 0.9968 D(G(z)): 0.0076 / 0.0046 Acc: 98.4375 (99.4816)\n",
      "[24/25][44/96] Loss_D: -1.1381 (-1.2776) Loss_G: 3.4293 (4.8907) D(x): 0.6834 D(G(z)): 0.0009 / 0.0416 Acc: 100.0000 (99.4818)\n",
      "[24/25][45/96] Loss_D: -1.4267 (-1.2776) Loss_G: 4.3118 (4.8904) D(x): 0.9823 D(G(z)): 0.0225 / 0.0163 Acc: 100.0000 (99.4820)\n",
      "[24/25][46/96] Loss_D: -1.4313 (-1.2777) Loss_G: 1.8783 (4.8891) D(x): 0.9993 D(G(z)): 0.0630 / 0.1522 Acc: 100.0000 (99.4823)\n",
      "[24/25][47/96] Loss_D: -0.9443 (-1.2776) Loss_G: 6.9619 (4.8900) D(x): 0.9985 D(G(z)): 0.3349 / 0.0013 Acc: 100.0000 (99.4825)\n",
      "[24/25][48/96] Loss_D: -1.3217 (-1.2776) Loss_G: 8.9129 (4.8917) D(x): 0.8217 D(G(z)): 0.0003 / 0.0002 Acc: 98.4375 (99.4820)\n",
      "[24/25][49/96] Loss_D: -0.9280 (-1.2774) Loss_G: 0.2696 (4.8898) D(x): 0.6376 D(G(z)): 0.0006 / 0.5533 Acc: 100.0000 (99.4823)\n",
      "[24/25][50/96] Loss_D: -1.2108 (-1.2774) Loss_G: 0.5566 (4.8879) D(x): 0.9997 D(G(z)): 0.1677 / 0.4353 Acc: 100.0000 (99.4825)\n",
      "[24/25][51/96] Loss_D: -0.8691 (-1.2772) Loss_G: 6.9404 (4.8888) D(x): 0.9998 D(G(z)): 0.3852 / 0.0015 Acc: 100.0000 (99.4827)\n",
      "[24/25][52/96] Loss_D: -0.8024 (-1.2770) Loss_G: 3.0025 (4.8880) D(x): 0.5466 D(G(z)): 0.0078 / 0.0553 Acc: 100.0000 (99.4829)\n",
      "[24/25][53/96] Loss_D: -1.4513 (-1.2771) Loss_G: 0.9920 (4.8863) D(x): 0.9792 D(G(z)): 0.0946 / 0.3033 Acc: 100.0000 (99.4831)\n",
      "[24/25][54/96] Loss_D: -1.3762 (-1.2771) Loss_G: 3.1726 (4.8856) D(x): 0.9961 D(G(z)): 0.0931 / 0.0471 Acc: 100.0000 (99.4834)\n",
      "[24/25][55/96] Loss_D: -1.4095 (-1.2772) Loss_G: 4.3445 (4.8854) D(x): 0.9748 D(G(z)): 0.0452 / 0.0167 Acc: 100.0000 (99.4836)\n",
      "[24/25][56/96] Loss_D: -1.4368 (-1.2773) Loss_G: 5.6009 (4.8857) D(x): 0.9858 D(G(z)): 0.0169 / 0.0070 Acc: 98.4375 (99.4831)\n",
      "[24/25][57/96] Loss_D: -1.5345 (-1.2774) Loss_G: 2.6781 (4.8847) D(x): 0.9955 D(G(z)): 0.0077 / 0.0669 Acc: 100.0000 (99.4834)\n",
      "[24/25][58/96] Loss_D: -1.4725 (-1.2775) Loss_G: 4.9723 (4.8848) D(x): 0.9887 D(G(z)): 0.0294 / 0.0110 Acc: 100.0000 (99.4836)\n",
      "[24/25][59/96] Loss_D: -1.3821 (-1.2775) Loss_G: 4.2058 (4.8845) D(x): 0.9665 D(G(z)): 0.0043 / 0.0171 Acc: 96.8750 (99.4825)\n",
      "[24/25][60/96] Loss_D: -1.3740 (-1.2775) Loss_G: 4.6855 (4.8844) D(x): 0.9710 D(G(z)): 0.0020 / 0.0166 Acc: 100.0000 (99.4827)\n",
      "[24/25][61/96] Loss_D: -1.5134 (-1.2776) Loss_G: 3.8794 (4.8840) D(x): 0.9628 D(G(z)): 0.0086 / 0.0255 Acc: 100.0000 (99.4829)\n",
      "[24/25][62/96] Loss_D: -1.2958 (-1.2777) Loss_G: 5.8899 (4.8844) D(x): 0.9933 D(G(z)): 0.1160 / 0.0040 Acc: 100.0000 (99.4831)\n",
      "[24/25][63/96] Loss_D: -1.2294 (-1.2776) Loss_G: 5.6402 (4.8847) D(x): 0.8743 D(G(z)): 0.0442 / 0.0049 Acc: 100.0000 (99.4833)\n",
      "[24/25][64/96] Loss_D: -1.4329 (-1.2777) Loss_G: 4.7118 (4.8847) D(x): 0.9950 D(G(z)): 0.0716 / 0.0200 Acc: 100.0000 (99.4836)\n",
      "[24/25][65/96] Loss_D: -1.4511 (-1.2778) Loss_G: 3.8598 (4.8842) D(x): 0.9113 D(G(z)): 0.0129 / 0.0278 Acc: 100.0000 (99.4838)\n",
      "[24/25][66/96] Loss_D: -1.3217 (-1.2778) Loss_G: 3.4522 (4.8836) D(x): 0.9580 D(G(z)): 0.0374 / 0.0465 Acc: 98.4375 (99.4833)\n",
      "[24/25][67/96] Loss_D: -1.4863 (-1.2779) Loss_G: 4.6442 (4.8835) D(x): 0.9657 D(G(z)): 0.0366 / 0.0137 Acc: 100.0000 (99.4836)\n",
      "[24/25][68/96] Loss_D: -1.4843 (-1.2780) Loss_G: 4.0528 (4.8832) D(x): 0.9823 D(G(z)): 0.0270 / 0.0186 Acc: 100.0000 (99.4838)\n",
      "[24/25][69/96] Loss_D: -1.4183 (-1.2780) Loss_G: 4.4601 (4.8830) D(x): 0.9846 D(G(z)): 0.0326 / 0.0126 Acc: 100.0000 (99.4840)\n",
      "[24/25][70/96] Loss_D: -1.4001 (-1.2781) Loss_G: 3.7022 (4.8825) D(x): 0.9734 D(G(z)): 0.0518 / 0.0284 Acc: 100.0000 (99.4842)\n",
      "[24/25][71/96] Loss_D: -1.4967 (-1.2782) Loss_G: 4.8260 (4.8825) D(x): 0.9486 D(G(z)): 0.0240 / 0.0089 Acc: 100.0000 (99.4844)\n",
      "[24/25][72/96] Loss_D: -1.2874 (-1.2782) Loss_G: 4.2953 (4.8822) D(x): 0.8964 D(G(z)): 0.0821 / 0.0167 Acc: 100.0000 (99.4846)\n",
      "[24/25][73/96] Loss_D: -1.2427 (-1.2782) Loss_G: 4.9001 (4.8822) D(x): 0.9466 D(G(z)): 0.2087 / 0.0079 Acc: 98.4375 (99.4842)\n",
      "[24/25][74/96] Loss_D: -1.3756 (-1.2782) Loss_G: 6.8960 (4.8831) D(x): 0.8684 D(G(z)): 0.0005 / 0.0011 Acc: 100.0000 (99.4844)\n",
      "[24/25][75/96] Loss_D: -1.4039 (-1.2783) Loss_G: 3.6631 (4.8826) D(x): 0.8894 D(G(z)): 0.0017 / 0.0260 Acc: 100.0000 (99.4846)\n",
      "[24/25][76/96] Loss_D: -1.4225 (-1.2783) Loss_G: 3.6024 (4.8820) D(x): 0.9928 D(G(z)): 0.0356 / 0.0341 Acc: 100.0000 (99.4849)\n",
      "[24/25][77/96] Loss_D: -1.4361 (-1.2784) Loss_G: 2.3973 (4.8810) D(x): 0.9924 D(G(z)): 0.0384 / 0.0843 Acc: 98.4375 (99.4844)\n",
      "[24/25][78/96] Loss_D: -1.3579 (-1.2784) Loss_G: 3.4016 (4.8804) D(x): 0.9842 D(G(z)): 0.0832 / 0.0352 Acc: 96.8750 (99.4833)\n",
      "[24/25][79/96] Loss_D: -1.1662 (-1.2784) Loss_G: 1.3308 (4.8789) D(x): 0.7985 D(G(z)): 0.0208 / 0.2676 Acc: 98.4375 (99.4829)\n",
      "[24/25][80/96] Loss_D: -1.3855 (-1.2784) Loss_G: -0.0581 (4.8768) D(x): 0.9995 D(G(z)): 0.1104 / 0.6997 Acc: 100.0000 (99.4831)\n",
      "[24/25][81/96] Loss_D: -1.2818 (-1.2784) Loss_G: 5.9408 (4.8773) D(x): 0.9969 D(G(z)): 0.1925 / 0.0030 Acc: 100.0000 (99.4833)\n",
      "[24/25][82/96] Loss_D: -0.6468 (-1.2781) Loss_G: 0.5710 (4.8754) D(x): 0.5276 D(G(z)): 0.0022 / 0.4268 Acc: 100.0000 (99.4835)\n",
      "[24/25][83/96] Loss_D: 0.2161 (-1.2775) Loss_G: 6.4956 (4.8761) D(x): 0.9977 D(G(z)): 0.6369 / 0.0022 Acc: 98.4375 (99.4831)\n",
      "[24/25][84/96] Loss_D: 0.2087 (-1.2769) Loss_G: -0.1623 (4.8740) D(x): 0.2591 D(G(z)): 0.0038 / 0.7662 Acc: 100.0000 (99.4833)\n",
      "[24/25][85/96] Loss_D: 0.7007 (-1.2761) Loss_G: 1.2731 (4.8725) D(x): 0.9977 D(G(z)): 0.8140 / 0.2782 Acc: 100.0000 (99.4835)\n",
      "[24/25][86/96] Loss_D: -1.2419 (-1.2761) Loss_G: 7.0191 (4.8734) D(x): 0.9469 D(G(z)): 0.1175 / 0.0016 Acc: 100.0000 (99.4837)\n",
      "[24/25][87/96] Loss_D: 0.5976 (-1.2753) Loss_G: 0.7192 (4.8717) D(x): 0.2195 D(G(z)): 0.0050 / 0.3722 Acc: 98.4375 (99.4833)\n",
      "[24/25][88/96] Loss_D: -0.0880 (-1.2748) Loss_G: 4.1327 (4.8714) D(x): 0.9941 D(G(z)): 0.6732 / 0.0262 Acc: 100.0000 (99.4835)\n",
      "[24/25][89/96] Loss_D: -1.2445 (-1.2748) Loss_G: 2.6590 (4.8704) D(x): 0.9275 D(G(z)): 0.0703 / 0.0718 Acc: 100.0000 (99.4837)\n",
      "[24/25][90/96] Loss_D: -1.1004 (-1.2747) Loss_G: 2.3951 (4.8694) D(x): 0.6654 D(G(z)): 0.0086 / 0.0947 Acc: 100.0000 (99.4840)\n",
      "[24/25][91/96] Loss_D: -1.2541 (-1.2747) Loss_G: 2.3743 (4.8684) D(x): 0.9396 D(G(z)): 0.1084 / 0.0981 Acc: 98.4375 (99.4835)\n",
      "[24/25][92/96] Loss_D: -1.0602 (-1.2746) Loss_G: 5.2999 (4.8685) D(x): 0.9776 D(G(z)): 0.2739 / 0.0064 Acc: 100.0000 (99.4837)\n",
      "[24/25][93/96] Loss_D: -1.4831 (-1.2747) Loss_G: 4.4142 (4.8684) D(x): 0.9550 D(G(z)): 0.0432 / 0.0169 Acc: 100.0000 (99.4839)\n",
      "[24/25][94/96] Loss_D: -1.1208 (-1.2746) Loss_G: 3.0867 (4.8676) D(x): 0.7429 D(G(z)): 0.0067 / 0.0495 Acc: 100.0000 (99.4842)\n",
      "[24/25][95/96] Loss_D: -1.2216 (-1.2746) Loss_G: 3.5902 (4.8671) D(x): 0.9071 D(G(z)): 0.1093 / 0.0384 Acc: 100.0000 (99.4844)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['niter']):  # 循环遍历每个 epoch（训练轮数）\n",
    "    for i, data in enumerate(train_loader, 0):  # 遍历数据加载器中的每个批次\n",
    "        ############################\n",
    "        # (1) 更新判别器网络：最大化 log(D(x)) + log(1 - D(G(z)))\n",
    "        ############################\n",
    "        # 用真实数据训练判别器\n",
    "        netD.zero_grad()  # 清空判别器的梯度\n",
    "        real_cpu, label = data  # 获取当前批次的真实图像和对应标签\n",
    "        batch_size = real_cpu.size(0)  # 获取当前批次的样本数\n",
    "        if args['cuda']:\n",
    "            real_cpu = real_cpu.cuda()  # 如果使用 CUDA，将真实图像移动到 GPU\n",
    "        input.data.resize_as_(real_cpu).copy_(real_cpu)  # 调整 input 张量大小以匹配真实图像，并复制图像数据\n",
    "        dis_label.data.resize_(batch_size).fill_(real_label)  # 将判别器标签调整为当前批次大小，并填充为真实标签\n",
    "        aux_label.data.resize_(batch_size).copy_(label)  # 将辅助分类标签调整为当前批次大小，并复制真实标签\n",
    "        dis_output, aux_output = netD(input)  # 将真实图像输入判别器，获得真伪输出和辅助分类输出\n",
    "\n",
    "        dis_errD_real = dis_criterion(dis_output, dis_label)  # 计算真实图像的判别器损失\n",
    "        aux_errD_real = aux_criterion(aux_output, aux_label)  # 计算真实图像的辅助分类损失\n",
    "        errD_real = dis_errD_real + aux_errD_real  # 计算真实图像总损失（判别器损失 + 辅助分类损失）\n",
    "        errD_real.backward()  # 对真实图像损失进行反向传播，计算梯度\n",
    "        D_x = dis_output.data.mean()  # 计算真实图像输出的平均值，用于监控\n",
    "\n",
    "        # 计算当前批次的分类准确率\n",
    "        accuracy = compute_acc(aux_output, aux_label)  # 调用 compute_acc 函数计算辅助分类器的准确率\n",
    "\n",
    "        # 用生成的假数据训练判别器\n",
    "        noise.data.resize_(batch_size, nz, 1, 1).normal_(0, 1)  # 生成随机噪声，并调整其形状为 [batchSize, nz, 1, 1]\n",
    "        label = np.random.randint(0, num_classes, batch_size)  # 随机生成当前批次的标签，用作条件输入\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))  # 生成噪声数组，尺寸为 [batchSize, nz]\n",
    "        class_onehot = np.zeros((batch_size, num_classes))  # 创建一个全零矩阵，用于生成 one-hot 编码\n",
    "        class_onehot[np.arange(batch_size), label] = 1  # 根据随机标签生成 one-hot 编码\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]  # 将 one-hot 编码嵌入噪声的前 num_classes 维\n",
    "        # noise_ = (torch.from_numpy(noise_))  # 将 numpy 数组转换为 PyTorch 张量\n",
    "        # noise.data.copy_(noise_.view(batch_size, nz, 1, 1))  # 将处理好的噪声复制到 noise 变量中，并调整形状\n",
    "        noise_ = torch.from_numpy(noise_)\n",
    "        noise.data.copy_(noise_.view(batch_size, nz, 1, 1)) # 将处理好的噪声复制到 noise 变量中，并调整形状\n",
    "        aux_label = torch.from_numpy(label).to(device)  # device could be 'cuda' if needed  # 将生成的标签转换为张量并复制到辅助标签变量中\n",
    "\n",
    "        fake = netG(noise)  # 使用生成器生成假图像\n",
    "        dis_label.data.fill_(fake_label)  # 将判别器标签全部设置为虚假标签\n",
    "        dis_output, aux_output = netD(fake.detach())  # 将生成的假图像输入判别器，detach() 防止梯度传回生成器\n",
    "        dis_errD_fake = dis_criterion(dis_output, dis_label)  # 计算假图像的判别器损失\n",
    "        aux_errD_fake = aux_criterion(aux_output, aux_label)  # 计算假图像的辅助分类损失\n",
    "        errD_fake = dis_errD_fake + aux_errD_fake  # 计算假图像总损失（判别器损失 + 辅助分类损失）\n",
    "        errD_fake.backward()  # 对假图像损失进行反向传播，计算梯度\n",
    "        D_G_z1 = dis_output.data.mean()  # 计算生成假图像的判别器输出平均值，用于监控\n",
    "        errD = errD_real + errD_fake  # 计算判别器总损失（真实 + 假数据的损失之和）\n",
    "        optimizerD.step()  # 更新判别器参数\n",
    "\n",
    "        ############################\n",
    "        # (2) 更新生成器网络：最大化 log(D(G(z)))\n",
    "        ############################\n",
    "        netG.zero_grad()  # 清空生成器的梯度\n",
    "        dis_label.data.fill_(real_label)  # 将判别器标签设置为真实标签（生成器目标是欺骗判别器）\n",
    "        dis_output, aux_output = netD(fake)  # 将生成的假图像输入判别器，获得输出\n",
    "        dis_errG = dis_criterion(dis_output, dis_label)  # 计算生成器的判别器损失\n",
    "        aux_errG = aux_criterion(aux_output, aux_label)  # 计算生成器的辅助分类损失\n",
    "        errG = dis_errG + aux_errG  # 计算生成器总损失（判别器损失 + 辅助分类损失）\n",
    "        errG.backward()  # 对生成器损失进行反向传播，计算梯度\n",
    "        D_G_z2 = dis_output.data.mean()  # 计算生成图像在判别器中的输出平均值，用于监控\n",
    "        optimizerG.step()  # 更新生成器参数\n",
    "\n",
    "        # 计算当前的平均损失和平均准确率\n",
    "        curr_iter = epoch * len(train_loader) + i  # 计算当前迭代的总步数\n",
    "        all_loss_G = avg_loss_G * curr_iter  # 累加之前的生成器总损失\n",
    "        all_loss_D = avg_loss_D * curr_iter  # 累加之前的判别器总损失\n",
    "        all_loss_A = avg_loss_A * curr_iter  # 累加之前的辅助分类准确率总和\n",
    "        all_loss_G += errG.item()  # 加入当前生成器损失\n",
    "        all_loss_D += errD.item() # 加入当前判别器损失\n",
    "        all_loss_A += accuracy  # 加入当前准确率\n",
    "        avg_loss_G = all_loss_G / (curr_iter + 1)  # 计算新的生成器平均损失\n",
    "        avg_loss_D = all_loss_D / (curr_iter + 1)  # 计算新的判别器平均损失\n",
    "        avg_loss_A = all_loss_A / (curr_iter + 1)  # 计算新的辅助分类平均准确率\n",
    "\n",
    "        # 打印当前训练状态，包括损失、判别器输出和准确率\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f (%.4f) Loss_G: %.4f (%.4f) D(x): %.4f D(G(z)): %.4f / %.4f Acc: %.4f (%.4f)'\n",
    "              % (epoch, args['niter'], i, len(train_loader),\n",
    "                 errD.item(), avg_loss_D, errG.item(), avg_loss_G, D_x, D_G_z1, D_G_z2, accuracy, avg_loss_A))\n",
    "        if i % 100 == 0:  # 每100个批次执行一次保存操作\n",
    "            vutils.save_image(\n",
    "                real_cpu, '%s/real_samples.png' % args['outf'])  # 保存当前批次的真实图像样本到指定文件夹\n",
    "            print('Label for eval = {}'.format(eval_label))  # 打印评估时使用的标签\n",
    "            fake = netG(eval_noise)  # 用评估噪声生成假图像\n",
    "            vutils.save_image(\n",
    "                fake.data,\n",
    "                '%s/fake_samples_epoch_%03d.png' % (args['outf'], epoch)  # 保存生成器生成的假图像样本，文件名中包含当前 epoch\n",
    "            )\n",
    "\n",
    "    # 每个 epoch 结束后，保存当前的模型检查点\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (args['outf'], epoch))  # 保存生成器参数\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (args['outf'], epoch))  # 保存判别器参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
