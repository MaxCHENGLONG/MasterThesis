{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections  # 导入collections模块，用于统计和操作容器数据，如Counter\n",
    "import torch  # 导入PyTorch库，用于深度学习任务\n",
    "import torch.nn as nn  # 从torch中导入神经网络模块，简化模型构建\n",
    "from torch.utils.data import TensorDataset  # 导入TensorDataset，用于将Tensor数据打包成数据集\n",
    "import numpy as np  # 导入NumPy库，用于高效的数值计算和数组操作\n",
    "from sklearn.neighbors import NearestNeighbors  # 导入最近邻算法，用于在SMOTE中寻找相邻样本\n",
    "import time  # 导入time模块，用于计时\n",
    "import os  # 导入os模块，用于文件和目录操作\n",
    "import torch.nn.functional as F\n",
    "# Attention Block\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        \"\"\"\n",
    "        :param filters: 输入特征图的通道数(同时也是卷积输出的通道数)\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        # 与 Keras 中的 Conv2D(filters, kernel_size=1, padding='same') 对应\n",
    "        # PyTorch 中 padding=0 就相当于 'same'（仅当 kernel_size=1 时）\n",
    "        self.query_conv = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "        self.key_conv   = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "        self.value_conv = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 的形状一般是 (batch_size, filters, H, W)\n",
    "        \"\"\"\n",
    "        # 1. 分别得到 query, key, value\n",
    "        query = F.relu(self.query_conv(x))\n",
    "        key   = F.relu(self.key_conv(x))\n",
    "        value = F.relu(self.value_conv(x))\n",
    "\n",
    "        # 2. 计算注意力图: 先元素乘，再对通道维度 (dim=1) 求和\n",
    "        attention_map = query * key                  # 形状 (N, filters, H, W)\n",
    "        attention_map = torch.sum(attention_map, dim=1, keepdim=True)  \n",
    "        # 现在 attention_map 的形状是 (N, 1, H, W)\n",
    "\n",
    "        # 3. 对空间维度 (H, W) 做 softmax\n",
    "        # 先展平再 softmax，再 reshape 回去\n",
    "        N, _, H, W = attention_map.shape\n",
    "        attention_map = attention_map.view(N, 1, -1)         # (N, 1, H*W)\n",
    "        attention_map = F.softmax(attention_map, dim=-1)     # 在 H*W 上做 softmax\n",
    "        attention_map = attention_map.view(N, 1, H, W)       # (N, 1, H, W)\n",
    "\n",
    "        # 4. 注意力加权 value，并与原输入相加\n",
    "        attended_value = attention_map * value\n",
    "        output = x + attended_value\n",
    "\n",
    "        return output\n",
    "    \n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # 卷积层：输入28x28 → 输出1x1\n",
    "        self.conv = nn.Sequential(\n",
    "            # 输入: (1, 28, 28)\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1),  # 输出: (dim_h, 14, 14)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            AttentionBlock(filters=self.dim_h),  # 保持尺寸不变\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1),  # 输出: (dim_h*2, 7, 7)\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1),  # 输出: (dim_h*4, 2, 2)\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1),  # 输出: (dim_h*8, 1, 1)\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        # 全连接层：输入dim_h*8 → 输出n_z\n",
    "        self.fc = nn.Linear(self.dim_h * 8, self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)          # 输出形状: (batch, dim_h*8, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # 展平: (batch, dim_h*8)\n",
    "        x = self.fc(x)             # 输出形状: (batch, n_z)\n",
    "        return x\n",
    "#解码器\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # 全连接层：将潜在变量映射到1x1特征图\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8),  # 输出: (batch, dim_h*8)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 反卷积层：1x1 → 28x28\n",
    "        self.deconv = nn.Sequential(\n",
    "            # 输入: (dim_h*8, 1, 1)\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, kernel_size=3, stride=2),  # 输出: (dim_h*4, 3, 3)\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, kernel_size=3, stride=2),  # 输出: (dim_h*2, 7, 7)\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, self.n_channel, kernel_size=4, stride=4),  # 输出: (1, 28, 28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)                          # 输出形状: (batch, dim_h*8)\n",
    "        x = x.view(-1, self.dim_h * 8, 1, 1)    # 重塑: (batch, dim_h*8, 1, 1)\n",
    "        x = self.deconv(x)                      # 输出形状: (batch, 1, 28, 28)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
