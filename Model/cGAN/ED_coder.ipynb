{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections  # 导入collections模块，用于统计和操作容器数据，如Counter\n",
    "import torch  # 导入PyTorch库，用于深度学习任务\n",
    "import torch.nn as nn  # 从torch中导入神经网络模块，简化模型构建\n",
    "from torch.utils.data import TensorDataset  # 导入TensorDataset，用于将Tensor数据打包成数据集\n",
    "import numpy as np  # 导入NumPy库，用于高效的数值计算和数组操作\n",
    "from sklearn.neighbors import NearestNeighbors  # 导入最近邻算法，用于在SMOTE中寻找相邻样本\n",
    "import time  # 导入time模块，用于计时\n",
    "import os  # 导入os模块，用于文件和目录操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'dim_h': 64, 'n_channel': 1, 'n_z': 300, 'sigma': 1.0, 'lambda': 0.01, 'lr': 0.0002, 'epochs': 50, 'batch_size': 64, 'save': True, 'train': True, 'dataset': 'mnist34', 'fraction': 0.005}\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)  # 打印当前PyTorch使用的CUDA版本，例如显示\"10.1\"\n",
    "\n",
    "t3 = time.time()  # 记录程序开始时的时间，用于后续计算总运行时间\n",
    "##############################################################################\n",
    "\"\"\"args for AE\"\"\"\n",
    "# 以下部分设置自动编码器（AE）的相关参数\n",
    "args = {}  # 创建一个空字典，用于存放模型和训练的参数\n",
    "args['dim_h'] = 64         # 设置隐藏层通道数的基础因子，后续卷积层的通道数会成倍增加\n",
    "args['n_channel'] = 1  #3    # 输入数据的通道数，1表示灰度图（3则为彩色图）；这里选用灰度图\n",
    "args['n_z'] = 300 #600     # 潜在空间（编码空间）的维度数，决定编码器输出特征向量的大小\n",
    "args['sigma'] = 1.0        # 潜在空间中使用的方差参数，可用于正则化\n",
    "args['lambda'] = 0.01      # 判别器损失的权重超参数（如在对抗训练中使用）\n",
    "args['lr'] = 0.0002        # Adam优化器的学习率，决定参数更新的步长\n",
    "args['epochs'] = 50       # 训练过程中遍历数据集的轮数\n",
    "args['batch_size'] = 64   # 每个训练批次的样本数量\n",
    "args['save'] = True        # 如果为True，则在每个训练轮结束时保存模型权重\n",
    "args['train'] = True       # 若为True则进行训练，否则加载已保存的模型进行测试\n",
    "args['dataset'] = 'mnist34'  #'fmnist' # 指定使用的数据集，这里选择MNIST数据集 mnist34 mnist17 fashionmnist34 cifar10\n",
    "args['fraction'] = 0.005   # 用于训练的数据集的子集比例，可用于快速测试\n",
    "\n",
    "##############################################################################\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# Attention Block\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        \"\"\"\n",
    "        :param filters: 输入特征图的通道数(同时也是卷积输出的通道数)\n",
    "        \"\"\"\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        # 与 Keras 中的 Conv2D(filters, kernel_size=1, padding='same') 对应\n",
    "        # PyTorch 中 padding=0 就相当于 'same'（仅当 kernel_size=1 时）\n",
    "        self.query_conv = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "        self.key_conv   = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "        self.value_conv = nn.Conv2d(filters, filters, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 的形状一般是 (batch_size, filters, H, W)\n",
    "        \"\"\"\n",
    "        # 1. 分别得到 query, key, value\n",
    "        query = F.relu(self.query_conv(x))\n",
    "        key   = F.relu(self.key_conv(x))\n",
    "        value = F.relu(self.value_conv(x))\n",
    "\n",
    "        # 2. 计算注意力图: 先元素乘，再对通道维度 (dim=1) 求和\n",
    "        attention_map = query * key                  # 形状 (N, filters, H, W)\n",
    "        attention_map = torch.sum(attention_map, dim=1, keepdim=True)  \n",
    "        # 现在 attention_map 的形状是 (N, 1, H, W)\n",
    "\n",
    "        # 3. 对空间维度 (H, W) 做 softmax\n",
    "        # 先展平再 softmax，再 reshape 回去\n",
    "        N, _, H, W = attention_map.shape\n",
    "        attention_map = attention_map.view(N, 1, -1)         # (N, 1, H*W)\n",
    "        attention_map = F.softmax(attention_map, dim=-1)     # 在 H*W 上做 softmax\n",
    "        attention_map = attention_map.view(N, 1, H, W)       # (N, 1, H, W)\n",
    "\n",
    "        # 4. 注意力加权 value，并与原输入相加\n",
    "        attended_value = attention_map * value\n",
    "        output = x + attended_value\n",
    "\n",
    "        return output\n",
    "# 定义编码器模型\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Encoder, self).__init__()  # 调用父类构造函数\n",
    "        self.n_channel = args['n_channel']  # 获取输入数据的通道数\n",
    "        self.dim_h = args['dim_h']          # 获取隐藏层基本通道数\n",
    "        self.n_z = args['n_z']              # 获取潜在空间的维度数\n",
    "        \n",
    "        # 使用卷积层提取图像特征\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),  \n",
    "            # 第一层卷积：输入通道数为n_channel，输出为dim_h，卷积核大小4，步幅2，填充1，无偏置\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 使用LeakyReLU激活函数，负半部斜率设为0.2\n",
    "            AttentionBlock(filters=self.dim_h),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),  \n",
    "            # 第二层卷积：通道数翻倍到dim_h*2\n",
    "            nn.BatchNorm2d(self.dim_h * 2),  # 对第二层卷积输出进行批归一化\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 激活函数\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),  \n",
    "            # 第三层卷积：通道数增加到dim_h*4\n",
    "            nn.BatchNorm2d(self.dim_h * 4),  # 批归一化\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),  # 激活函数\n",
    "            \n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),  \n",
    "            # 第四层卷积：通道数增加到dim_h*8\n",
    "            \n",
    "            #3d and 32 by 32\n",
    "            #nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 1, 0, bias=False),  # 备用卷积层配置\n",
    "            nn.BatchNorm2d(self.dim_h * 8),  # 对第四层卷积输出进行批归一化\n",
    "            #nn.ReLU(True),\n",
    "            nn.LeakyReLU(0.2, inplace=True)  # 激活函数\n",
    "            # 注释中还有其他可能的卷积配置，这里使用的是标准配置\n",
    "        )\n",
    "        # 全连接层：将卷积层输出映射到潜在空间\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)  \n",
    "        # 这里计算dim_h * (2**3)相当于dim_h*8，假设卷积层最后输出特征数为dim_h*8\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数\n",
    "        # print('enc')  # 调试打印，可查看编码器被调用\n",
    "        # print('input ', x.size())  # 打印输入尺寸，例如torch.Size([batch_size, channel, H, W])\n",
    "        x = self.conv(x)  # 将输入图像通过卷积层提取特征\n",
    "        x = x.squeeze()   # 去除多余的尺寸（例如将[batch_size, 1, N]变为[batch_size, N]）\n",
    "        # print('aft squeeze ', x.size())  # 调试打印压缩后的尺寸\n",
    "        x = self.fc(x)    # 通过全连接层映射到潜在空间维度\n",
    "        # print('out ', x.size())  # 打印最终输出尺寸，应为[batch_size, n_z]\n",
    "        return x  # 返回编码后的潜在表示\n",
    "\n",
    "# 定义解码器模型\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Decoder, self).__init__()  # 调用父类构造函数\n",
    "        self.n_channel = args['n_channel']  # 获取输入通道数（用于输出重构图像）\n",
    "        self.dim_h = args['dim_h']          # 获取隐藏层基本通道数\n",
    "        self.n_z = args['n_z']              # 获取潜在空间的维度数\n",
    "\n",
    "        # 全连接层：将潜在向量映射到足够重构卷积特征图的尺寸\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),  # 将潜在向量转换为高维特征，尺寸为[batch_size, dim_h*8*7*7]\n",
    "            nn.ReLU()  # 使用ReLU激活函数\n",
    "        )\n",
    "\n",
    "        # 反卷积层（转置卷积）：将全连接层的输出转换为图像\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),  \n",
    "            # 第一层反卷积：将通道数从dim_h*8降到dim_h*4，卷积核大小4\n",
    "            nn.BatchNorm2d(self.dim_h * 4),  # 批归一化\n",
    "            nn.ReLU(True),  # 激活函数\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),  \n",
    "            # 第二层反卷积：将通道数从dim_h*4降到dim_h*2\n",
    "            nn.BatchNorm2d(self.dim_h * 2),  # 批归一化\n",
    "            nn.ReLU(True),  # 激活函数\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),  \n",
    "            # 第三层反卷积：将通道数降为1，同时上采样（步幅为2），恢复到原图大小\n",
    "            # nn.Sigmoid())  # 也可用Sigmoid激活函数使输出在[0,1]之间\n",
    "            nn.Tanh()  # 这里使用Tanh激活函数，将输出映射到[-1,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数\n",
    "        # print('dec')  # 调试打印，查看解码器调用\n",
    "        # print('input ', x.size())  # 打印输入潜在向量的尺寸\n",
    "        x = self.fc(x)  # 通过全连接层处理潜在向量\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)  \n",
    "        # 将全连接层输出重塑为特征图，尺寸为[batch_size, dim_h*8, 7, 7]，为反卷积做准备\n",
    "        x = self.deconv(x)  # 通过反卷积层重构出图像\n",
    "        return x  # 返回重构图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the MNIST dataset with labels 3 and 4.\n",
      "Imbalanced Ratio:  0.1\n",
      "Number of label 3 in the final training set:  6131\n",
      "Number of label 4 in the final training set (after downsampling):  613\n",
      "Number of label 3 in the final test set:  1010\n",
      "Number of label 4 in the final test set:  982\n",
      "Total samples in final training set:  6744\n",
      "Total samples in final test set:  1992\n",
      "Number of batches in training set:  106\n",
      "Number of batches in test set:  32\n",
      "Images shape: torch.Size([64, 1, 28, 28])\n",
      "Labels shape: torch.Size([64])\n",
      "First image tensor:\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.5451, 0.9922, 0.6627, 0.5412, 0.5412, 0.0941, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2549, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.6784,\n",
      "          0.0510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0275, 0.3529, 0.5922, 0.4353, 0.7176, 0.9922, 0.9882,\n",
      "          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882,\n",
      "          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882,\n",
      "          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.8941,\n",
      "          0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3725, 0.9922, 0.7255,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.3922, 0.9647, 0.9686, 0.2627,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.2235, 0.9569, 0.9882, 0.5294, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0275, 0.6196, 0.9882, 0.9451, 0.5294,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4157, 1.0000, 0.9922,\n",
      "          0.7451, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4039, 0.9255,\n",
      "          0.9882, 0.8588, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3059,\n",
      "          0.9882, 0.9882, 0.0863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n",
      "          0.9882, 0.8549, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5922,\n",
      "          0.9882, 0.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5843, 0.9922,\n",
      "          0.8039, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0667, 0.3882, 0.9922, 0.7922,\n",
      "          0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0275, 0.2745, 0.2745, 0.7373, 0.9882, 0.8431, 0.2627,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2588, 0.9882, 0.9882, 0.9882, 0.8235, 0.0824, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.3373, 0.9882, 0.9882, 0.4549, 0.0471, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "First image label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (6744, 784)\n",
      "y_train.shape: (6744,)\n",
      "X_test.shape: (1992, 784)\n",
      "y_test.shape: (1992,)\n",
      "Encoder(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): AttentionBlock(\n",
      "      (query_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (key_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (value_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=300, bias=True)\n",
      ")\n",
      "Decoder(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=25088, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (deconv): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "train image shape: (6744, 784)\n",
      "train label shape: (6744,)\n",
      "Counter({np.int64(1): 6131, np.int64(0): 613})\n",
      "train image shape: (6744, 1, 28, 28)\n",
      "(Features)Tensor Dec_X: torch.Size([6744, 1, 28, 28])\n",
      "(Labels)Tensor Dec_y: torch.Size([6744])\n",
      "train_loader: <torch.utils.data.dataloader.DataLoader object at 0x17f26f460>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#print('labsn:',labsn.shape, labsn)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m z_hat \u001b[38;5;241m=\u001b[39m encoder(images) \u001b[38;5;66;03m# 通过编码器生成潜在向量\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_hat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 通过解码器生成重构图像\u001b[39;00m\n\u001b[1;32m     58\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m criterion(x_hat, images) \u001b[38;5;66;03m# 计算重构图像与原始图像的均方误差\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#print('mse_loss:',mse_loss)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 129\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim_h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m)  \n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# 将全连接层输出重塑为特征图，尺寸为[batch_size, dim_h*8, 7, 7]，为反卷积做准备\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 通过反卷积层重构出图像\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/Thesis/lib/python3.9/site-packages/torch/nn/modules/conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1154\u001b[0m     output_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m )\n\u001b[0;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Get_datasets import get_datasets\n",
    "X_train, y_train, X_test, y_test,train_loader,test_loader = get_datasets(dataname=\"mnist34\",fraction=0.1)  # 获取数据集\n",
    "encoder = Encoder(args)  # 创建编码器模型\n",
    "decoder = Decoder(args)  # 创建解码器模型\n",
    "print(encoder)\n",
    "print(decoder)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 检测GPU是否可用\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)  # 将模型移动到GPU上\n",
    "train_on_gpu = torch.cuda.is_available()  # 检测GPU是否可用\n",
    "#Deconder loss function\n",
    "criterion = nn.MSELoss()  # 定义均方误差损失函数\n",
    "criterion = criterion.to(device)  # 将损失函数移动到GPU上\n",
    "\n",
    "dec_x = X_train  # 获取训练数据\n",
    "dec_y = y_train  # 获取训练标签\n",
    "print('train image shape:', dec_x.shape)  # 打印训练数据形状\n",
    "print('train label shape:', dec_y.shape)  # 打印训练标签形状\n",
    "#print('train image:', dec_x)  # 打印训练数据\n",
    "#print('train label:', dec_y)  # 打印训练标签\n",
    "print(collections.Counter(dec_y))  # 使用Counter统计每个类别的样本数量\n",
    "dec_x = dec_x.reshape(-1, 1, 28, 28)  # 将训练数据重塑为合适的形状\n",
    "print('train image shape:', dec_x.shape)  # 打印重塑后的训练数据形状\n",
    "batch_size = args['batch_size']  # 获取批次大小\n",
    "num_workers = 0  # 设置数据加载器的线程数\n",
    "tensor_x = torch.Tensor(dec_x)  # 将NumPy数组转换为PyTorch张量\n",
    "tensor_y = torch.tensor(dec_y, dtype = torch.long)  # 将NumPy数组转换为PyTorch张量\n",
    "print('(Features)Tensor Dec_X:',tensor_x.shape)\n",
    "print('(Labels)Tensor Dec_y:',tensor_y.shape)\n",
    "\n",
    "mnist_train  =  TensorDataset(tensor_x, tensor_y)  # 将特征和标签打包为数据集\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, num_workers=num_workers, shuffle=True)  # 创建数据加载器\n",
    "print('train_loader:',train_loader)\n",
    "classes = ('0','1') # binary classification\n",
    "best_loss = np.inf  # 初始化最佳损失为无穷大\n",
    "\n",
    "t0 = time.time()  # 记录当前时间，用于计算训练时间\n",
    "if args['train']:\n",
    "    fraction = args['fraction']  # 获取数据集子集比例\n",
    "    encoder_optim = torch.optim.Adam(encoder.parameters(), lr=args['lr'])  # 创建编码器的Adam优化器\n",
    "    decoder_optim = torch.optim.Adam(decoder.parameters(), lr=args['lr'])  # 创建解码器的Adam优化器\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        train_loss = 0.0  # 初始化训练损失为0\n",
    "        tmse_loss = 0.0 # 初始化均方误差损失为0\n",
    "        tdiscr_loss = 0.0 # 初始化判别器损失为0\n",
    "        encoder.train()  # 设置编码器为训练模式\n",
    "        decoder.train()\n",
    "        for images, labels in train_loader: # 从数据加载器中加载数据\n",
    "            encoder_optim.zero_grad()\n",
    "            decoder_optim.zero_grad()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labsn = labels.detach().cpu().numpy()  # 将标签转换为NumPy数组\n",
    "            #print('labsn:',labsn.shape, labsn)\n",
    "            z_hat = encoder(images) # 通过编码器生成潜在向量\n",
    "            x_hat = decoder(z_hat) # 通过解码器生成重构图像\n",
    "            mse_loss = criterion(x_hat, images) # 计算重构图像与原始图像的均方误差\n",
    "            #print('mse_loss:',mse_loss)\n",
    "            resx = [] \n",
    "            resy = []\n",
    "            tc = 0 # 固定类别\n",
    "            xbeg = dec_x[dec_y == tc]  # 从全局训练图像dec_x中选择标签等于c的所有样本\n",
    "            ybeg = dec_y[dec_y == tc]  # 从全局标签dec_y中选择标签等于c的所有样本\n",
    "            xlen = len(xbeg)\n",
    "            #print('xbeg:',xbeg.shape)\n",
    "            nsample = min(100, xlen)  # 生成样本数\n",
    "            ind = np.random.choice(list(range(xlen)), nsample, replace=False)  # 随机选择nsample个样本\n",
    "            xclass = xbeg[ind]  # 选择对应的图像\n",
    "            yclass = ybeg[ind]\n",
    "            xclen = len(xclass)\n",
    "            xcminus = np.arange(1,xclen) # 1 to xclen-1\n",
    "            xcplus = np.append(xcminus, 0 ) # 0 to xclen-2\n",
    "\n",
    "            xcnew = (xclass[[xcplus], :])  \n",
    "\n",
    "            xcnew = xcnew = xcnew.reshape(xcnew.shape[1], xcnew.shape[2], xcnew.shape[3], xcnew.shape[4]) # 1 to xclen-1, 0 to xclen-2\n",
    "            #print('xcnew:',xcnew.shape)\n",
    "\n",
    "            xcnew = torch.Tensor(xcnew)\n",
    "            xcnew = xcnew.to(device)\n",
    "            xclass = torch.Tensor(xclass)\n",
    "            xclass = xclass.to(device)\n",
    "            xclass = encoder(xclass)\n",
    "            xclass = xclass.detach().cpu().numpy()\n",
    "            xc_enc = (xclass[[xcplus],:])\n",
    "            xc_enc = np.squeeze(xc_enc)\n",
    "            xc_enc = torch.Tensor(xc_enc)\n",
    "            xc_enc = xc_enc.to(device)\n",
    "            #print('xc_enc:',xc_enc.shape)\n",
    "            ximg = decoder(xc_enc)\n",
    "            mse_loss2 = criterion(ximg, xcnew)\n",
    "            #print('mse_loss2:',mse_loss2)\n",
    "            combined_loss = mse_loss + mse_loss2\n",
    "            combined_loss.backward()\n",
    "\n",
    "            encoder_optim.step()\n",
    "            decoder_optim.step()\n",
    "\n",
    "            train_loss += combined_loss.item() * images.size(0)\n",
    "            tmse_loss += mse_loss.item() * images.size(0)\n",
    "            #print('train_loss:',train_loss)\n",
    "            #print('tmse_loss:',tmse_loss)\n",
    "            tdiscr_loss += mse_loss2.item() * images.size(0)\n",
    "            #print('tdiscr_loss:',tdiscr_loss)\n",
    "\n",
    "\n",
    "        # print training statistics\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        tmse_loss = tmse_loss / len(train_loader.dataset)\n",
    "        tdiscr_loss = tdiscr_loss / len(train_loader.dataset)\n",
    "        print('Epoch: {} \\tTrain Loss: {:.6f} \\tmse loss: {:.6f} \\tmse2 loss: {:.6f}'.format(epoch,\n",
    "                  train_loss, tmse_loss, tdiscr_loss))  \n",
    "            # 打印当前epoch的损失信息\n",
    "        # store the best encoder and decoder models\n",
    "            # here, /crs5 is a reference to 5 way cross validation, but is not\n",
    "            # necessary for illustration purposes\n",
    "        if train_loss < best_loss:  \n",
    "            # 如果当前epoch的平均损失低于历史最佳损失，则保存模型\n",
    "            print('Saving..')\n",
    "            path_enc = 'DeepSMOTE_{}_bst_enc.pth'.format(fraction)\n",
    "            # 构造保存最佳编码器模型的文件路径\n",
    "            path_dec = 'DeepSMOTE_{}_bst_dec.pth'.format(fraction)\n",
    "            # 构造保存最佳解码器模型的文件路径\n",
    "            \n",
    "            torch.save(encoder.state_dict(), path_enc)  \n",
    "            # 保存编码器当前状态字典（权重参数）\n",
    "            torch.save(decoder.state_dict(), path_dec)  \n",
    "            # 保存解码器当前状态字典\n",
    "            \n",
    "            best_loss = train_loss  # 更新历史最佳损失值\n",
    "     # in addition, store the final model (may not be the best) for\n",
    "    # informational purposes\n",
    "    \n",
    "    path_enc = 'DS_{}_final_enc.pth'.format(fraction)  \n",
    "    # 构造保存最终编码器模型（可能不是最佳）的文件路径\n",
    "    path_dec = 'DS_{}_final_dec.pth'.format(fraction)  \n",
    "    # 构造保存最终解码器模型的文件路径\n",
    "    print(path_enc)\n",
    "    print(path_dec)\n",
    "    torch.save(encoder.state_dict(), path_enc)  # 保存最终编码器状态\n",
    "    torch.save(decoder.state_dict(), path_dec)  # 保存最终解码器状态\n",
    "    print()\n",
    "t1 = time.time()  # 记录当前fold训练结束时间\n",
    "print('total time(min): {:.2f}'.format((t1 - t0) / 60))  \n",
    "# 输出当前fold训练耗时（单位：分钟）\n",
    "\n",
    "t4 = time.time()  # 记录整个程序结束时的时间\n",
    "print('final time(min): {:.2f}'.format((t4 - t3) / 60))  \n",
    "# 输出整个程序运行的总耗时（单位：分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
